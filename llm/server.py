#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from flask import Flask, request, jsonify, Response, stream_template
from flask_cors import CORS
import json
import sys
import time
import io
import torch
from robot_system import RobotSystem

app = Flask(__name__)
CORS(app)  # CORS í™œì„±í™”

print("ğŸš€ PyTorch ê¸°ë°˜ RobotSystem ì„œë²„ ì´ˆê¸°í™” ì¤‘...")

# GPU í™˜ê²½ í™•ì¸
if torch.cuda.is_available():
    print(f"âœ… GPU ëª¨ë“œ: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š CUDA ë²„ì „: {torch.version.cuda}")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    print("âœ… Apple Silicon MPS ëª¨ë“œ")
else:
    print("ğŸ’» CPU ëª¨ë“œ")

print(f"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}")

# RobotSystem ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (PyTorch ìµœì í™” ì„¤ì •)
robot = RobotSystem(
    use_real_model=True, 
    use_reasoning=False, 
    fast_mode=True, 
    debug_mode=False
)

@app.route('/api/chat', methods=['POST'])
def chat():
    """ì¼ë°˜ ì±„íŒ… API - ìµœì¢… ì‘ë‹µë§Œ ë°˜í™˜"""
    try:
        data = request.get_json()
        user_input = data.get('message', '')
        
        if not user_input:
            return jsonify({'error': 'ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400
        
        print(f"ğŸ“± Androidì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ : {user_input}")
        
        # RobotSystem ì²˜ë¦¬ (ìµœì¢… ì‘ë‹µë§Œ ë°˜í™˜)
        response = robot.process_user_input(user_input)
        print(f"ğŸ” ì¼ë°˜ ì±„íŒ… ì„œë²„ì—ì„œ ë°›ì€ ì‘ë‹µ: '{response}'")
        print(f"ğŸ” ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
        
        return jsonify({'response': response})
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return jsonify({'error': f'ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    # GPU ìƒíƒœ ì •ë³´ í¬í•¨
    gpu_info = {}
    if torch.cuda.is_available():
        gpu_info = {
            'gpu_available': True,
            'gpu_name': torch.cuda.get_device_name(0),
            'cuda_version': torch.version.cuda,
            'memory_allocated': f"{torch.cuda.memory_allocated() / 1024**3:.1f}GB",
            'memory_reserved': f"{torch.cuda.memory_reserved() / 1024**3:.1f}GB"
        }
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        gpu_info = {
            'gpu_available': True,
            'gpu_name': 'Apple Silicon MPS',
            'device_type': 'mps'
        }
    else:
        gpu_info = {
            'gpu_available': False,
            'device_type': 'cpu'
        }
    
    return jsonify({
        'status': 'ok', 
        'message': 'PyTorch ê¸°ë°˜ RobotSystem ì„œë²„ê°€ ì •ìƒ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤',
        'pytorch_version': torch.__version__,
        'gpu_info': gpu_info
    })

@app.route('/api/stream', methods=['POST'])
def stream_chat():
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (Server-Sent Events) - PyTorch ìµœì í™”"""
    try:
        data = request.get_json()
        user_input = data.get('message', '')
        
        if not user_input:
            return jsonify({'error': 'ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400
        
        print(f"ğŸ“± Androidì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {user_input}")
        
        def generate_stream():
            try:
                # RobotSystem ì²˜ë¦¬ (PyTorch ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°)
                response = robot.process_user_input(user_input)
                print(f"ğŸ” ì„œë²„ì—ì„œ ë°›ì€ ì‘ë‹µ: '{response}'")
                print(f"ğŸ” ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
                print(f"ğŸ” ì‘ë‹µì´ ë¹„ì–´ìˆë‚˜? {not response}")
                print(f"ğŸ” ì‘ë‹µì´ ê³µë°±ë§Œ ìˆë‚˜? {not response.strip() if response else True}")
                
                if response and response.strip():
                    # ìµœì¢… ì‘ë‹µì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ê³µë°± ë³´ì¡´)
                    # ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ë˜ ê³µë°±ë„ í¬í•¨
                    import re
                    words_with_spaces = re.split(r'(\s+)', response)
                    
                    for i, word in enumerate(words_with_spaces):
                        if word:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                            yield f"data: {json.dumps({'type': 'stream', 'content': word, 'index': i})}\n\n"
                            time.sleep(0.05)  # ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° ì†ë„
                    
                    # ì™„ë£Œ ì‹ í˜¸
                    yield f"data: {json.dumps({'type': 'complete', 'content': response})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'error', 'content': 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                    
            except Exception as e:
                error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
                yield f"data: {json.dumps({'type': 'error', 'content': error_msg})}\n\n"
        
        return Response(generate_stream(), mimetype='text/event-stream')
        
    except Exception as e:
        return jsonify({'error': f'ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/token_stream', methods=['POST'])
def token_stream_chat():
    """í† í° ë‹¨ìœ„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° - PyTorch ìµœì í™”"""
    try:
        data = request.get_json()
        user_input = data.get('message', '')
        
        if not user_input:
            return jsonify({'error': 'ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤'}), 400
        
        print(f"ğŸ“± Androidì—ì„œ í† í° ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {user_input}")
        
        def generate_token_stream():
            try:
                # RobotSystem ì²˜ë¦¬ (PyTorch ê¸°ë°˜ í† í° ìŠ¤íŠ¸ë¦¬ë°)
                response = robot.process_user_input(user_input)
                print(f"ğŸ” í† í° ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ì—ì„œ ë°›ì€ ì‘ë‹µ: '{response}'")
                print(f"ğŸ” í† í° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ê¸¸ì´: {len(response) if response else 0}")
                
                if response and response.strip():
                    # ìµœì¢… ì‘ë‹µì„ ë¬¸ì ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ê³µë°± ë³´ì¡´)
                    for i, char in enumerate(response):
                        # ê³µë°± ë¬¸ìë„ í¬í•¨í•˜ì—¬ ì „ì†¡
                        yield f"data: {json.dumps({'type': 'token', 'content': char, 'index': i})}\n\n"
                        time.sleep(0.01)  # ë¹ ë¥¸ í† í° ìŠ¤íŠ¸ë¦¬ë°
                    
                    # ì™„ë£Œ ì‹ í˜¸
                    yield f"data: {json.dumps({'type': 'complete', 'content': response})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'error', 'content': 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                    
            except Exception as e:
                error_msg = f"í† í° ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}"
                yield f"data: {json.dumps({'type': 'error', 'content': error_msg})}\n\n"
        
        return Response(generate_token_stream(), mimetype='text/event-stream')
        
    except Exception as e:
        return jsonify({'error': f'í† í° ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/gpu_status', methods=['GET'])
def gpu_status():
    """GPU ìƒíƒœ ìƒì„¸ ì •ë³´"""
    try:
        status = {}
        
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            
            status = {
                'cuda_available': True,
                'device_count': device_count,
                'current_device': current_device,
                'device_name': torch.cuda.get_device_name(current_device),
                'cuda_version': torch.version.cuda,
                'pytorch_version': torch.__version__,
                'total_memory': f"{torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.1f}GB",
                'allocated_memory': f"{torch.cuda.memory_allocated() / 1024**3:.1f}GB",
                'reserved_memory': f"{torch.cuda.memory_reserved() / 1024**3:.1f}GB",
                'free_memory': f"{(torch.cuda.get_device_properties(current_device).total_memory - torch.cuda.memory_reserved()) / 1024**3:.1f}GB"
            }
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            status = {
                'mps_available': True,
                'device_name': 'Apple Silicon MPS',
                'pytorch_version': torch.__version__,
                'device_type': 'mps'
            }
        else:
            status = {
                'gpu_available': False,
                'device_type': 'cpu',
                'pytorch_version': torch.__version__
            }
            
        return jsonify(status)
        
    except Exception as e:
        return jsonify({'error': f'GPU ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}'}), 500

if __name__ == '__main__':
    print("\nğŸš€ PyTorch ê¸°ë°˜ RobotSystem HTTP ì„œë²„ ì‹œì‘...")
    print("ğŸ“± Android ì•±ì—ì„œ ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì ‘ì† ê°€ëŠ¥:")
    print("  - ì¼ë°˜ ì±„íŒ…: http://localhost:5000/api/chat")
    print("  - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°: http://localhost:5000/api/stream")
    print("  - í† í° ìŠ¤íŠ¸ë¦¬ë°: http://localhost:5000/api/token_stream")
    print("ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸:")
    print("  - ê¸°ë³¸ ìƒíƒœ: http://localhost:5000/api/health")
    print("  - GPU ìƒíƒœ: http://localhost:5000/api/gpu_status")
    print("âš¡ PyTorch ê¸°ë°˜ ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë°!")
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        print(f"ğŸ® GPU ê°€ì†: {torch.cuda.get_device_name(0)}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("ğŸ Apple Silicon MPS ê°€ì†")
    else:
        print("ğŸ’» CPU ëª¨ë“œ (GPU ê°€ì† ì—†ìŒ)")
    
    app.run(host='0.0.0.0', port=5000, debug=False) 
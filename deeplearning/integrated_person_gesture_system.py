import cv2
import numpy as np
from ultralytics import YOLO
import mediapipe as mp
import torch
import torch.nn as nn
from collections import deque
import os
from datetime import datetime

# Qt í”Œë«í¼ í”ŒëŸ¬ê·¸ì¸ ì˜¤ë¥˜ ë°©ì§€
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['QT_AUTO_SCREEN_SCALE_FACTOR'] = '0'
os.environ['QT_SCALE_FACTOR'] = '1'

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ í´ë˜ìŠ¤
class GestureCNN(nn.Module):
    """ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN)"""
    def __init__(self, num_classes=3, num_frames=60, num_joints=21):
        super(GestureCNN, self).__init__()
        
        self.num_classes = num_classes
        self.num_frames = num_frames
        self.num_joints = num_joints
        
        # ì…ë ¥ íŠ¹ì§• ê³„ì‚°
        self.basic_features = 99  # ëœë“œë§ˆí¬(84) + ê°ë„(15)
        self.gesture_features = 260  # ë™ì (252) + ì†ëª¨ì–‘(8)
        self.total_features = self.basic_features * num_frames + self.gesture_features
        
        # 1. ê¸°ë³¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
        self.sequence_encoder = nn.Sequential(
            nn.Linear(self.basic_features * num_frames, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # 2. ì œìŠ¤ì²˜ íŠ¹ì„± ì²˜ë¦¬
        self.gesture_encoder = nn.Sequential(
            nn.Linear(self.gesture_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 3. ê²°í•©ëœ íŠ¹ì§• ì²˜ë¦¬
        self.combined_encoder = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # ê¸°ë³¸ ì‹œí€€ìŠ¤ íŠ¹ì§• ì¶”ì¶œ
        sequence_features = x[:, :self.basic_features * self.num_frames]
        sequence_encoded = self.sequence_encoder(sequence_features)
        
        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ
        gesture_features = x[:, self.basic_features * self.num_frames:]
        gesture_encoded = self.gesture_encoder(gesture_features)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([sequence_encoded, gesture_encoded], dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.combined_encoder(combined_features)
        
        return output

def estimate_distance(bbox_height, ref_height=300, ref_distance=1.0):
    distance = ref_height / (bbox_height + 1e-6) * ref_distance
    return round(distance, 2)

def estimate_distance_from_mask(mask, ref_height=300, ref_distance=1.0):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œ ë” ì •í™•í•œ ê±°ë¦¬ ê³„ì‚°"""
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        person_height = h
        distance = ref_height / (person_height + 1e-6) * ref_distance
        return round(distance, 2)
    
    return 2.0

def estimate_distance_advanced(mask, ref_height=300, ref_distance=1.0):
    """ê³ ê¸‰ ê±°ë¦¬ ê³„ì‚° - ë§ˆìŠ¤í¬ì˜ ì‹¤ì œ í”½ì…€ ìˆ˜ì™€ í˜•íƒœ ê³ ë ¤"""
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        person_pixels = cv2.contourArea(cnt)
        bbox_area = w * h
        density = person_pixels / (bbox_area + 1e-6)
        adjusted_height = h * density
        distance = ref_height / (adjusted_height + 1e-6) * ref_distance
        
        return round(distance, 2), {
            'person_height': h,
            'person_pixels': person_pixels,
            'bbox_area': bbox_area,
            'density': density,
            'adjusted_height': adjusted_height
        }
    
    return 2.0, {}

def extract_gesture_features(data):
    """ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ (í•™ìŠµ ì‹œì™€ ë™ì¼)"""
    landmarks = data[:, :84]
    angles = data[:, 84:99]
    
    # 1. ë™ì  íŠ¹ì„±
    motion_features = []
    for i in range(1, len(data)):
        landmark_diff = landmarks[i] - landmarks[i-1]
        motion_features.append(landmark_diff)
    
    if len(motion_features) > 0:
        motion_features = np.array(motion_features)
        motion_mean = np.mean(motion_features, axis=0)
        motion_std = np.std(motion_features, axis=0)
        motion_max = np.max(np.abs(motion_features), axis=0)
    else:
        motion_mean = np.zeros(84)
        motion_std = np.zeros(84)
        motion_max = np.zeros(84)
    
    # 2. ì† ëª¨ì–‘ íŠ¹ì„±
    finger_tips = [8, 12, 16, 20]
    palm_center = 0
    
    hand_shape_features = []
    for frame in landmarks:
        frame_reshaped = frame.reshape(-1, 4)
        finger_distances = []
        for tip_idx in finger_tips:
            tip_pos = frame_reshaped[tip_idx][:3]
            palm_pos = frame_reshaped[palm_center][:3]
            distance = np.linalg.norm(tip_pos - palm_pos)
            finger_distances.append(distance)
        hand_shape_features.append(finger_distances)
    
    hand_shape_features = np.array(hand_shape_features)
    hand_shape_mean = np.mean(hand_shape_features, axis=0)
    hand_shape_std = np.std(hand_shape_features, axis=0)
    
    # 3. ì œìŠ¤ì²˜ë³„ íŠ¹ì„± ë²¡í„°
    gesture_specific = np.concatenate([
        motion_mean, motion_std, motion_max,
        hand_shape_mean, hand_shape_std
    ])
    
    return gesture_specific

class IntegratedPersonGestureSystem:
    def __init__(self):
        # YOLO ëª¨ë¸ ì´ˆê¸°í™”
        self.model = YOLO('yolov8s-seg.pt')
        
        # MediaPipe Hands ì´ˆê¸°í™”
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=4,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # ì œìŠ¤ì²˜ ëª¨ë¸ ì´ˆê¸°í™”
        self.gesture_model = GestureCNN(num_classes=3, num_frames=60, num_joints=21)
        if os.path.exists('best_gesture_cnn_model.pth'):
            self.gesture_model.load_state_dict(torch.load('best_gesture_cnn_model.pth'))
        self.gesture_model = self.gesture_model.to(device)
        self.gesture_model.eval()
        
        # ì‚¬ëŒ ì¬ì‹ë³„ ë°ì´í„°
        self.people_data = {}
        self.next_id = 0
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.process_every_n_frames = 3
        self.frame_skip_counter = 0
        
        # ë§¤ì¹­ ê´€ë ¨ ì„¤ì •
        self.match_threshold = 0.35
        self.reentry_threshold = 0.30
        self.min_detection_confidence = 0.6
        self.min_person_area = 5000
        self.max_frames_without_seen = 300
        
        # íˆìŠ¤í† ê·¸ë¨ ê¸°ì–µ ì„¤ì •
        self.max_histograms_per_person = 10
        self.histogram_memory_duration = 30
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì •
        self.enable_gesture_recognition = True
        self.gesture_frame_buffer = deque(maxlen=60)
        self.gesture_prediction_buffer = deque(maxlen=5)
        self.actions = ['COME', 'AWAY', 'STOP']
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.analysis_dir = "./integrated_analysis"
        if not os.path.exists(self.analysis_dir):
            os.makedirs(self.analysis_dir)
            print(f"ğŸ“ í†µí•© ë¶„ì„ ë””ë ‰í† ë¦¬ ìƒì„±: {self.analysis_dir}")
    
    def extract_histogram(self, img, mask, bins=16):
        """HSVì˜ ëª¨ë“  ì±„ë„(H, S, V)ì„ ê³ ë ¤í•œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        h_hist = cv2.calcHist([hsv], [0], mask, [bins], [0, 180])
        s_hist = cv2.calcHist([hsv], [1], mask, [bins], [0, 256])
        v_hist = cv2.calcHist([hsv], [2], mask, [bins], [0, 256])
        
        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()
        
        combined_hist = np.concatenate([h_hist, s_hist, v_hist])
        
        return combined_hist, h_hist, s_hist, v_hist
    
    def calculate_similarity_metrics(self, hist1, hist2):
        """ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        bhatt_dist = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_BHATTACHARYYA)
        
        dot_product = np.dot(hist1, hist2)
        norm1 = np.linalg.norm(hist1)
        norm2 = np.linalg.norm(hist2)
        cosine_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
        
        corr = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)
        chi_square = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CHISQR)
        intersection = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_INTERSECT)
        
        return {
            'bhattacharyya': bhatt_dist,
            'cosine_similarity': cosine_sim,
            'correlation': corr,
            'chi_square': chi_square,
            'intersection': intersection
        }
    
    def find_best_match(self, current_hist, current_bbox, used_ids):
        """ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ëŒ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)"""
        best_match_id = None
        best_score = 0.0
        best_metrics = {}
        
        x1, y1, x2, y2 = current_bbox
        current_center = ((x1 + x2) // 2, (y1 + y2) // 2)
        
        for pid, pdata in self.people_data.items():
            if pid in used_ids:
                continue
                
            if len(pdata['histograms']) == 0:
                continue
            
            hist_scores = []
            for i, stored_hist in enumerate(pdata['histograms'][-self.max_histograms_per_person:]):
                metrics = self.calculate_similarity_metrics(current_hist, stored_hist)
                hist_score = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                hist_scores.append(hist_score)
            
            best_hist_score = max(hist_scores) if hist_scores else 0.0
            
            latest_bbox = pdata['bboxes'][-1]
            stored_center = ((latest_bbox[0] + latest_bbox[2]) // 2, (latest_bbox[1] + latest_bbox[3]) // 2)
            
            center_distance = np.sqrt((current_center[0] - stored_center[0])**2 + 
                                     (current_center[1] - stored_center[1])**2)
            max_distance = np.sqrt(640**2 + 480**2)
            spatial_score = 1.0 - (center_distance / max_distance)
            
            total_score = 0.9 * best_hist_score + 0.1 * spatial_score
            
            if total_score > best_score:
                best_score = total_score
                best_match_id = pid
                best_metrics = {
                    'hist_score': best_hist_score,
                    'spatial_score': spatial_score,
                    'hist_scores': hist_scores
                }
        
        return best_match_id, best_score, best_metrics
    
    def process_gesture(self, frame):
        """ì œìŠ¤ì²˜ ì¸ì‹ ì²˜ë¦¬"""
        if not self.enable_gesture_recognition:
            return "DISABLED", 0.0
        
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(img_rgb)
        
        prediction = "WAITING..."
        confidence = 0.0
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                
                # ëœë“œë§ˆí¬ ë°ì´í„° ì¶”ì¶œ
                joint = np.zeros((21, 4))
                for j, lm in enumerate(hand_landmarks.landmark):
                    joint[j] = [lm.x, lm.y, lm.z, lm.visibility]
                
                # ê°ë„ ê³„ì‚°
                v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
                v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
                v = v2 - v1
                v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]
                
                angle = np.arccos(np.einsum('nt,nt->n',
                    v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], 
                    v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))
                angle = np.degrees(angle)
                
                # íŠ¹ì§• ë²¡í„° ìƒì„±
                d = np.concatenate([joint.flatten(), angle])
                self.gesture_frame_buffer.append(d)
                
                # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
                if len(self.gesture_frame_buffer) >= 30:
                    try:
                        frame_data = np.array(list(self.gesture_frame_buffer))
                        
                        frames = len(frame_data)
                        if frames < 60:
                            padding = np.zeros((60 - frames, 99), dtype=np.float32)
                            x = np.vstack([frame_data, padding])
                        elif frames > 60:
                            start = (frames - 60) // 2
                            x = frame_data[start:start + 60]
                        else:
                            x = frame_data
                        
                        gesture_features = extract_gesture_features(x)
                        x_with_gesture = np.concatenate([x.flatten(), gesture_features])
                        model_input = torch.FloatTensor(x_with_gesture).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            outputs = self.gesture_model(model_input)
                            probabilities = torch.softmax(outputs, dim=1)
                            predicted = torch.argmax(outputs, dim=1).item()
                            confidence = probabilities[0][predicted].item()
                        
                        prediction = self.actions[predicted]
                        self.gesture_prediction_buffer.append(predicted)
                        
                    except Exception as e:
                        print(f"ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                        prediction = "ERROR"
        
        # ë¶€ë“œëŸ¬ìš´ ì˜ˆì¸¡ ê²°ê³¼
        if len(self.gesture_prediction_buffer) >= 3:
            from collections import Counter
            most_common = Counter(self.gesture_prediction_buffer).most_common(1)[0]
            final_prediction = self.actions[most_common[0]]
            final_confidence = most_common[1] / len(self.gesture_prediction_buffer)
        else:
            final_prediction = prediction
            final_confidence = confidence
        
        return final_prediction, final_confidence
    
    def run_integrated_system(self, camera_device="/dev/video0"):
        """í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰ (ë¬´í•œ ì‹¤í–‰)"""
        print("ğŸš€ í†µí•© ì‚¬ëŒ ì¬ì‹ë³„ + ì œìŠ¤ì²˜ ì¸ì‹ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ“¹ ì¹´ë©”ë¼: {camera_device}")
        print("â±ï¸ ë¬´í•œ ì‹¤í–‰ ëª¨ë“œ (qí‚¤ë¡œ ì¢…ë£Œ)")
        
        cap = cv2.VideoCapture(camera_device)
        
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {camera_device}")
            return
        
        frame_count = 0
        start_time = datetime.now()
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            # í”„ë ˆì„ ì²˜ë¦¬ ê°„ê²© ì¡°ì ˆ
            if self.frame_skip_counter < self.process_every_n_frames - 1:
                self.frame_skip_counter += 1
                continue
            
            self.frame_skip_counter = 0
            
            # ì‚¬ëŒ ê°ì§€
            results = self.model(frame, classes=[0])
            annotated = frame.copy()
            
            # ì œìŠ¤ì²˜ ì¸ì‹
            gesture_prediction, gesture_confidence = self.process_gesture(annotated)
            
            # í˜„ì¬ í”„ë ˆì„ì—ì„œ ê°ì§€ëœ ëª¨ë“  ì‚¬ëŒì˜ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
            current_detections = []
            
            for result in results:
                for i in range(len(result.boxes)):
                    seg = result.masks.data[i]
                    box = result.boxes[i]
                    confidence = box.conf[0].item()
                    
                    if confidence < self.min_detection_confidence:
                        continue
                    
                    mask = seg.cpu().numpy().astype(np.uint8) * 255
                    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)
                    
                    # ë…¸ì´ì¦ˆ ì œê±°
                    kernel = np.ones((5,5), np.uint8)
                    mask_cleaned = cv2.morphologyEx(mask_resized, cv2.MORPH_CLOSE, kernel)
                    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)
                    
                    # HSV íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
                    combined_hist, h_hist, s_hist, v_hist = self.extract_histogram(frame, mask_cleaned)
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
                    bbox = box.xyxy[0].cpu().numpy()
                    x1, y1, x2, y2 = bbox
                    area = (x2 - x1) * (y2 - y1)
                    
                    if area < self.min_person_area:
                        continue
                    
                    # ê±°ë¦¬ ì¶”ì •
                    est_dist, dist_info = estimate_distance_advanced(mask_cleaned, ref_height=300, ref_distance=1.0)
                    
                    current_detections.append({
                        'hist': combined_hist,
                        'bbox': bbox,
                        'mask': mask_cleaned,
                        'confidence': confidence,
                        'area': area,
                        'distance': est_dist,
                        'dist_info': dist_info
                    })
            
            # ê°ì§€ëœ ì‚¬ëŒë“¤ì„ ì˜ì—­ í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
            current_detections.sort(key=lambda x: x['area'], reverse=True)
            
            # ì´ë¯¸ ë§¤ì¹­ëœ IDë“¤ì„ ì¶”ì 
            used_ids = set()
            
            # ê° ê°ì§€ëœ ì‚¬ëŒì— ëŒ€í•´ ë§¤ì¹­ ìˆ˜í–‰
            for detection in current_detections:
                combined_hist = detection['hist']
                bbox = detection['bbox']
                
                # ë§¤ì¹­ ì‹œë„
                matched_id, match_score, metrics = self.find_best_match(combined_hist, bbox, used_ids)
                
                # ë§¤ì¹­ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬
                if matched_id is not None and match_score > self.match_threshold:
                    # ê¸°ì¡´ ì‚¬ëŒ ì—…ë°ì´íŠ¸
                    self.people_data[matched_id]['histograms'].append(combined_hist)
                    self.people_data[matched_id]['bboxes'].append(bbox)
                    self.people_data[matched_id]['timestamps'].append(elapsed_time)
                    used_ids.add(matched_id)
                    
                    # íˆìŠ¤í† ê·¸ë¨ ë©”ëª¨ë¦¬ ê´€ë¦¬
                    if len(self.people_data[matched_id]['histograms']) > self.max_histograms_per_person:
                        self.people_data[matched_id]['histograms'].pop(0)
                        self.people_data[matched_id]['bboxes'].pop(0)
                        self.people_data[matched_id]['timestamps'].pop(0)
                    
                    color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (ê¸°ì¡´ ì‚¬ëŒ)
                    print(f"ğŸ”„ ê¸°ì¡´ ì‚¬ëŒ ì¬ì‹ë³„: {matched_id} (ì ìˆ˜: {match_score:.3f})")
                    
                else:
                    # ìƒˆë¡œìš´ ì‚¬ëŒ
                    new_id = f"Person_{self.next_id}"
                    self.people_data[new_id] = {
                        'histograms': [combined_hist],
                        'bboxes': [bbox],
                        'timestamps': [elapsed_time],
                        'images': []
                    }
                    self.next_id += 1
                    used_ids.add(new_id)
                    
                    color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (ìƒˆë¡œìš´ ì‚¬ëŒ)
                    print(f"ğŸ†• ìƒˆë¡œìš´ ì‚¬ëŒ ê°ì§€: {new_id} (ìµœê³  ì ìˆ˜: {match_score:.3f}, ì„ê³„ê°’ ë¯¸ë‹¬)")
                
                # ì‹œê°í™”
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                
                # ID í‘œì‹œ
                id_text = matched_id if matched_id else new_id
                cv2.putText(annotated, id_text, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                
                # Confidence í‘œì‹œ
                conf_text = f"Conf: {detection['confidence']:.2f}"
                cv2.putText(annotated, conf_text, (x1, y2+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ê±°ë¦¬ í‘œì‹œ
                distance_text = f"Dist: {detection['distance']:.2f}m"
                cv2.putText(annotated, distance_text, (x1, y2+40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ë°€ë„ ì •ë³´ í‘œì‹œ
                if detection['dist_info']:
                    density_text = f"Density: {detection['dist_info']['density']:.2f}"
                    cv2.putText(annotated, density_text, (x1, y2+60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
                score_text = f"{match_score:.3f}"
                cv2.putText(annotated, score_text, (x1, y2+80), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # ì œìŠ¤ì²˜ ì •ë³´ í‘œì‹œ
            gesture_text = f"Gesture: {gesture_prediction}"
            cv2.putText(annotated, gesture_text, (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            
            gesture_conf_text = f"Gesture Conf: {gesture_confidence:.2f}"
            cv2.putText(annotated, gesture_conf_text, (10, 150), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            
            # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
            info_text = f"People: {len(self.people_data)} | Frame: {frame_count} | Time: {elapsed_time:.1f}s"
            cv2.putText(annotated, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # ì œìŠ¤ì²˜ ê°€ì´ë“œ
            cv2.putText(annotated, 'COME: Move hand', (10, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(annotated, 'STOP: Fist', (10, 420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(annotated, 'AWAY: Open palm', (10, 440), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # ì„±ëŠ¥ ìµœì í™”: 5í”„ë ˆì„ë§ˆë‹¤ë§Œ í™”ë©´ ì—…ë°ì´íŠ¸
            if frame_count % 5 == 0:
                cv2.imshow("Integrated Person + Gesture Recognition", annotated)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        print(f"\nğŸ“Š í†µí•© ë¶„ì„ ì™„ë£Œ!")
        print(f"   - ì´ í”„ë ˆì„: {frame_count}")
        print(f"   - ê°ì§€ëœ ì‚¬ëŒ ìˆ˜: {len(self.people_data)}")
        print(f"   - ì œìŠ¤ì²˜ ì¸ì‹: {gesture_prediction} (ì‹ ë¢°ë„: {gesture_confidence:.2f})")

if __name__ == "__main__":
    system = IntegratedPersonGestureSystem()
    system.run_integrated_system(camera_device="/dev/video0") 
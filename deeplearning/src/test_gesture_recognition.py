"""
í•™ìŠµ ë°ì´í„°ì—ì„œ ëœë¤ìœ¼ë¡œ 3ê°œ ì˜ìƒì„ ë½‘ì•„ì„œ ì œìŠ¤ì²˜ ì¸ì‹ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
ì‹œê°í™”ì™€ í•¨ê»˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from collections import deque
import time
import os
import random
import glob

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SimpleShiftGCN(nn.Module):
    """Shift-GCN ëª¨ë¸"""
    
    def __init__(self, num_classes, num_joints, in_channels=3, dropout=0.5):
        super(SimpleShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_joints = num_joints
        
        # Adjacency matrix
        self.register_buffer('A', torch.eye(num_joints))
        
        # ì…ë ¥ ì •ê·œí™”
        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)
        
        # 3ì¸µ êµ¬ì¡°
        self.conv1 = nn.Conv2d(in_channels, 32, 1)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 64, 1) 
        self.bn2 = nn.BatchNorm2d(64)
        
        self.conv3 = nn.Conv2d(64, 128, 1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Temporal convolution
        self.tcn = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))
        self.tcn_bn = nn.BatchNorm2d(128)
        
        # Classifier
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(128, num_classes)
        
    def set_adjacency_matrix(self, A):
        self.A = torch.FloatTensor(A)
        if next(self.parameters()).is_cuda:
            self.A = self.A.cuda()
    
    def forward(self, x):
        N, C, T, V, M = x.size()
        
        # Focus on first person
        x = x[:, :, :, :, 0]  # (N, C, T, V)
        
        # Data normalization
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, V, T)
        x = x.view(N, C * V, T)
        x = self.data_bn(x)
        x = x.view(N, C, V, T)
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, T, V)
        
        # Graph convolution layers
        # Layer 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Temporal convolution
        x = self.tcn(x)
        x = self.tcn_bn(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Global pooling
        x = F.avg_pool2d(x, x.size()[2:])  # (N, 128, 1, 1)
        x = x.view(N, -1)  # (N, 128)
        
        # Classification
        x = self.fc(x)
        
        return x

class TestGestureRecognizer:
    """ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ğŸš€ ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # YOLO Pose ëª¨ë¸
        pose_model_path = '/home/ckim/ros-repo-4/deeplearning/yolov8n-pose.pt'
        if os.path.exists(pose_model_path):
            self.pose_model = YOLO(pose_model_path)
            print(f"âœ… YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
            try:
                self.pose_model = YOLO('yolov8n-pose.pt')
                print("âš ï¸ ìƒëŒ€ ê²½ë¡œë¡œ YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ YOLO Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.pose_model = None
        
        # Shift-GCN ëª¨ë¸ ë¡œë“œ
        self.load_gesture_model()
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì •
        self.gesture_frame_buffer = deque(maxlen=30)  # 90 â†’ 30ìœ¼ë¡œ ì¤„ì„ (1ì´ˆ)
        self.actions = ['COME', 'NORMAL']
        self.upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]  # ì½”+ì–´ê¹¨+íŒ”+ì—‰ë©ì´
        
        # ì¸ì ‘ í–‰ë ¬ ìƒì„±
        self.adjacency_matrix = self.create_adjacency_matrix()
        if hasattr(self, 'gesture_model'):
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        
        print("âœ… ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_gesture_model(self):
        """Shift-GCN ëª¨ë¸ ë¡œë“œ"""
        model_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/models/simple_shift_gcn_model.pth'
        
        if os.path.exists(model_path):
            self.gesture_model = SimpleShiftGCN(num_classes=2, num_joints=9, dropout=0.5)
            self.gesture_model.load_state_dict(torch.load(model_path, map_location=device))
            self.gesture_model = self.gesture_model.to(device)
            self.gesture_model.eval()
            print(f"âœ… Shift-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            self.gesture_model = None
    
    def create_adjacency_matrix(self):
        """ì¸ì ‘ í–‰ë ¬ ìƒì„±"""
        num_joints = len(self.upper_body_joints)
        A = np.zeros((num_joints, num_joints))
        
        # self-connection (ëŒ€ê°ì„ )
        for i in range(num_joints):
            A[i, i] = 1
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜
        connections = [
            (0, 1), (0, 2),  # nose - shoulders
            (1, 2),  # shoulders
            (1, 3), (2, 4),  # shoulders - elbows
            (3, 5), (4, 6),  # elbows - wrists
            (1, 7), (2, 8),  # shoulders - hips
            (7, 8),  # hips
        ]
        
        # ë§¤í•‘: ì›ë³¸ ê´€ì ˆì  â†’ ìƒì²´ ë°°ì—´ ì¸ë±ìŠ¤
        joint_mapping = {joint: i for i, joint in enumerate(self.upper_body_joints)}
        
        for joint1, joint2 in connections:
            if joint1 in joint_mapping and joint2 in joint_mapping:
                i, j = joint_mapping[joint1], joint_mapping[joint2]
                A[i, j] = 1
                A[j, i] = 1
        
        return A
    
    def normalize_keypoints(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)"""
        if keypoints.shape[0] == 0:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš©
            valid_joints = frame_keypoints[:, 2] > 0.3
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· )
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€í¸ì°¨)
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš©
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                # ìœ íš¨í•œ ê´€ì ˆì ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def draw_keypoints(self, frame, keypoints, color=(0, 255, 0)):
        """í‚¤í¬ì¸íŠ¸ ì‹œê°í™”"""
        if keypoints is None:
            return frame
        
        annotated = frame.copy()
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜
        connections = [
            (0, 1), (0, 2),  # nose - shoulders
            (1, 2),  # shoulders
            (1, 3), (2, 4),  # shoulders - elbows
            (3, 5), (4, 6),  # elbows - wrists
            (1, 7), (2, 8),  # shoulders - hips
            (7, 8),  # hips
        ]
        
        # ê´€ì ˆì  ê·¸ë¦¬ê¸°
        for i, (x, y, conf) in enumerate(keypoints):
            if conf > 0.3:
                cv2.circle(annotated, (int(x), int(y)), 3, color, -1)
                cv2.putText(annotated, str(i), (int(x)+5, int(y)-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
        
        # ê´€ì ˆì  ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        for connection in connections:
            start_idx, end_idx = connection
            if (keypoints[start_idx][2] > 0.3 and keypoints[end_idx][2] > 0.3):
                start_point = (int(keypoints[start_idx][0]), int(keypoints[start_idx][1]))
                end_point = (int(keypoints[end_idx][0]), int(keypoints[end_idx][1]))
                cv2.line(annotated, start_point, end_point, color, 2)
        
        return annotated
    
    def process_video(self, video_path, expected_gesture):
        """ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ì œìŠ¤ì²˜ ì¸ì‹"""
        print(f"\nğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)}")
        print(f"   ì˜ˆìƒ ì œìŠ¤ì²˜: {expected_gesture}")
        print(f"   íŒŒì¼ ê²½ë¡œ: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None
        
        # ë¹„ë””ì˜¤ ì •ë³´ ì¶œë ¥
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"   ë¹„ë””ì˜¤ ì •ë³´: {frame_count_total}í”„ë ˆì„, {fps:.1f} FPS")
        
        frame_count = 0
        self.gesture_frame_buffer.clear()
        
        # ê²°ê³¼ ì €ì¥
        results = []
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # 30í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            if frame_count % 30 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘: {frame_count}/{frame_count_total} í”„ë ˆì„")
            
            # YOLO Poseë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
            results_pose = self.pose_model(frame, imgsz=256, conf=0.01, verbose=False, device=0)
            
            keypoints_detected = False
            current_keypoints = None
            
            for result in results_pose:
                if result.keypoints is not None:
                    keypoints = result.keypoints.data.cpu().numpy()  # (N, 17, 3)
                    
                    for person_idx, person_kpts in enumerate(keypoints):
                        # ìƒì²´ ê´€ì ˆì ë§Œ ì¶”ì¶œ
                        upper_body_kpts = person_kpts[self.upper_body_joints]  # (9, 3)
                        
                        # ì‹ ë¢°ë„ ì²´í¬
                        valid_joints = upper_body_kpts[:, 2] >= 0.01
                        valid_count = np.sum(valid_joints)
                        
                        if valid_count >= 4:  # ìµœì†Œ 4ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš”
                            keypoints_detected = True
                            current_keypoints = person_kpts  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ì¥
                            
                            # ì œìŠ¤ì²˜ ë¶„ì„ì„ ìœ„í•´ ë²„í¼ì— ì¶”ê°€
                            self.gesture_frame_buffer.append(upper_body_kpts)
                            break
                    
                    if keypoints_detected:
                        break
            
            # 90í”„ë ˆì„(3ì´ˆ) ë‹¨ìœ„ë¡œ ì œìŠ¤ì²˜ íŒë‹¨
            if len(self.gesture_frame_buffer) >= 30: # 30í”„ë ˆì„(1ì´ˆ) ë‹¨ìœ„ë¡œ íŒë‹¨
                print(f"   ğŸ¯ ì œìŠ¤ì²˜ íŒë‹¨ ì‹œì‘ (í”„ë ˆì„ {frame_count})")
                try:
                    # í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬
                    keypoints_sequence = list(self.gesture_frame_buffer)
                    keypoints_array = np.array(keypoints_sequence)  # (T, 9, 3)
                    
                    # í‚¤í¬ì¸íŠ¸ í’ˆì§ˆ ê²€ì¦
                    valid_frames = []
                    for i, frame_kpts in enumerate(keypoints_array):
                        valid_joints = frame_kpts[:, 2] >= 0.01
                        valid_count = np.sum(valid_joints)
                        if valid_count >= 4:
                            valid_frames.append(frame_kpts)
                    
                    print(f"   ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ í”„ë ˆì„: {len(valid_frames)}/30")
                    
                    if len(valid_frames) >= 30: # 30í”„ë ˆì„(1ì´ˆ) ë‹¨ìœ„ë¡œ íŒë‹¨
                        # ìœ íš¨í•œ í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©
                        keypoints_array = np.array(valid_frames)
                        
                        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬
                        T, V, C = keypoints_array.shape
                        target_frames = 30 # 30í”„ë ˆì„(1ì´ˆ)
                    
                        if T != target_frames:
                            old_indices = np.linspace(0, T-1, T)
                            new_indices = np.linspace(0, T-1, target_frames)
                            
                            resampled_keypoints = np.zeros((target_frames, V, C))
                            for v in range(V):
                                for c in range(C):
                                    resampled_keypoints[:, v, c] = np.interp(new_indices, old_indices, keypoints_array[:, v, c])
                            
                            keypoints_array = resampled_keypoints
                        
                        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì •ê·œí™” ì ìš©
                        normalized_keypoints = self.normalize_keypoints(keypoints_array)
                        
                        # Shift-GCN ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                        shift_gcn_data = np.zeros((C, target_frames, V, 1))
                        shift_gcn_data[:, :, :, 0] = normalized_keypoints.transpose(2, 0, 1)
                        
                        # ëª¨ë¸ ì˜ˆì¸¡
                        input_tensor = torch.FloatTensor(shift_gcn_data).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            output = self.gesture_model(input_tensor)
                            probabilities = F.softmax(output, dim=1)
                            prediction = torch.argmax(probabilities, dim=1).item()
                            confidence = probabilities[0, prediction].item()
                        
                        # ê²°ê³¼ í•´ì„
                        gesture_name = self.actions[prediction]
                        
                        results.append({
                            'frame': frame_count,
                            'gesture': gesture_name,
                            'confidence': confidence,
                            'expected': expected_gesture,
                            'correct': gesture_name == expected_gesture
                        })
                        
                        print(f"   í”„ë ˆì„ {frame_count}: {gesture_name} (ì‹ ë¢°ë„: {confidence:.3f}) - {'âœ…' if gesture_name == expected_gesture else 'âŒ'}")
                        
                        # ë²„í¼ ì´ˆê¸°í™”
                        self.gesture_frame_buffer.clear()
                    else:
                        print(f"   âš ï¸ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ í”„ë ˆì„ ë¶€ì¡±: {len(valid_frames)}/30")
                        self.gesture_frame_buffer.clear()
                        
                except Exception as e:
                    print(f"   âŒ ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
                    self.gesture_frame_buffer.clear()
            
            # ì‹œê°í™” (í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°)
            if keypoints_detected and current_keypoints is not None:
                annotated_frame = self.draw_keypoints(frame, current_keypoints)
                
                # ì œìŠ¤ì²˜ ì •ë³´ í‘œì‹œ
                if results:
                    latest_result = results[-1]
                    cv2.putText(annotated_frame, f"Gesture: {latest_result['gesture']}", 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                    cv2.putText(annotated_frame, f"Confidence: {latest_result['confidence']:.3f}", 
                               (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                    cv2.putText(annotated_frame, f"Expected: {latest_result['expected']}", 
                               (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
                
                cv2.imshow('Gesture Recognition Test', annotated_frame)
                if cv2.waitKey(30) & 0xFF == ord('q'):
                    break
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"   ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {frame_count}í”„ë ˆì„ ì²˜ë¦¬ë¨")
        
        # ê²°ê³¼ ìš”ì•½
        if results:
            correct_count = sum(1 for r in results if r['correct'])
            total_count = len(results)
            accuracy = correct_count / total_count if total_count > 0 else 0
            
            print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
            print(f"   ì´ íŒë‹¨ íšŸìˆ˜: {total_count}")
            print(f"   ì •í™•í•œ íŒë‹¨: {correct_count}")
            print(f"   ì •í™•ë„: {accuracy:.2%}")
            
            for i, result in enumerate(results):
                status = "âœ…" if result['correct'] else "âŒ"
                print(f"   {i+1}. í”„ë ˆì„ {result['frame']}: {result['gesture']} (ì‹ ë¢°ë„: {result['confidence']:.3f}) {status}")
        else:
            print(f"   âš ï¸ ì œìŠ¤ì²˜ íŒë‹¨ ê²°ê³¼ ì—†ìŒ")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¯ í•™ìŠµ ë°ì´í„° ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recognizer = TestGestureRecognizer()
    
    if recognizer.gesture_model is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    come_dir = "/home/ckim/ros-repo-4/deeplearning/gesture_recognition/pose_dataset/come"
    normal_dir = "/home/ckim/ros-repo-4/deeplearning/gesture_recognition/pose_dataset/normal"
    
    # ëœë¤ìœ¼ë¡œ 3ê°œ ì˜ìƒ ì„ íƒ
    come_videos = glob.glob(os.path.join(come_dir, "*.avi"))
    normal_videos = glob.glob(os.path.join(normal_dir, "*.avi"))
    
    # COME ì˜ìƒ 2ê°œ, NORMAL ì˜ìƒ 1ê°œ ì„ íƒ
    selected_videos = []
    selected_videos.extend(random.sample(come_videos, 2))
    selected_videos.extend(random.sample(normal_videos, 1))
    random.shuffle(selected_videos)  # ìˆœì„œ ì„ê¸°
    
    print(f"\nğŸ“ ì„ íƒëœ ì˜ìƒ:")
    for i, video_path in enumerate(selected_videos):
        gesture_type = "COME" if "come" in video_path else "NORMAL"
        print(f"   {i+1}. {os.path.basename(video_path)} ({gesture_type})")
    
    # ê° ì˜ìƒ ì²˜ë¦¬
    all_results = []
    for video_path in selected_videos:
        gesture_type = "COME" if "come" in video_path else "NORMAL"
        results = recognizer.process_video(video_path, gesture_type)
        if results:
            all_results.extend(results)
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if all_results:
        correct_count = sum(1 for r in all_results if r['correct'])
        total_count = len(all_results)
        overall_accuracy = correct_count / total_count if total_count > 0 else 0
        
        print(f"\nğŸ¯ ì „ì²´ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì´ íŒë‹¨ íšŸìˆ˜: {total_count}")
        print(f"   ì •í™•í•œ íŒë‹¨: {correct_count}")
        print(f"   ì „ì²´ ì •í™•ë„: {overall_accuracy:.2%}")
        
        # ì œìŠ¤ì²˜ë³„ ì •í™•ë„
        come_results = [r for r in all_results if r['expected'] == 'COME']
        normal_results = [r for r in all_results if r['expected'] == 'NORMAL']
        
        if come_results:
            come_accuracy = sum(1 for r in come_results if r['correct']) / len(come_results)
            print(f"   COME ì •í™•ë„: {come_accuracy:.2%} ({len(come_results)}ê°œ íŒë‹¨)")
        
        if normal_results:
            normal_accuracy = sum(1 for r in normal_results if r['correct']) / len(normal_results)
            print(f"   NORMAL ì •í™•ë„: {normal_accuracy:.2%} ({len(normal_results)}ê°œ íŒë‹¨)")

if __name__ == "__main__":
    main() 
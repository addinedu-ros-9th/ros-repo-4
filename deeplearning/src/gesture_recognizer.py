import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from collections import deque
import threading
import queue
import time
import os
import sys

# SlidingShiftGCN ëª¨ë¸ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/ckim/ros-repo-4/deeplearning/gesture_recognition/src')
from train_sliding_shift_gcn import SlidingShiftGCN

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class GestureRecognizer:
    """ì œìŠ¤ì²˜ ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ðŸš€ Gesture Recognizer ì´ˆê¸°í™”")
        
        # YOLO Pose ëª¨ë¸ (ê²½ë¡œ ìˆ˜ì •)
        pose_model_path = '/home/ckim/ros-repo-4/deeplearning/yolov8n-pose.pt'  # ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
        if os.path.exists(pose_model_path):
            try:
                self.pose_model = YOLO(pose_model_path)
                print(f"âœ… YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {pose_model_path}")
                
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_image = np.zeros((100, 100, 3), dtype=np.uint8)
                test_results = self.pose_model(test_image, verbose=False)
                print(f"âœ… YOLO Pose ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(test_results)} ê²°ê³¼")
                
            except Exception as e:
                print(f"âŒ YOLO Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.pose_model = None
        else:
            print(f"âŒ YOLO Pose ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {pose_model_path}")
            # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
            try:
                self.pose_model = YOLO('yolov8n-pose.pt')
                print("âš ï¸ ìƒëŒ€ ê²½ë¡œë¡œ YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ ìƒëŒ€ ê²½ë¡œ YOLO Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.pose_model = None
        
        # Shift-GCN ëª¨ë¸ ë¡œë“œ
        self.load_gesture_model()
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì • (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
        self.gesture_frame_buffer = deque(maxlen=30)  # 90 â†’ 30ìœ¼ë¡œ ë³€ê²½ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
        self.actions = ['COME', 'NORMAL']  # 2í´ëž˜ìŠ¤
        self.min_gesture_frames = 30  # 90 â†’ 30ìœ¼ë¡œ ë³€ê²½ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
        
        # 1ì´ˆ ë‹¨ìœ„ íŒë‹¨ ì„¤ì • (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
        self.gesture_decision_interval = 30  # 90 â†’ 30ìœ¼ë¡œ ë³€ê²½ (1ì´ˆë§ˆë‹¤ íŒë‹¨)
        self.last_gesture_decision_frame = 0
        self.current_gesture_confidence = 0.5
        
        # COME ì œìŠ¤ì²˜ ì¸ì‹ ê°œì„  ì„¤ì • (ìž„ê³„ê°’ ì¡°ì •)
        self.come_detection_threshold = 0.08  # 0.10 â†’ 0.08ë¡œ ë” ë‚®ì¶¤ (COME ê°ì§€ ê°œì„ )
        self.normal_detection_threshold = 0.30  # 0.35 â†’ 0.30ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        self.min_keypoints_for_gesture = 2  # 3 â†’ 2ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        
        # ì‹¤ì‹œê°„ ê°ì§€ ì„¤ì • (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶° í™œì„±í™”)
        self.realtime_detection_enabled = True  # ì‹¤ì‹œê°„ ê°ì§€ í™œì„±í™” (30í”„ë ˆìž„ í•™ìŠµ ëª¨ë¸ì— ë§žì¶¤)
        self.realtime_confidence_threshold = 0.20  # 0.25 â†’ 0.20ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        
        # ìƒì²´ ê´€ì ˆì  (YOLO Pose 17ê°œ ì¤‘ ìƒì²´ 9ê°œ)
        self.upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]  # ì½”+ì–´ê¹¨+íŒ”+ì—‰ë©ì´
        
        # ì¸ì ‘ í–‰ë ¬ ìƒì„±
        self.adjacency_matrix = self.create_adjacency_matrix()
        if hasattr(self, 'gesture_model'):
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        
        # ì“°ë ˆë“œ ì„¤ì •
        self.frame_queue = queue.Queue(maxsize=3)
        self.running = True
        self.lock = threading.Lock()
        
        # ê²°ê³¼ ì €ìž¥
        self.latest_gesture = ("NORMAL", self.current_gesture_confidence, False, None)
        self.last_valid_keypoints = None
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ìžë™ ì‹œìž‘
        self.worker_thread = threading.Thread(target=self.recognition_worker)
        self.worker_thread.start()
        print(f"âœ… ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œìž‘ë¨ (ìŠ¤ë ˆë“œ ID: {self.worker_thread.ident})")
        
        print("âœ… Gesture Recognizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start(self):
        """ì¸ì‹ê¸° ì‹œìž‘"""
        self.worker_thread = threading.Thread(target=self.recognition_worker)
        self.worker_thread.start()
        print("ðŸš€ Gesture Recognizer ì‹œìž‘")
    
    def stop(self):
        """ì¸ì‹ê¸° ì¤‘ì§€"""
        self.running = False
        try:
            self.frame_queue.put_nowait(None)
        except queue.Full:
            pass
        if hasattr(self, 'worker_thread'):
            self.worker_thread.join()
        print("ðŸ›‘ Gesture Recognizer ì¤‘ì§€")
    
    def load_gesture_model(self):
        """Shift-GCN ëª¨ë¸ ë¡œë“œ"""
        model_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/models/sliding_shift_gcn_model.pth'
        
        if os.path.exists(model_path):
            self.gesture_model = SlidingShiftGCN(num_classes=2, num_joints=9, dropout=0.3)
            self.gesture_model.load_state_dict(torch.load(model_path, map_location=device))
            self.gesture_model = self.gesture_model.to(device)
            self.gesture_model.eval()
            self.enable_gesture_recognition = True
            print(f"âœ… SlidingShift-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            
            # í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì¸ì ‘ í–‰ë ¬ ë¡œë“œ ì‹œë„
            adj_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/shift_gcn_data_sliding/adjacency_matrix.npy'
            if os.path.exists(adj_path):
                self.adjacency_matrix = np.load(adj_path)
                print(f"âœ… í•™ìŠµ ì‹œ ì¸ì ‘ í–‰ë ¬ ë¡œë“œ: {self.adjacency_matrix.shape}")
            else:
                print("âš ï¸ í•™ìŠµ ì‹œ ì¸ì ‘ í–‰ë ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.adjacency_matrix = self.create_adjacency_matrix()
            
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            self.enable_gesture_recognition = False
    
    def create_adjacency_matrix(self):
        """ì¸ì ‘ í–‰ë ¬ ìƒì„± (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)"""
        num_joints = len(self.upper_body_joints)
        A = np.zeros((num_joints, num_joints))
        
        # self-connection (ëŒ€ê°ì„ )
        for i in range(num_joints):
            A[i, i] = 1
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜ (í•™ìŠµ ì‹œì™€ ë™ì¼)
        connections = [
            (0, 1), (0, 2),  # nose - shoulders
            (1, 2),  # shoulders
            (1, 3), (2, 4),  # shoulders - elbows
            (3, 5), (4, 6),  # elbows - wrists
            (1, 7), (2, 8),  # shoulders - hips
            (7, 8),  # hips
        ]
        
        # ë§¤í•‘: ì›ë³¸ ê´€ì ˆì  â†’ ìƒì²´ ë°°ì—´ ì¸ë±ìŠ¤
        joint_mapping = {joint: i for i, joint in enumerate(self.upper_body_joints)}
        
        for joint1, joint2 in connections:
            if joint1 in joint_mapping and joint2 in joint_mapping:
                i, j = joint_mapping[joint1], joint_mapping[joint2]
                A[i, j] = 1
                A[j, i] = 1
        
        return A
    
    def normalize_keypoints(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ì™„ì „ížˆ ë™ì¼í•œ ë°©ì‹)"""
        if keypoints.shape[0] == 0:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ìž„ê³„ê°’)
            valid_joints = frame_keypoints[:, 2] > 0.3  # 0.1 â†’ 0.3ìœ¼ë¡œ ë³µì› (í•™ìŠµ ì‹œì™€ ë™ì¼)
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· ) - í•™ìŠµ ì‹œì™€ ë™ì¼
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€íŽ¸ì°¨) - í•™ìŠµ ì‹œì™€ ë™ì¼
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                # ìœ íš¨í•œ ê´€ì ˆì ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_with_training_stats(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ë°ì´í„° í†µê³„ ì‚¬ìš©)"""
        if keypoints is None:
            return keypoints
        
        # í•™ìŠµ ë°ì´í„° í†µê³„ (ì‹¤ì œ í•™ìŠµëœ ë°ì´í„° ê¸°ë°˜)
        # COME: í‰ê· =0.315, í‘œì¤€íŽ¸ì°¨=0.798, ë²”ìœ„=[-2.396, 2.119]
        # NORMAL: í‰ê· =0.307, í‘œì¤€íŽ¸ì°¨=0.699, ë²”ìœ„=[-2.566, 1.955]
        
        # í•™ìŠµ ë°ì´í„°ì˜ ì „ì²´ í†µê³„
        training_mean = 0.311  # (0.315 + 0.307) / 2
        training_std = 0.749   # (0.798 + 0.699) / 2
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš©
            valid_joints = frame_keypoints[:, 2] > 0.3
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚°
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚°
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš©
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                
                # í•™ìŠµ ë°ì´í„° í†µê³„ì— ë§žì¶° ì¶”ê°€ ì •ê·œí™”
                normalized_keypoints[t, :, 0] = (normalized_keypoints[t, :, 0] - training_mean) / training_std
                normalized_keypoints[t, :, 1] = (normalized_keypoints[t, :, 1] - training_mean) / training_std
            else:
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_exact_training(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ì™„ì „ížˆ ë™ì¼)"""
        if keypoints is None:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ìž„ê³„ê°’)
            valid_joints = frame_keypoints[:, 2] > 0.3  # 0.1 â†’ 0.3ìœ¼ë¡œ ë³µì› (í•™ìŠµ ì‹œì™€ ë™ì¼)
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· ) - í•™ìŠµ ì‹œì™€ ë™ì¼
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€íŽ¸ì°¨) - í•™ìŠµ ì‹œì™€ ë™ì¼
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                # ìœ íš¨í•œ ê´€ì ˆì ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_distance_compensated(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (ê±°ë¦¬ ì°¨ì´ ë³´ìƒ)"""
        if keypoints is None:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš©
            valid_joints = frame_keypoints[:, 2] > 0.3
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚°
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ê±°ë¦¬ ë³´ìƒ ì ìš©)
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ê±°ë¦¬ ë³´ìƒ: í˜„ìž¬ ìŠ¤ì¼€ì¼ì„ í•™ìŠµ ì‹œ ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •
                # í•™ìŠµ ì‹œ í‰ê·  ìŠ¤ì¼€ì¼: ì•½ 0.8, í˜„ìž¬ ìŠ¤ì¼€ì¼: ì•½ 170
                # ë³´ìƒ ê³„ìˆ˜ = í•™ìŠµ_ìŠ¤ì¼€ì¼ / í˜„ìž¬_ìŠ¤ì¼€ì¼ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
                if scale > 50:  # ìŠ¤ì¼€ì¼ì´ ë§¤ìš° í° ê²½ìš°ë§Œ ë³´ìƒ
                    compensation_factor = min(0.8 / scale, 0.1)  # ìµœëŒ€ 0.1ë¡œ ì œí•œ
                elif scale > 10:  # ì¤‘ê°„ ìŠ¤ì¼€ì¼
                    compensation_factor = min(0.8 / scale, 0.3)  # ìµœëŒ€ 0.3ìœ¼ë¡œ ì œí•œ
                else:  # ìž‘ì€ ìŠ¤ì¼€ì¼
                    compensation_factor = 1.0  # ë³´ìƒ ì—†ìŒ
                
                adjusted_scale = scale * compensation_factor
                
                # ì •ê·œí™” ì ìš© (ê±°ë¦¬ ë³´ìƒ í¬í•¨)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / adjusted_scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / adjusted_scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def draw_visualization(self, frame, keypoints, prediction, confidence):
        """ê´€ì ˆì  ì‹œê°í™” (predict_webcam_realtime.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
        if keypoints is not None:
            # ê´€ì ˆì  ì—°ê²°ì„  ì •ì˜ (predict_webcam_realtime.pyì™€ ë™ì¼)
            connections = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6), (1, 7), (2, 8), (7, 8)]
            for i, j in connections:
                idx1, idx2 = self.upper_body_joints[i], self.upper_body_joints[j]
                if keypoints[idx1, 2] > 0.3 and keypoints[idx2, 2] > 0.3:
                    p1 = (int(keypoints[idx1, 0]), int(keypoints[idx1, 1]))
                    p2 = (int(keypoints[idx2, 0]), int(keypoints[idx2, 1]))
                    cv2.line(frame, p1, p2, (255, 255, 255), 2)
        
            # ê´€ì ˆì  ê·¸ë¦¬ê¸° (predict_webcam_realtime.pyì™€ ë™ì¼)
            for idx in self.upper_body_joints:
                if keypoints[idx, 2] > 0.3:
                    center = (int(keypoints[idx, 0]), int(keypoints[idx, 1]))
                    cv2.circle(frame, center, 5, (0, 255, 255), -1)

        # ì œìŠ¤ì²˜ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ (COME: ë¹¨ê°„ìƒ‰, NORMAL: ì´ˆë¡ìƒ‰)
        if prediction is None:
            text = f"Collecting frames... [{len(self.gesture_frame_buffer)}/{self.min_gesture_frames}]"
            cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            label = prediction  # ì´ë¯¸ ë¬¸ìžì—´ë¡œ ë˜ì–´ ìžˆìŒ
            # COME: ë¹¨ê°„ìƒ‰, NORMAL: ì´ˆë¡ìƒ‰ (BGR ìƒ‰ìƒ)
            if label == "COME":
                color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
            else:  # NORMAL
                color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
            
            text = f"{label}: {confidence:.2f}"
            cv2.rectangle(frame, (10, 10), (350, 60), color, -1)
            cv2.putText(frame, text, (20, 45), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
        
        return frame
    
    def recognition_worker(self):
        """ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ (ê°€ìž¥ í° ì‚¬ëžŒì˜ poseë§Œ ê°ì§€)"""
        print("ðŸ”„ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œìž‘ë¨")
        frame_count = 0
        
        while self.running:
            try:
                frame_data = self.frame_queue.get(timeout=1.0)
                if frame_data is None:
                    print("ðŸ›‘ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                    break
                
                frame, frame_id, elapsed_time, latest_detections = frame_data
                frame_count += 1
                
                # í”„ë ˆìž„ ìˆ˜ì‹  í™•ì¸ (60í”„ë ˆìž„ë§ˆë‹¤)
                if frame_count % 60 == 0:
                    print(f"ðŸ”„ ì›Œì»¤ ìŠ¤ë ˆë“œ: í”„ë ˆìž„ {frame_id} ì²˜ë¦¬ ì¤‘ (ì´ {frame_count}ê°œ ì²˜ë¦¬ë¨)")
                    print(f"   - í”„ë ˆìž„ í¬ê¸°: {frame.shape}")
                    print(f"   - ê°ì§€ëœ ì‚¬ëžŒ: {len(latest_detections)}ëª…")
                    print(f"   - ì œìŠ¤ì²˜ ì¸ì‹ í™œì„±í™”: {self.enable_gesture_recognition}")
                
                if not self.enable_gesture_recognition:
                    self.frame_queue.task_done()
                    continue
                
                # ê¸°ë³¸ê°’ ìœ ì§€ (ìƒˆë¡œìš´ íŒë‹¨ì´ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€)
                with self.lock:
                    prediction, confidence, keypoints_detected, current_keypoints = self.latest_gesture
        
                # ê°€ìž¥ í° ì‚¬ëžŒ(ì£¼ìš” ëŒ€ìƒ) ì°¾ê¸°
                target_person = None
                if latest_detections:
                    # ë©´ì ì´ ê°€ìž¥ í° ì‚¬ëžŒ ì„ íƒ
                    target_person = max(latest_detections, key=lambda p: 
                        (p['bbox'][2] - p['bbox'][0]) * (p['bbox'][3] - p['bbox'][1]))
                    
                    if frame_id % 60 == 0:  # 60í”„ë ˆìž„ë§ˆë‹¤ ë””ë²„ê¹…
                        area = (target_person['bbox'][2] - target_person['bbox'][0]) * (target_person['bbox'][3] - target_person['bbox'][1])
                        print(f"ðŸŽ¯ ì£¼ìš” ëŒ€ìƒ: {target_person['id']} (ë©´ì : {area:.0f})")
                        print(f"   - ë°”ìš´ë”© ë°•ìŠ¤: {target_person['bbox']}")
                else:
                    if frame_id % 60 == 0:
                        print(f"ðŸŽ¯ ê°ì§€ëœ ì‚¬ëžŒ ì—†ìŒ")
                
                # YOLO Poseë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê°€ìž¥ í° ì‚¬ëžŒë§Œ)
                if target_person is not None:
                    # ì£¼ìš” ëŒ€ìƒì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ROI ì„¤ì •
                    x1, y1, x2, y2 = map(int, target_person['bbox'])
                    
                    # ROI í™•ìž¥ (ë” ë„“ì€ ì˜ì—­ì—ì„œ pose ê°ì§€)
                    margin = 100  # 50 â†’ 100ìœ¼ë¡œ ëŠ˜ë¦¼
                    x1 = max(0, x1 - margin)
                    y1 = max(0, y1 - margin)
                    x2 = min(frame.shape[1], x2 + margin)
                    y2 = min(frame.shape[0], y2 + margin)
                    
                    if frame_id % 60 == 0:
                        print(f"   - ROI ì„¤ì •: [{x1}, {y1}, {x2}, {y2}] (ë§ˆì§„: {margin})")
                    
                    # ROI ì¶”ì¶œ
                    roi = frame[y1:y2, x1:x2]
                    
                    if roi.size > 0:  # ROIê°€ ìœ íš¨í•œ ê²½ìš°
                        if frame_id % 60 == 0:
                            print(f"   - ROI í¬ê¸°: {roi.shape}")
                        
                        # ROIì—ì„œ pose ê°ì§€ (ì„¤ì • ê°œì„ )
                        results = self.pose_model(roi, imgsz=256, conf=0.01,  # 0.05 â†’ 0.01ë¡œ ë” ë‚®ì¶¤
                                                verbose=False, device=0)
                        
                        keypoints_data = []
                        
                        for result in results:
                            if result.keypoints is not None:
                                keypoints = result.keypoints.data.cpu().numpy()  # (N, 17, 3)
                                
                                if frame_id % 60 == 0:  # 60í”„ë ˆìž„ë§ˆë‹¤ ë””ë²„ê¹…
                                    print(f"   ðŸ” YOLO Pose ê²°ê³¼: {len(keypoints)}ëª… ê°ì§€")
                                
                                for person_idx, person_kpts in enumerate(keypoints):
                                    # ìƒì²´ ê´€ì ˆì ë§Œ ì¶”ì¶œ
                                    upper_body_kpts = person_kpts[self.upper_body_joints]  # (9, 3)
                                    
                                    # ì‹ ë¢°ë„ ì²´í¬ (ìž„ê³„ê°’ ë‚®ì¶¤)
                                    valid_joints = upper_body_kpts[:, 2] >= 0.01  # 0.05 â†’ 0.01ë¡œ ë” ë‚®ì¶¤
                                    valid_count = np.sum(valid_joints)
                                    
                                    if frame_id % 60 == 0:
                                        print(f"   ì£¼ìš” ëŒ€ìƒ {target_person['id']}: {valid_count}/9 í‚¤í¬ì¸íŠ¸")
                                        print(f"      - ì–´ê¹¨: {person_kpts[5][2]:.2f}/{person_kpts[6][2]:.2f}")
                                        print(f"      - íŒ”ê¿ˆì¹˜: {person_kpts[7][2]:.2f}/{person_kpts[8][2]:.2f}")
                                        print(f"      - ì†ëª©: {person_kpts[9][2]:.2f}/{person_kpts[10][2]:.2f}")
                                    
                                    if valid_count >= self.min_keypoints_for_gesture:  # 3 â†’ 2ë¡œ ë‚®ì¶¤
                                        # ROI ì¢Œí‘œë¥¼ ì „ì²´ í”„ë ˆìž„ ì¢Œí‘œë¡œ ë³€í™˜
                                        person_kpts[:, 0] += x1
                                        person_kpts[:, 1] += y1
                                        
                                        keypoints_data.append(upper_body_kpts)
                                        keypoints_detected = True
                                        current_keypoints = person_kpts  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ìž¥ (ì‹œê°í™”ìš©)
                                        
                                        if frame_id % 60 == 0:
                                            print(f"   âœ… í‚¤í¬ì¸íŠ¸ ê°ì§€ ì„±ê³µ! {valid_count}ê°œ ìœ íš¨")
                                        break  # ì²« ë²ˆì§¸ ì‚¬ëžŒë§Œ ì‚¬ìš©
                                    else:
                                        if frame_id % 60 == 0:
                                            print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±: {valid_count}/{self.min_keypoints_for_gesture} (ìž„ê³„ê°’ ë¯¸ë‹¬)")
                            else:
                                if frame_id % 60 == 0:
                                    print(f"   âŒ YOLO Pose ê²°ê³¼ì— í‚¤í¬ì¸íŠ¸ ì—†ìŒ")
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ë” ìžì„¸í•œ ë””ë²„ê¹…
                        if len(keypoints_data) == 0 and frame_id % 60 == 0:
                            print(f"   âŒ í‚¤í¬ì¸íŠ¸ ê°ì§€ ì‹¤íŒ¨ - ROI í¬ê¸°: {roi.shape}")
                            print(f"      - ë°”ìš´ë”© ë°•ìŠ¤: [{x1}, {y1}, {x2}, {y2}]")
                            print(f"      - ROI í¬ê¸°: {roi.size}")
                            print(f"      - YOLO Pose ê²°ê³¼ ê°œìˆ˜: {len(results)}")
                            if len(results) > 0:
                                print(f"      - ì²« ë²ˆì§¸ ê²°ê³¼ í‚¤í¬ì¸íŠ¸: {results[0].keypoints is not None}")
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ë©´ ì²˜ë¦¬
                        if len(keypoints_data) > 0:
                            current_keypoints_data = keypoints_data[0]
                            keypoints_detected = True
                            current_keypoints = current_keypoints  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ìž¥ (ì‹œê°í™”ìš©)
                        else:
                            # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì œìŠ¤ì²˜ íŒë‹¨ ì¤‘ë‹¨
                            if frame_id % 60 == 0:
                                print(f"   âš ï¸ ì£¼ìš” ëŒ€ìƒ í‚¤í¬ì¸íŠ¸ ë¶€ì¡± - ì œìŠ¤ì²˜ íŒë‹¨ ì¤‘ë‹¨")
                            keypoints_detected = False
                            current_keypoints = None
                            # ì´ì „ í”„ë ˆìž„ ë°ì´í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                            current_keypoints_data = None
                        
                        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ ì €ìž¥
                        if len(keypoints_data) > 0:
                            self.last_valid_keypoints = keypoints_data[0]
                    else:
                        # ROIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
                        if frame_id % 60 == 0:
                            print(f"   âŒ ROIê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - í¬ê¸°: {roi.size}")
                        keypoints_detected = False
                        current_keypoints = None
                        current_keypoints_data = None
                else:
                    # ê°ì§€ëœ ì‚¬ëžŒì´ ì—†ëŠ” ê²½ìš°
                    if frame_id % 60 == 0:
                        print(f"   âŒ ê°ì§€ëœ ì‚¬ëžŒ ì—†ìŒ - í‚¤í¬ì¸íŠ¸ ê°ì§€ ë¶ˆê°€")
                    keypoints_detected = False
                    current_keypoints = None
                    current_keypoints_data = None
                
                # ì œìŠ¤ì²˜ ë¶„ì„ (í‚¤í¬ì¸íŠ¸ê°€ ì¶©ë¶„ížˆ ê°ì§€ëœ ê²½ìš°ì—ë§Œ)
                if keypoints_detected and current_keypoints_data is not None:
                    # ìµœì†Œ 4ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ ìžˆì–´ì•¼ ì œìŠ¤ì²˜ íŒë‹¨ (6 â†’ 4ë¡œ ë‚®ì¶¤)
                    valid_joints = current_keypoints_data[:, 2] >= 0.05  # 0.1 â†’ 0.05ë¡œ ë‚®ì¶¤
                    valid_count = np.sum(valid_joints)
                    
                    if valid_count >= 4:  # ìµœì†Œ 4ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš” (6 â†’ 4ë¡œ ë‚®ì¶¤)
                        self.gesture_frame_buffer.append(current_keypoints_data)
                        
                        if frame_id % 60 == 0:
                            print(f"   âœ… ì œìŠ¤ì²˜ ë¶„ì„ ì§„í–‰: {valid_count}/9 í‚¤í¬ì¸íŠ¸")
                    else:
                        if frame_id % 60 == 0:
                            print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ì œìŠ¤ì²˜ ë¶„ì„ ì¤‘ë‹¨: {valid_count}/4 (ìµœì†Œ í•„ìš”)")
                else:
                    if frame_id % 60 == 0:
                        print(f"   âŒ í‚¤í¬ì¸íŠ¸ ì—†ìŒìœ¼ë¡œ ì œìŠ¤ì²˜ ë¶„ì„ ì¤‘ë‹¨")
                
                # ì‹¤ì‹œê°„ ê°ì§€ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶° í™œì„±í™”)
                if (self.realtime_detection_enabled and 
                    len(self.gesture_frame_buffer) >= 30 and
                    keypoints_detected and current_keypoints_data is not None):
                    
                    print(f"ðŸŽ¯ [Frame {frame_id}] ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ íŒë‹¨ ì‹œìž‘!")
                    
                    try:
                        # í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                        keypoints_sequence = list(self.gesture_frame_buffer)
                        keypoints_array = np.array(keypoints_sequence)  # (T, 9, 3)
                        
                        # í‚¤í¬ì¸íŠ¸ í’ˆì§ˆ ê²€ì¦
                        valid_frames = []
                        for i, frame_kpts in enumerate(keypoints_array):
                            valid_joints = frame_kpts[:, 2] >= 0.01
                            valid_count = np.sum(valid_joints)
                            if valid_count >= 4:
                                valid_frames.append(frame_kpts)
                        
                        if len(valid_frames) < 30:  # ìµœì†Œ 30í”„ë ˆìž„ í•„ìš” (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                            print(f"   âš ï¸ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ í”„ë ˆìž„ ë¶€ì¡±: {len(valid_frames)}/30")
                            self.frame_queue.task_done()
                            continue
                        
                        # ìœ íš¨í•œ í”„ë ˆìž„ë“¤ë§Œ ì‚¬ìš©
                        keypoints_array = np.array(valid_frames)
                        
                        # SlidingShiftGCN ëª¨ë¸ì— ë§žëŠ” ì „ì²˜ë¦¬
                        T, V, C = keypoints_array.shape
                        target_frames = 30  # SlidingShiftGCN ëª¨ë¸ì€ 30í”„ë ˆìž„
                    
                        if T != target_frames:
                            old_indices = np.linspace(0, T-1, T)
                            new_indices = np.linspace(0, T-1, target_frames)
                            
                            resampled_keypoints = np.zeros((target_frames, V, C))
                            for v in range(V):
                                for c in range(C):
                                    resampled_keypoints[:, v, c] = np.interp(new_indices, old_indices, keypoints_array[:, v, c])
                            
                            keypoints_array = resampled_keypoints
                        
                        # ì •ê·œí™” ì ìš© (ì¤‘ì‹¬ì  ê¸°ë°˜)
                        normalized_keypoints = self.normalize_keypoints(keypoints_array)
                        
                        # SlidingShiftGCN ìž…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                        shift_gcn_data = np.zeros((C, target_frames, V, 1))
                        shift_gcn_data[:, :, :, 0] = normalized_keypoints.transpose(2, 0, 1)
                        
                        # ëª¨ë¸ ì˜ˆì¸¡
                        input_tensor = torch.FloatTensor(shift_gcn_data).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            self.gesture_model.eval()
                            output = self.gesture_model(input_tensor)
                            probabilities = F.softmax(output, dim=1)
                            prediction = torch.argmax(probabilities, dim=1).item()
                            confidence = probabilities[0, prediction].item()
                        
                        # ê²°ê³¼ í•´ì„
                        gesture_name = self.actions[prediction]
                        
                        # ì‹¤ì‹œê°„ íŒë‹¨ ê²°ê³¼ ì—…ë°ì´íŠ¸
                        with self.lock:
                            self.latest_gesture = (gesture_name, confidence, keypoints_detected, current_keypoints)
                            self.current_gesture_confidence = confidence
                        
                        print(f"ðŸŽ¯ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ê²°ê³¼: {gesture_name} (ì‹ ë¢°ë„: {confidence:.3f})")
                        
                        # ë²„í¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì‹¤ì‹œê°„ êµ¬ê°„ ì‹œìž‘)
                        self.gesture_frame_buffer.clear()
                        
                    except Exception as e:
                        print(f"âŒ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
                
                # 1ì´ˆ(30í”„ë ˆìž„) ë‹¨ìœ„ë¡œ ì •ê¸° íŒë‹¨ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                frames_since_last_decision = frame_id - self.last_gesture_decision_frame
                
                if (len(self.gesture_frame_buffer) >= self.min_gesture_frames and 
                    frames_since_last_decision >= self.gesture_decision_interval):
                    
                    print(f"ðŸŽ¯ [Frame {frame_id}] 1ì´ˆ ë‹¨ìœ„ ì œìŠ¤ì²˜ íŒë‹¨ ì‹œìž‘!")
                    
                    try:
                        # í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬ (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                        keypoints_sequence = list(self.gesture_frame_buffer)
                        keypoints_array = np.array(keypoints_sequence)  # (T, 9, 3)
                        
                        # í‚¤í¬ì¸íŠ¸ í’ˆì§ˆ ê²€ì¦
                        valid_frames = []
                        for i, frame_kpts in enumerate(keypoints_array):
                            valid_joints = frame_kpts[:, 2] >= 0.01
                            valid_count = np.sum(valid_joints)
                            if valid_count >= 4:
                                valid_frames.append(frame_kpts)
                        
                        if len(valid_frames) < 30:  # ìµœì†Œ 30í”„ë ˆìž„ í•„ìš” (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                            print(f"   âš ï¸ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ í”„ë ˆìž„ ë¶€ì¡±: {len(valid_frames)}/30")
                            self.frame_queue.task_done()
                            continue
                        
                        # ìœ íš¨í•œ í”„ë ˆìž„ë“¤ë§Œ ì‚¬ìš©
                        keypoints_array = np.array(valid_frames)
                        
                        # SlidingShiftGCN ëª¨ë¸ì— ë§žëŠ” ì „ì²˜ë¦¬
                        T, V, C = keypoints_array.shape
                        target_frames = 30  # SlidingShiftGCN ëª¨ë¸ì€ 30í”„ë ˆìž„
                    
                        if T != target_frames:
                            old_indices = np.linspace(0, T-1, T)
                            new_indices = np.linspace(0, T-1, target_frames)
                            
                            resampled_keypoints = np.zeros((target_frames, V, C))
                            for v in range(V):
                                for c in range(C):
                                    resampled_keypoints[:, v, c] = np.interp(new_indices, old_indices, keypoints_array[:, v, c])
                            
                            keypoints_array = resampled_keypoints
                        
                        # ì •ê·œí™” ì ìš© (ì¤‘ì‹¬ì  ê¸°ë°˜)
                        normalized_keypoints = self.normalize_keypoints(keypoints_array)
                        
                        # SlidingShiftGCN ìž…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                        shift_gcn_data = np.zeros((C, target_frames, V, 1))
                        shift_gcn_data[:, :, :, 0] = normalized_keypoints.transpose(2, 0, 1)
                        
                        # ëª¨ë¸ ì˜ˆì¸¡
                        input_tensor = torch.FloatTensor(shift_gcn_data).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            self.gesture_model.eval()
                            output = self.gesture_model(input_tensor)
                            probabilities = F.softmax(output, dim=1)
                            prediction = torch.argmax(probabilities, dim=1).item()
                            confidence = probabilities[0, prediction].item()
                        
                        # ê²°ê³¼ í•´ì„
                        gesture_name = self.actions[prediction]
                        
                        # ì •ê¸° íŒë‹¨ ê²°ê³¼ ì—…ë°ì´íŠ¸
                        with self.lock:
                            self.latest_gesture = (gesture_name, confidence, keypoints_detected, current_keypoints)
                            self.current_gesture_confidence = confidence
                        
                        self.last_gesture_decision_frame = frame_id
                        
                        print(f"ðŸŽ¯ 1ì´ˆ ì œìŠ¤ì²˜ ê²°ê³¼: {gesture_name} (ì‹ ë¢°ë„: {confidence:.3f})")
                        
                        # ë²„í¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ 1ì´ˆ êµ¬ê°„ ì‹œìž‘)
                        self.gesture_frame_buffer.clear()
                        
                    except Exception as e:
                        print(f"âŒ 1ì´ˆ ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
                
                # ê²°ê³¼ ì €ìž¥ (í‚¤í¬ì¸íŠ¸ ì •ë³´ í¬í•¨)
                # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if keypoints_detected and current_keypoints is not None:
                    # ì‹¤ì‹œê°„ ê°ì§€ì™€ 1ì´ˆ ë‹¨ìœ„ íŒë‹¨ ê²°ê³¼ ëª¨ë‘ ì‚¬ìš© (SlidingShiftGCN ëª¨ë¸ì— ë§žì¶¤)
                    # predictionì´ ì´ë¯¸ ë¬¸ìžì—´ì¸ì§€ í™•ì¸
                    if isinstance(prediction, str):
                        prediction_label = prediction
                    else:
                        prediction_label = self.actions[prediction] if prediction is not None else "NORMAL"
                    with self.lock:
                        self.latest_gesture = (prediction_label, confidence, True, current_keypoints)
                # í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€ (ì—…ë°ì´íŠ¸ ì•ˆ í•¨)
                elif not keypoints_detected and frame_id % 120 == 0:
                    # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì„ ë•Œ ë””ë²„ê¹…
                    print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ì—†ìŒ - ì´ì „ ê²°ê³¼ ìœ ì§€: {self.latest_gesture[0]} ({self.latest_gesture[1]:.3f})")
                
                self.frame_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                break
    
    def get_latest_gesture(self):
        """ìµœì‹  ì œìŠ¤ì²˜ ê²°ê³¼ ë°˜í™˜"""
        with self.lock:
            return self.latest_gesture
    
    def add_frame(self, frame, frame_id, elapsed_time, latest_detections):
        """í”„ë ˆìž„ ì¶”ê°€ (ë¹„ë™ê¸°)"""
        try:
            self.frame_queue.put_nowait((frame, frame_id, elapsed_time, latest_detections))
            
            # í”„ë ˆìž„ ì „ë‹¬ í™•ì¸ (120í”„ë ˆìž„ë§ˆë‹¤)
            if frame_id % 120 == 0:
                print(f"ðŸ“¤ ì œìŠ¤ì²˜ ì¸ì‹ê¸°ì— í”„ë ˆìž„ {frame_id} ì „ë‹¬ë¨")
                print(f"   - í í¬ê¸°: {self.frame_queue.qsize()}")
                print(f"   - ê°ì§€ëœ ì‚¬ëžŒ: {len(latest_detections)}ëª…")
                
        except queue.Full:
            if frame_id % 120 == 0:
                print(f"âš ï¸ ì œìŠ¤ì²˜ ì¸ì‹ê¸° íê°€ ê°€ë“ì°¸ - í”„ë ˆìž„ {frame_id} ê±´ë„ˆëœ€")
        except Exception as e:
            if frame_id % 120 == 0:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ê¸° í”„ë ˆìž„ ì „ë‹¬ ì˜¤ë¥˜: {e}") 
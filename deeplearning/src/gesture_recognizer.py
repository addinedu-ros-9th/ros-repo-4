import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from collections import deque
import threading
import queue
import time
import os

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SimpleShiftGCN(nn.Module):
    """Shift-GCN ëª¨ë¸"""
    
    def __init__(self, num_classes, num_joints, in_channels=3, dropout=0.5):
        super(SimpleShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_joints = num_joints
        
        # Adjacency matrix
        self.register_buffer('A', torch.eye(num_joints))
        
        # ì…ë ¥ ì •ê·œí™”
        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)
        
        # 3ì¸µ êµ¬ì¡°
        self.conv1 = nn.Conv2d(in_channels, 32, 1)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 64, 1) 
        self.bn2 = nn.BatchNorm2d(64)
        
        self.conv3 = nn.Conv2d(64, 128, 1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Temporal convolution
        self.tcn = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))
        self.tcn_bn = nn.BatchNorm2d(128)
        
        # Classifier
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(128, num_classes)
        
    def set_adjacency_matrix(self, A):
        self.A = torch.FloatTensor(A)
        if next(self.parameters()).is_cuda:
            self.A = self.A.cuda()
    
    def forward(self, x):
        N, C, T, V, M = x.size()
        
        # Focus on first person
        x = x[:, :, :, :, 0]  # (N, C, T, V)
        
        # Data normalization
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, V, T)
        x = x.view(N, C * V, T)
        x = self.data_bn(x)
        x = x.view(N, C, V, T)
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, T, V)
        
        # Graph convolution layers
        # Layer 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Temporal convolution
        x = self.tcn(x)
        x = self.tcn_bn(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Global pooling
        x = F.avg_pool2d(x, x.size()[2:])  # (N, 128, 1, 1)
        x = x.view(N, -1)  # (N, 128)
        
        # Classification
        x = self.fc(x)
        
        return x

class GestureRecognizer:
    """ì œìŠ¤ì²˜ ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ğŸš€ Gesture Recognizer ì´ˆê¸°í™”")
        
        # YOLO Pose ëª¨ë¸ (ê²½ë¡œ ìˆ˜ì •)
        pose_model_path = '/home/ckim/ros-repo-4/deeplearning/yolov8n-pose.pt'  # ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
        if os.path.exists(pose_model_path):
            try:
                self.pose_model = YOLO(pose_model_path)
                print(f"âœ… YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {pose_model_path}")
                
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_image = np.zeros((100, 100, 3), dtype=np.uint8)
                test_results = self.pose_model(test_image, verbose=False)
                print(f"âœ… YOLO Pose ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(test_results)} ê²°ê³¼")
                
            except Exception as e:
                print(f"âŒ YOLO Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.pose_model = None
        else:
            print(f"âŒ YOLO Pose ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {pose_model_path}")
            # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
            try:
                self.pose_model = YOLO('yolov8n-pose.pt')
                print("âš ï¸ ìƒëŒ€ ê²½ë¡œë¡œ YOLO Pose ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ ìƒëŒ€ ê²½ë¡œ YOLO Pose ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.pose_model = None
        
        # Shift-GCN ëª¨ë¸ ë¡œë“œ
        self.load_gesture_model()
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì • (í•™ìŠµ ì‹œì™€ ì™„ì „íˆ ë™ì¼)
        self.gesture_frame_buffer = deque(maxlen=90)  # 90 í”„ë ˆì„ (í•™ìŠµ ì‹œì™€ ë™ì¼)
        self.actions = ['COME', 'NORMAL']  # 2í´ë˜ìŠ¤
        self.min_gesture_frames = 90  # 30 â†’ 90ìœ¼ë¡œ ë³µì› (í•™ìŠµ ì‹œì™€ ë™ì¼)
        
        # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼)
        self.gesture_decision_interval = 90  # 30 â†’ 90ìœ¼ë¡œ ë³µì› (3ì´ˆë§ˆë‹¤ íŒë‹¨)
        self.last_gesture_decision_frame = 0
        self.current_gesture_confidence = 0.5
        
        # COME ì œìŠ¤ì²˜ ì¸ì‹ ê°œì„  ì„¤ì • (ì„ê³„ê°’ ì¡°ì •)
        self.come_detection_threshold = 0.08  # 0.10 â†’ 0.08ë¡œ ë” ë‚®ì¶¤ (COME ê°ì§€ ê°œì„ )
        self.normal_detection_threshold = 0.30  # 0.35 â†’ 0.30ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        self.min_keypoints_for_gesture = 2  # 3 â†’ 2ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        
        # ì‹¤ì‹œê°„ ê°ì§€ ì„¤ì • (ë¹„í™œì„±í™” - í•™ìŠµ ì‹œ 90í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµë¨)
        # ì‹¤ì‹œê°„ ê°ì§€ëŠ” 30í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì— ë¶€ì ì ˆ
        # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ë§Œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì„±ëŠ¥ í™•ë³´
        self.realtime_detection_enabled = False  # ì‹¤ì‹œê°„ ê°ì§€ ë¹„í™œì„±í™” (90í”„ë ˆì„ í•™ìŠµ ëª¨ë¸ì— ë§ì¶¤)
        self.realtime_confidence_threshold = 0.20  # 0.25 â†’ 0.20ë¡œ ë‚®ì¶¤ (ë” ê´€ëŒ€í•˜ê²Œ)
        
        # ìƒì²´ ê´€ì ˆì  (YOLO Pose 17ê°œ ì¤‘ ìƒì²´ 9ê°œ)
        self.upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]  # ì½”+ì–´ê¹¨+íŒ”+ì—‰ë©ì´
        
        # ì¸ì ‘ í–‰ë ¬ ìƒì„±
        self.adjacency_matrix = self.create_adjacency_matrix()
        if hasattr(self, 'gesture_model'):
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        
        # ì“°ë ˆë“œ ì„¤ì •
        self.frame_queue = queue.Queue(maxsize=3)
        self.running = True
        self.lock = threading.Lock()
        
        # ê²°ê³¼ ì €ì¥
        self.latest_gesture = ("NORMAL", self.current_gesture_confidence, False, None)
        self.last_valid_keypoints = None
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ìë™ ì‹œì‘
        self.worker_thread = threading.Thread(target=self.recognition_worker)
        self.worker_thread.start()
        print(f"âœ… ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ë¨ (ìŠ¤ë ˆë“œ ID: {self.worker_thread.ident})")
        
        print("âœ… Gesture Recognizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start(self):
        """ì¸ì‹ê¸° ì‹œì‘"""
        self.worker_thread = threading.Thread(target=self.recognition_worker)
        self.worker_thread.start()
        print("ğŸš€ Gesture Recognizer ì‹œì‘")
    
    def stop(self):
        """ì¸ì‹ê¸° ì¤‘ì§€"""
        self.running = False
        try:
            self.frame_queue.put_nowait(None)
        except queue.Full:
            pass
        if hasattr(self, 'worker_thread'):
            self.worker_thread.join()
        print("ğŸ›‘ Gesture Recognizer ì¤‘ì§€")
    
    def load_gesture_model(self):
        """Shift-GCN ëª¨ë¸ ë¡œë“œ"""
        model_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/models/simple_shift_gcn_model.pth'
        
        if os.path.exists(model_path):
            self.gesture_model = SimpleShiftGCN(num_classes=2, num_joints=9, dropout=0.5)
            self.gesture_model.load_state_dict(torch.load(model_path, map_location=device))
            self.gesture_model = self.gesture_model.to(device)
            self.gesture_model.eval()
            self.enable_gesture_recognition = True
            print(f"âœ… Shift-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
            
            # í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì¸ì ‘ í–‰ë ¬ ë¡œë“œ ì‹œë„
            adj_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/shift_gcn_data/adjacency_matrix.npy'
            if os.path.exists(adj_path):
                self.adjacency_matrix = np.load(adj_path)
                print(f"âœ… í•™ìŠµ ì‹œ ì¸ì ‘ í–‰ë ¬ ë¡œë“œ: {self.adjacency_matrix.shape}")
            else:
                print("âš ï¸ í•™ìŠµ ì‹œ ì¸ì ‘ í–‰ë ¬ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.adjacency_matrix = self.create_adjacency_matrix()
            
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            self.enable_gesture_recognition = False
    
    def create_adjacency_matrix(self):
        """ì¸ì ‘ í–‰ë ¬ ìƒì„± (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)"""
        num_joints = len(self.upper_body_joints)
        A = np.zeros((num_joints, num_joints))
        
        # self-connection (ëŒ€ê°ì„ )
        for i in range(num_joints):
            A[i, i] = 1
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜ (í•™ìŠµ ì‹œì™€ ë™ì¼)
        connections = [
            (0, 1), (0, 2),  # nose - shoulders
            (1, 2),  # shoulders
            (1, 3), (2, 4),  # shoulders - elbows
            (3, 5), (4, 6),  # elbows - wrists
            (1, 7), (2, 8),  # shoulders - hips
            (7, 8),  # hips
        ]
        
        # ë§¤í•‘: ì›ë³¸ ê´€ì ˆì  â†’ ìƒì²´ ë°°ì—´ ì¸ë±ìŠ¤
        joint_mapping = {joint: i for i, joint in enumerate(self.upper_body_joints)}
        
        for joint1, joint2 in connections:
            if joint1 in joint_mapping and joint2 in joint_mapping:
                i, j = joint_mapping[joint1], joint_mapping[joint2]
                A[i, j] = 1
                A[j, i] = 1
        
        return A
    
    def normalize_keypoints(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)"""
        if keypoints.shape[0] == 0:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì„ê³„ê°’)
            valid_joints = frame_keypoints[:, 2] > 0.3  # 0.1 â†’ 0.3ìœ¼ë¡œ ë³µì› (í•™ìŠµ ì‹œì™€ ë™ì¼)
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· ) - í•™ìŠµ ì‹œì™€ ë™ì¼
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€í¸ì°¨) - í•™ìŠµ ì‹œì™€ ë™ì¼
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                # ìœ íš¨í•œ ê´€ì ˆì ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_with_training_stats(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ë°ì´í„° í†µê³„ ì‚¬ìš©)"""
        if keypoints is None:
            return keypoints
        
        # í•™ìŠµ ë°ì´í„° í†µê³„ (ì‹¤ì œ í•™ìŠµëœ ë°ì´í„° ê¸°ë°˜)
        # COME: í‰ê· =0.315, í‘œì¤€í¸ì°¨=0.798, ë²”ìœ„=[-2.396, 2.119]
        # NORMAL: í‰ê· =0.307, í‘œì¤€í¸ì°¨=0.699, ë²”ìœ„=[-2.566, 1.955]
        
        # í•™ìŠµ ë°ì´í„°ì˜ ì „ì²´ í†µê³„
        training_mean = 0.311  # (0.315 + 0.307) / 2
        training_std = 0.749   # (0.798 + 0.699) / 2
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš©
            valid_joints = frame_keypoints[:, 2] > 0.3
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚°
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚°
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš©
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                
                # í•™ìŠµ ë°ì´í„° í†µê³„ì— ë§ì¶° ì¶”ê°€ ì •ê·œí™”
                normalized_keypoints[t, :, 0] = (normalized_keypoints[t, :, 0] - training_mean) / training_std
                normalized_keypoints[t, :, 1] = (normalized_keypoints[t, :, 1] - training_mean) / training_std
            else:
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_exact_training(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ì™„ì „íˆ ë™ì¼)"""
        if keypoints is None:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì„ê³„ê°’)
            valid_joints = frame_keypoints[:, 2] > 0.3  # 0.1 â†’ 0.3ìœ¼ë¡œ ë³µì› (í•™ìŠµ ì‹œì™€ ë™ì¼)
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· ) - í•™ìŠµ ì‹œì™€ ë™ì¼
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€í¸ì°¨) - í•™ìŠµ ì‹œì™€ ë™ì¼
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                # ìœ íš¨í•œ ê´€ì ˆì ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def normalize_keypoints_distance_compensated(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (ê±°ë¦¬ ì°¨ì´ ë³´ìƒ)"""
        if keypoints is None:
            return keypoints
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš©
            valid_joints = frame_keypoints[:, 2] > 0.3
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚°
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ê±°ë¦¬ ë³´ìƒ ì ìš©)
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ê±°ë¦¬ ë³´ìƒ: í˜„ì¬ ìŠ¤ì¼€ì¼ì„ í•™ìŠµ ì‹œ ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •
                # í•™ìŠµ ì‹œ í‰ê·  ìŠ¤ì¼€ì¼: ì•½ 0.8, í˜„ì¬ ìŠ¤ì¼€ì¼: ì•½ 170
                # ë³´ìƒ ê³„ìˆ˜ = í•™ìŠµ_ìŠ¤ì¼€ì¼ / í˜„ì¬_ìŠ¤ì¼€ì¼ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
                if scale > 50:  # ìŠ¤ì¼€ì¼ì´ ë§¤ìš° í° ê²½ìš°ë§Œ ë³´ìƒ
                    compensation_factor = min(0.8 / scale, 0.1)  # ìµœëŒ€ 0.1ë¡œ ì œí•œ
                elif scale > 10:  # ì¤‘ê°„ ìŠ¤ì¼€ì¼
                    compensation_factor = min(0.8 / scale, 0.3)  # ìµœëŒ€ 0.3ìœ¼ë¡œ ì œí•œ
                else:  # ì‘ì€ ìŠ¤ì¼€ì¼
                    compensation_factor = 1.0  # ë³´ìƒ ì—†ìŒ
                
                adjusted_scale = scale * compensation_factor
                
                # ì •ê·œí™” ì ìš© (ê±°ë¦¬ ë³´ìƒ í¬í•¨)
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / adjusted_scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / adjusted_scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            else:
                normalized_keypoints[t, :, 0] = 0.0
                normalized_keypoints[t, :, 1] = 0.0
        
        return normalized_keypoints
    
    def draw_keypoints(self, frame, keypoints, color=(0, 255, 0)):
        """ê´€ì ˆì  ì‹œê°í™” (ìƒì²´ë§Œ)"""
        if keypoints is None or len(keypoints) == 0:
            return frame
            
        # ìƒì²´ ê´€ì ˆì  ì¸ë±ìŠ¤ (YOLO Pose 17ê°œ ì¤‘)
        display_joints = [0, 5, 6, 7, 8, 9, 10]  # ì–¼êµ´ + ìƒì²´
        
        # ê´€ì ˆì  ì—°ê²°ì„  ì •ì˜
        connections = [
            (5, 6),   # ì–´ê¹¨ ì—°ê²°
            (5, 7),   # ì™¼ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜
            (7, 9),   # ì™¼ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
            (6, 8),   # ì˜¤ë¥¸ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜
            (8, 10),  # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
        ]
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for joint_idx in display_joints:
            if joint_idx < len(keypoints):
                x, y, conf = keypoints[joint_idx]
                if conf > 0.3:  # ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ
                    cv2.circle(frame, (int(x), int(y)), 4, color, -1)
                    cv2.circle(frame, (int(x), int(y)), 6, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        for joint1, joint2 in connections:
            if joint1 < len(keypoints) and joint2 < len(keypoints):
                x1, y1, conf1 = keypoints[joint1]
                x2, y2, conf2 = keypoints[joint2]
                if conf1 > 0.3 and conf2 > 0.3:
                    cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        
        return frame
    
    def recognition_worker(self):
        """ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ (ê°€ì¥ í° ì‚¬ëŒì˜ poseë§Œ ê°ì§€)"""
        print("ğŸ”„ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")
        frame_count = 0
        
        while self.running:
            try:
                frame_data = self.frame_queue.get(timeout=1.0)
                if frame_data is None:
                    print("ğŸ›‘ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                    break
                
                frame, frame_id, elapsed_time, latest_detections = frame_data
                frame_count += 1
                
                # í”„ë ˆì„ ìˆ˜ì‹  í™•ì¸ (60í”„ë ˆì„ë§ˆë‹¤)
                if frame_count % 60 == 0:
                    print(f"ğŸ”„ ì›Œì»¤ ìŠ¤ë ˆë“œ: í”„ë ˆì„ {frame_id} ì²˜ë¦¬ ì¤‘ (ì´ {frame_count}ê°œ ì²˜ë¦¬ë¨)")
                    print(f"   - í”„ë ˆì„ í¬ê¸°: {frame.shape}")
                    print(f"   - ê°ì§€ëœ ì‚¬ëŒ: {len(latest_detections)}ëª…")
                    print(f"   - ì œìŠ¤ì²˜ ì¸ì‹ í™œì„±í™”: {self.enable_gesture_recognition}")
                
                if not self.enable_gesture_recognition:
                    self.frame_queue.task_done()
                    continue
                
                # ê¸°ë³¸ê°’ ìœ ì§€ (ìƒˆë¡œìš´ íŒë‹¨ì´ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€)
                with self.lock:
                    prediction, confidence, keypoints_detected, current_keypoints = self.latest_gesture
        
                # ê°€ì¥ í° ì‚¬ëŒ(ì£¼ìš” ëŒ€ìƒ) ì°¾ê¸°
                target_person = None
                if latest_detections:
                    # ë©´ì ì´ ê°€ì¥ í° ì‚¬ëŒ ì„ íƒ
                    target_person = max(latest_detections, key=lambda p: 
                        (p['bbox'][2] - p['bbox'][0]) * (p['bbox'][3] - p['bbox'][1]))
                    
                    if frame_id % 60 == 0:  # 60í”„ë ˆì„ë§ˆë‹¤ ë””ë²„ê¹…
                        area = (target_person['bbox'][2] - target_person['bbox'][0]) * (target_person['bbox'][3] - target_person['bbox'][1])
                        print(f"ğŸ¯ ì£¼ìš” ëŒ€ìƒ: {target_person['id']} (ë©´ì : {area:.0f})")
                        print(f"   - ë°”ìš´ë”© ë°•ìŠ¤: {target_person['bbox']}")
                else:
                    if frame_id % 60 == 0:
                        print(f"ğŸ¯ ê°ì§€ëœ ì‚¬ëŒ ì—†ìŒ")
                
                # YOLO Poseë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê°€ì¥ í° ì‚¬ëŒë§Œ)
                if target_person is not None:
                    # ì£¼ìš” ëŒ€ìƒì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ROI ì„¤ì •
                    x1, y1, x2, y2 = map(int, target_person['bbox'])
                    
                    # ROI í™•ì¥ (ë” ë„“ì€ ì˜ì—­ì—ì„œ pose ê°ì§€)
                    margin = 100  # 50 â†’ 100ìœ¼ë¡œ ëŠ˜ë¦¼
                    x1 = max(0, x1 - margin)
                    y1 = max(0, y1 - margin)
                    x2 = min(frame.shape[1], x2 + margin)
                    y2 = min(frame.shape[0], y2 + margin)
                    
                    if frame_id % 60 == 0:
                        print(f"   - ROI ì„¤ì •: [{x1}, {y1}, {x2}, {y2}] (ë§ˆì§„: {margin})")
                    
                    # ROI ì¶”ì¶œ
                    roi = frame[y1:y2, x1:x2]
                    
                    if roi.size > 0:  # ROIê°€ ìœ íš¨í•œ ê²½ìš°
                        if frame_id % 60 == 0:
                            print(f"   - ROI í¬ê¸°: {roi.shape}")
                        
                        # ROIì—ì„œ pose ê°ì§€ (ì„¤ì • ê°œì„ )
                        results = self.pose_model(roi, imgsz=256, conf=0.01,  # 0.05 â†’ 0.01ë¡œ ë” ë‚®ì¶¤
                                                verbose=False, device=0)
                        
                        keypoints_data = []
                        
                        for result in results:
                            if result.keypoints is not None:
                                keypoints = result.keypoints.data.cpu().numpy()  # (N, 17, 3)
                                
                                if frame_id % 60 == 0:  # 60í”„ë ˆì„ë§ˆë‹¤ ë””ë²„ê¹…
                                    print(f"   ğŸ” YOLO Pose ê²°ê³¼: {len(keypoints)}ëª… ê°ì§€")
                                
                                for person_idx, person_kpts in enumerate(keypoints):
                                    # ìƒì²´ ê´€ì ˆì ë§Œ ì¶”ì¶œ
                                    upper_body_kpts = person_kpts[self.upper_body_joints]  # (9, 3)
                                    
                                    # ì‹ ë¢°ë„ ì²´í¬ (ì„ê³„ê°’ ë‚®ì¶¤)
                                    valid_joints = upper_body_kpts[:, 2] >= 0.01  # 0.05 â†’ 0.01ë¡œ ë” ë‚®ì¶¤
                                    valid_count = np.sum(valid_joints)
                                    
                                    if frame_id % 60 == 0:
                                        print(f"   ì£¼ìš” ëŒ€ìƒ {target_person['id']}: {valid_count}/9 í‚¤í¬ì¸íŠ¸")
                                        print(f"      - ì–´ê¹¨: {person_kpts[5][2]:.2f}/{person_kpts[6][2]:.2f}")
                                        print(f"      - íŒ”ê¿ˆì¹˜: {person_kpts[7][2]:.2f}/{person_kpts[8][2]:.2f}")
                                        print(f"      - ì†ëª©: {person_kpts[9][2]:.2f}/{person_kpts[10][2]:.2f}")
                                    
                                    if valid_count >= self.min_keypoints_for_gesture:  # 3 â†’ 2ë¡œ ë‚®ì¶¤
                                        # ROI ì¢Œí‘œë¥¼ ì „ì²´ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                                        person_kpts[:, 0] += x1
                                        person_kpts[:, 1] += y1
                                        
                                        keypoints_data.append(upper_body_kpts)
                                        keypoints_detected = True
                                        current_keypoints = person_kpts  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ì¥ (ì‹œê°í™”ìš©)
                                        
                                        if frame_id % 60 == 0:
                                            print(f"   âœ… í‚¤í¬ì¸íŠ¸ ê°ì§€ ì„±ê³µ! {valid_count}ê°œ ìœ íš¨")
                                        break  # ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ ì‚¬ìš©
                                    else:
                                        if frame_id % 60 == 0:
                                            print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±: {valid_count}/{self.min_keypoints_for_gesture} (ì„ê³„ê°’ ë¯¸ë‹¬)")
                            else:
                                if frame_id % 60 == 0:
                                    print(f"   âŒ YOLO Pose ê²°ê³¼ì— í‚¤í¬ì¸íŠ¸ ì—†ìŒ")
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ë” ìì„¸í•œ ë””ë²„ê¹…
                        if len(keypoints_data) == 0 and frame_id % 60 == 0:
                            print(f"   âŒ í‚¤í¬ì¸íŠ¸ ê°ì§€ ì‹¤íŒ¨ - ROI í¬ê¸°: {roi.shape}")
                            print(f"      - ë°”ìš´ë”© ë°•ìŠ¤: [{x1}, {y1}, {x2}, {y2}]")
                            print(f"      - ROI í¬ê¸°: {roi.size}")
                            print(f"      - YOLO Pose ê²°ê³¼ ê°œìˆ˜: {len(results)}")
                            if len(results) > 0:
                                print(f"      - ì²« ë²ˆì§¸ ê²°ê³¼ í‚¤í¬ì¸íŠ¸: {results[0].keypoints is not None}")
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ë©´ ì²˜ë¦¬
                        if len(keypoints_data) > 0:
                            current_keypoints_data = keypoints_data[0]
                            keypoints_detected = True
                            current_keypoints = current_keypoints  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ì¥ (ì‹œê°í™”ìš©)
                        else:
                            # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì œìŠ¤ì²˜ íŒë‹¨ ì¤‘ë‹¨
                            if frame_id % 60 == 0:
                                print(f"   âš ï¸ ì£¼ìš” ëŒ€ìƒ í‚¤í¬ì¸íŠ¸ ë¶€ì¡± - ì œìŠ¤ì²˜ íŒë‹¨ ì¤‘ë‹¨")
                            keypoints_detected = False
                            current_keypoints = None
                            # ì´ì „ í”„ë ˆì„ ë°ì´í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                            current_keypoints_data = None
                        
                        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ ì €ì¥
                        if len(keypoints_data) > 0:
                            self.last_valid_keypoints = keypoints_data[0]
                    else:
                        # ROIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
                        if frame_id % 60 == 0:
                            print(f"   âŒ ROIê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ - í¬ê¸°: {roi.size}")
                        keypoints_detected = False
                        current_keypoints = None
                        current_keypoints_data = None
                else:
                    # ê°ì§€ëœ ì‚¬ëŒì´ ì—†ëŠ” ê²½ìš°
                    if frame_id % 60 == 0:
                        print(f"   âŒ ê°ì§€ëœ ì‚¬ëŒ ì—†ìŒ - í‚¤í¬ì¸íŠ¸ ê°ì§€ ë¶ˆê°€")
                    keypoints_detected = False
                    current_keypoints = None
                    current_keypoints_data = None
                
                # ì œìŠ¤ì²˜ ë¶„ì„ (í‚¤í¬ì¸íŠ¸ê°€ ì¶©ë¶„íˆ ê°ì§€ëœ ê²½ìš°ì—ë§Œ)
                if keypoints_detected and current_keypoints_data is not None:
                    # ìµœì†Œ 4ê°œ ì´ìƒì˜ í‚¤í¬ì¸íŠ¸ê°€ ìˆì–´ì•¼ ì œìŠ¤ì²˜ íŒë‹¨ (6 â†’ 4ë¡œ ë‚®ì¶¤)
                    valid_joints = current_keypoints_data[:, 2] >= 0.05  # 0.1 â†’ 0.05ë¡œ ë‚®ì¶¤
                    valid_count = np.sum(valid_joints)
                    
                    if valid_count >= 4:  # ìµœì†Œ 4ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš” (6 â†’ 4ë¡œ ë‚®ì¶¤)
                        self.gesture_frame_buffer.append(current_keypoints_data)
                        
                        if frame_id % 60 == 0:
                            print(f"   âœ… ì œìŠ¤ì²˜ ë¶„ì„ ì§„í–‰: {valid_count}/9 í‚¤í¬ì¸íŠ¸")
                    else:
                        if frame_id % 60 == 0:
                            print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±ìœ¼ë¡œ ì œìŠ¤ì²˜ ë¶„ì„ ì¤‘ë‹¨: {valid_count}/4 (ìµœì†Œ í•„ìš”)")
                else:
                    if frame_id % 60 == 0:
                        print(f"   âŒ í‚¤í¬ì¸íŠ¸ ì—†ìŒìœ¼ë¡œ ì œìŠ¤ì²˜ ë¶„ì„ ì¤‘ë‹¨")
                
                # ì‹¤ì‹œê°„ ê°ì§€ (ë¹„í™œì„±í™” - í•™ìŠµ ì‹œ 90í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµë¨)
                # ì‹¤ì‹œê°„ ê°ì§€ëŠ” 30í”„ë ˆì„ìœ¼ë¡œ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì— ë¶€ì ì ˆ
                # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ë§Œ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì„±ëŠ¥ í™•ë³´
                # if (self.realtime_detection_enabled and 
                #     len(self.gesture_frame_buffer) >= 30 and
                #     keypoints_detected and current_keypoints_data is not None):
                #     # ì‹¤ì‹œê°„ ê°ì§€ ë¡œì§ ì œê±°
                #     pass
                
                # 3ì´ˆ(90í”„ë ˆì„) ë‹¨ìœ„ë¡œ ì •ê¸° íŒë‹¨ (í•™ìŠµ ì‹œì™€ ë™ì¼)
                frames_since_last_decision = frame_id - self.last_gesture_decision_frame
                
                if (len(self.gesture_frame_buffer) >= self.min_gesture_frames and 
                    frames_since_last_decision >= self.gesture_decision_interval):
                    
                    print(f"ğŸ¯ [Frame {frame_id}] 3ì´ˆ ë‹¨ìœ„ ì œìŠ¤ì²˜ íŒë‹¨ ì‹œì‘!")
                    
                    try:
                        # í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë¡œì§)
                        keypoints_sequence = list(self.gesture_frame_buffer)
                        keypoints_array = np.array(keypoints_sequence)  # (T, 9, 3)
                        
                        # í‚¤í¬ì¸íŠ¸ í’ˆì§ˆ ê²€ì¦
                        valid_frames = []
                        for i, frame_kpts in enumerate(keypoints_array):
                            valid_joints = frame_kpts[:, 2] >= 0.01
                            valid_count = np.sum(valid_joints)
                            if valid_count >= 4:
                                valid_frames.append(frame_kpts)
                        
                        if len(valid_frames) < 90:  # ìµœì†Œ 90í”„ë ˆì„ í•„ìš” (í•™ìŠµ ì‹œì™€ ë™ì¼)
                            print(f"   âš ï¸ ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ í”„ë ˆì„ ë¶€ì¡±: {len(valid_frames)}/90")
                            self.frame_queue.task_done()
                            continue
                        
                        # ìœ íš¨í•œ í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©
                        keypoints_array = np.array(valid_frames)
                        
                        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬
                        T, V, C = keypoints_array.shape
                        target_frames = 90
                    
                        if T != target_frames:
                            old_indices = np.linspace(0, T-1, T)
                            new_indices = np.linspace(0, T-1, target_frames)
                            
                            resampled_keypoints = np.zeros((target_frames, V, C))
                            for v in range(V):
                                for c in range(C):
                                    resampled_keypoints[:, v, c] = np.interp(new_indices, old_indices, keypoints_array[:, v, c])
                            
                            keypoints_array = resampled_keypoints
                        
                        # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì •ê·œí™” ì ìš© (ì¤‘ì‹¬ì  ê¸°ë°˜)
                        normalized_keypoints = self.normalize_keypoints(keypoints_array)
                        
                        # Shift-GCN ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                        shift_gcn_data = np.zeros((C, target_frames, V, 1))
                        shift_gcn_data[:, :, :, 0] = normalized_keypoints.transpose(2, 0, 1)
                        
                        # ëª¨ë¸ ì˜ˆì¸¡
                        input_tensor = torch.FloatTensor(shift_gcn_data).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            self.gesture_model.eval()
                            output = self.gesture_model(input_tensor)
                            probabilities = F.softmax(output, dim=1)
                            prediction = torch.argmax(probabilities, dim=1).item()
                            confidence = probabilities[0, prediction].item()
                        
                        # ê²°ê³¼ í•´ì„
                        gesture_name = self.actions[prediction]
                        
                        # ì •ê¸° íŒë‹¨ ê²°ê³¼ ì—…ë°ì´íŠ¸
                        with self.lock:
                            self.latest_gesture = (gesture_name, confidence, keypoints_detected, current_keypoints)
                            self.current_gesture_confidence = confidence
                        
                        self.last_gesture_decision_frame = frame_id
                        
                        print(f"ğŸ¯ 3ì´ˆ ì œìŠ¤ì²˜ ê²°ê³¼: {gesture_name} (ì‹ ë¢°ë„: {confidence:.3f})")
                        
                        # ë²„í¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ 3ì´ˆ êµ¬ê°„ ì‹œì‘)
                        self.gesture_frame_buffer.clear()
                        
                    except Exception as e:
                        print(f"âŒ 3ì´ˆ ì œìŠ¤ì²˜ ì¸ì‹ ì˜¤ë¥˜: {e}")
                
                # ê²°ê³¼ ì €ì¥ (í‚¤í¬ì¸íŠ¸ ì •ë³´ í¬í•¨)
                # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if keypoints_detected and current_keypoints is not None:
                    # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ ê²°ê³¼ë§Œ ì‚¬ìš© (ì‹¤ì‹œê°„ ê°ì§€ ë¹„í™œì„±í™”)
                    with self.lock:
                        self.latest_gesture = (prediction, confidence, True, current_keypoints)
                # í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€ (ì—…ë°ì´íŠ¸ ì•ˆ í•¨)
                elif not keypoints_detected and frame_id % 120 == 0:
                    # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì„ ë•Œ ë””ë²„ê¹…
                    print(f"   âš ï¸ í‚¤í¬ì¸íŠ¸ ì—†ìŒ - ì´ì „ ê²°ê³¼ ìœ ì§€: {self.latest_gesture[0]} ({self.latest_gesture[1]:.3f})")
                
                self.frame_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                break
    
    def get_latest_gesture(self):
        """ìµœì‹  ì œìŠ¤ì²˜ ê²°ê³¼ ë°˜í™˜"""
        with self.lock:
            return self.latest_gesture
    
    def add_frame(self, frame, frame_id, elapsed_time, latest_detections):
        """í”„ë ˆì„ ì¶”ê°€ (ë¹„ë™ê¸°)"""
        try:
            self.frame_queue.put_nowait((frame, frame_id, elapsed_time, latest_detections))
            
            # í”„ë ˆì„ ì „ë‹¬ í™•ì¸ (120í”„ë ˆì„ë§ˆë‹¤)
            if frame_id % 120 == 0:
                print(f"ğŸ“¤ ì œìŠ¤ì²˜ ì¸ì‹ê¸°ì— í”„ë ˆì„ {frame_id} ì „ë‹¬ë¨")
                print(f"   - í í¬ê¸°: {self.frame_queue.qsize()}")
                print(f"   - ê°ì§€ëœ ì‚¬ëŒ: {len(latest_detections)}ëª…")
                
        except queue.Full:
            if frame_id % 120 == 0:
                print(f"âš ï¸ ì œìŠ¤ì²˜ ì¸ì‹ê¸° íê°€ ê°€ë“ì°¸ - í”„ë ˆì„ {frame_id} ê±´ë„ˆëœ€")
        except Exception as e:
            if frame_id % 120 == 0:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ê¸° í”„ë ˆì„ ì „ë‹¬ ì˜¤ë¥˜: {e}") 
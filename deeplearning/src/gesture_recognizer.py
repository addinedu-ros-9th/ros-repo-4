import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
from collections import deque
import threading
import queue
import time
import os
import sys

# SlidingShiftGCN ëª¨ë¸ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/ckim/ros-repo-4/deeplearning/gesture_recognition/src')
from train_sliding_shift_gcn import SlidingShiftGCN

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (GestureRecognizerìš© 30%)
if torch.cuda.is_available():
    try:
        total_memory = torch.cuda.get_device_properties(0).total_memory
        gesture_recognizer_memory = int(total_memory * 0.3)
        torch.cuda.set_per_process_memory_fraction(0.3, 0)  # 30% ì œí•œ
        print(f"ğŸ® GestureRecognizer GPU ë©”ëª¨ë¦¬ ì œí•œ: {gesture_recognizer_memory / 1024**3:.1f}GB")
    except Exception as e:
        print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • ì‹¤íŒ¨: {e}")

class GestureRecognizer:
    """predict_webcam_realtime.py ê¸°ë°˜ ì œìŠ¤ì²˜ ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ğŸš€ Gesture Recognizer ì´ˆê¸°í™” (predict_webcam_realtime.py ê¸°ë°˜)")
        
        # predict_webcam_realtime.pyì™€ ë™ì¼í•œ ì´ˆê¸°í™”
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        yolo_path = '/home/ckim/ros-repo-4/deeplearning/yolov8n-pose.pt'
        self.yolo_model = YOLO(yolo_path)
        if torch.cuda.is_available():
            self.yolo_model.to(self.device)
        
        # ê´€ì ˆì  ì¸ë±ìŠ¤ (predict_webcam_realtime.pyì™€ ë™ì¼)
        self.joint_indices = [0, 5, 6, 7, 8, 9, 10, 11, 12]
        self.num_joints = len(self.joint_indices)
        
        # GCN ëª¨ë¸ ë¡œë“œ
        self.load_shift_gcn_model()
        
        # ë²„í¼ ë° ì„¤ì • (predict_webcam_realtime.pyì™€ ë™ì¼)
        self.window_size = 30
        self.keypoint_buffer = deque(maxlen=self.window_size)
        
        # ì•¡ì…˜ ë¼ë²¨ ë° ìƒ‰ìƒ (predict_webcam_realtime.pyì™€ ë™ì¼)
        self.action_labels = {0: 'COME', 1: 'NORMAL'}
        self.actions = ['COME', 'NORMAL']  # í˜¸í™˜ì„± ìœ ì§€
        self.colors = {0: (0, 255, 0), 1: (0, 0, 255)}
        self.last_prediction = None
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.current_gesture = "NORMAL"
        self.current_confidence = 0.5
        
        # ì“°ë ˆë“œ ì„¤ì •
        self.frame_queue = queue.Queue(maxsize=3)
        self.running = True
        self.lock = threading.Lock()
        
        # ê²°ê³¼ ì €ì¥
        self.latest_gesture = ("NORMAL", 0.5, False, None)
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        self.worker_thread = threading.Thread(target=self.recognition_worker)
        self.worker_thread.start()
        print(f"âœ… ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ë¨ (ìŠ¤ë ˆë“œ ID: {self.worker_thread.ident})")
        
        print("âœ… Gesture Recognizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_shift_gcn_model(self):
        """predict_webcam_realtime.pyì™€ ë™ì¼í•œ GCN ëª¨ë¸ ë¡œë”©"""
        model_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/models/sliding_shift_gcn_model.pth'
        
        # ì¸ì ‘ í–‰ë ¬ ë¡œë“œ
        adj_matrix_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/shift_gcn_data_sliding/adjacency_matrix.npy'
        adjacency_matrix = np.load(adj_matrix_path) if os.path.exists(adj_matrix_path) else np.eye(self.num_joints)
        
        # ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
        self.model = SlidingShiftGCN(num_classes=2, num_joints=self.num_joints, dropout=0.3)
        self.model.set_adjacency_matrix(adjacency_matrix)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def normalize_keypoints(self, keypoints):
        """predict_webcam_realtime.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ì •ê·œí™”"""
        normalized = keypoints.copy()
        valid_joints = keypoints[:, 2] > 0
        if not np.any(valid_joints): 
            return None
        valid_points = keypoints[valid_joints, :2]
        center = np.mean(valid_points, axis=0)
        scale = np.std(valid_points)
        if scale == 0: 
            scale = 1.0
        normalized[:, :2] = (keypoints[:, :2] - center) / scale
        return normalized

    def predict_gesture(self):
        """predict_webcam_realtime.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ì˜ˆì¸¡"""
        if len(self.keypoint_buffer) < self.window_size:
            return None, 0.0
        
        window_data = np.array(list(self.keypoint_buffer))
        shift_gcn_input = window_data.transpose(2, 0, 1)[np.newaxis, :, :, :, np.newaxis]
        
        with torch.no_grad():
            input_tensor = torch.FloatTensor(shift_gcn_input).to(self.device)
            outputs = self.model(input_tensor)
            probabilities = F.softmax(outputs, dim=1).squeeze()
            prediction = torch.argmax(probabilities).item()
            confidence = probabilities[prediction].item()
        return prediction, confidence

    def draw_visualization(self, frame, keypoints, prediction, confidence):
        """predict_webcam_realtime.pyì™€ ë™ì¼í•œ ì‹œê°í™” (í˜¸í™˜ì„±ìš©)"""
        if keypoints is not None:
            # ê´€ì ˆì  ì—°ê²°ì„  ì •ì˜ (predict_webcam_realtime.pyì™€ ë™ì¼)
            connections = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6), (1, 7), (2, 8), (7, 8)]
            for i, j in connections:
                idx1, idx2 = self.joint_indices[i], self.joint_indices[j]
                if keypoints[idx1, 2] > 0.3 and keypoints[idx2, 2] > 0.3:
                    p1 = (int(keypoints[idx1, 0]), int(keypoints[idx1, 1]))
                    p2 = (int(keypoints[idx2, 0]), int(keypoints[idx2, 1]))
                    cv2.line(frame, p1, p2, (255, 255, 255), 2)
        
            # ê´€ì ˆì  ê·¸ë¦¬ê¸° (predict_webcam_realtime.pyì™€ ë™ì¼)
            for idx in self.joint_indices:
                if keypoints[idx, 2] > 0.3:
                    center = (int(keypoints[idx, 0]), int(keypoints[idx, 1]))
                    cv2.circle(frame, center, 5, (0, 255, 255), -1)

        # ì œìŠ¤ì²˜ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ (predictionì´ ë¬¸ìì—´ì¸ ê²½ìš° ê³ ë ¤)
        if prediction is None or (isinstance(prediction, str) and prediction == "NORMAL" and confidence == 0.5):
            text = f"Collecting frames... [{len(self.keypoint_buffer)}/{self.window_size}]"
            cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            # predictionì´ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ìˆ«ìë©´ ë³€í™˜
            if isinstance(prediction, str):
                label = prediction
                color = (0, 0, 255) if label == "COME" else (0, 255, 0)  # BGR
            else:
                label = self.action_labels[prediction]
                color = self.colors[prediction]  # BGR ìƒ‰ìƒ
            
            text = f"{label}: {confidence:.2f}"
            cv2.rectangle(frame, (10, 10), (350, 60), color, -1)
            cv2.putText(frame, text, (20, 45), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
        
        return frame

    def recognition_worker(self):
        """predict_webcam_realtime.py ë¡œì§ ê¸°ë°˜ ì›Œì»¤"""
        print("ğŸ”„ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘ë¨")
        frame_count = 0
        
        while self.running:
            try:
                frame_data = self.frame_queue.get(timeout=1.0)
                if frame_data is None:
                    print("ğŸ›‘ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ")
                    break
                
                frame, frame_id, elapsed_time, latest_detections = frame_data
                frame_count += 1
                
                # predict_webcam_realtime.pyì™€ ë™ì¼í•œ ì²˜ë¦¬
                all_keypoints = None
                keypoints_detected = False
                
                # YOLOë¡œ ì „ì²´ í”„ë ˆì„ì—ì„œ í¬ì¦ˆ ê²€ì¶œ (predict_webcam_realtime.pyì™€ ë™ì¼)
                results = self.yolo_model(frame, verbose=False)
                if results[0].keypoints is not None and len(results[0].keypoints.data) > 0:
                    all_keypoints = results[0].keypoints.data[0].cpu().numpy()
                    selected_keypoints = all_keypoints[self.joint_indices]
                    
                    # predict_webcam_realtime.pyì™€ ë™ì¼í•œ ê²€ì¦
                    if np.all(selected_keypoints[:, 2] > 0.3):
                        normalized = self.normalize_keypoints(selected_keypoints)
                        if normalized is not None:
                            self.keypoint_buffer.append(normalized)
                            keypoints_detected = True
                
                # predict_webcam_realtime.pyì™€ ë™ì¼: ë§¤ í”„ë ˆì„ ì˜ˆì¸¡
                prediction, confidence = self.predict_gesture()
                
                if prediction is not None:
                    gesture_name = self.action_labels[prediction]
                    
                    # predict_webcam_realtime.pyì™€ ë™ì¼í•œ ë³€ê²½ ê°ì§€
                    if self.last_prediction != prediction:
                        print(f"ğŸ¯ ì œìŠ¤ì²˜ ë³€ê²½: {gesture_name} (ì‹ ë¢°ë„: {confidence:.3f})")
                        self.last_prediction = prediction
                    
                    # ê²°ê³¼ ì—…ë°ì´íŠ¸
                    with self.lock:
                        self.latest_gesture = (gesture_name, confidence, keypoints_detected, all_keypoints)
                        self.current_gesture = gesture_name
                        self.current_confidence = confidence

                self.frame_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                break
    
    def get_latest_gesture(self):
        """ìµœì‹  ì œìŠ¤ì²˜ ê²°ê³¼ ë°˜í™˜"""
        with self.lock:
            return self.latest_gesture
    
    def add_frame(self, frame, frame_id, elapsed_time, latest_detections):
        """í”„ë ˆì„ ì¶”ê°€ (ë¹„ë™ê¸°)"""
        try:
            self.frame_queue.put_nowait((frame, frame_id, elapsed_time, latest_detections))
        except queue.Full:
            if frame_id % 120 == 0:
                print(f"âš ï¸ ì œìŠ¤ì²˜ ì¸ì‹ê¸° íê°€ ê°€ë“ì°¸ - í”„ë ˆì„ {frame_id} ê±´ë„ˆëœ€")
        except Exception as e:
            if frame_id % 120 == 0:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ê¸° í”„ë ˆì„ ì „ë‹¬ ì˜¤ë¥˜: {e}")
    
    def stop(self):
        """ì¸ì‹ê¸° ì¤‘ì§€"""
        self.running = False
        try:
            self.frame_queue.put_nowait(None)
        except queue.Full:
            pass
        if hasattr(self, 'worker_thread'):
            self.worker_thread.join()
        print("ğŸ›‘ Gesture Recognizer ì¤‘ì§€") 
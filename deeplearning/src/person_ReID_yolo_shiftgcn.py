"""
ğŸš€ ì™„ì „ ìƒˆë¡œìš´ í†µí•© ì‹œìŠ¤í…œ
- MediaPipe ìˆ˜ì¤€ì˜ ì™„ë²½í•œ ìƒ‰ìƒ ë§¤ì¹­
- YOLO Pose + Shift-GCN ì œìŠ¤ì²˜ ì¸ì‹  
- ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ê³ ì„±ëŠ¥ ë‹¬ì„±
"""

import cv2
import numpy as np
from ultralytics import YOLO
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import os
from datetime import datetime
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import queue

# Qt ì˜¤ë¥˜ ë°©ì§€
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”¥ Using device: {device}")

class SimpleShiftGCN(nn.Module):
    """Shift-GCN ëª¨ë¸ (í•™ìŠµëœ êµ¬ì¡°ì™€ ë™ì¼)"""
    
    def __init__(self, num_classes, num_joints, in_channels=3, dropout=0.5):
        super(SimpleShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_joints = num_joints
        
        # Adjacency matrix
        self.register_buffer('A', torch.eye(num_joints))
        
        # ì…ë ¥ ì •ê·œí™”
        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)
        
        # 3ì¸µ êµ¬ì¡°
        self.conv1 = nn.Conv2d(in_channels, 32, 1)
        self.bn1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 64, 1) 
        self.bn2 = nn.BatchNorm2d(64)
        
        self.conv3 = nn.Conv2d(64, 128, 1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Temporal convolution
        self.tcn = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))
        self.tcn_bn = nn.BatchNorm2d(128)
        
        # Classifier
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(128, num_classes)
        
    def set_adjacency_matrix(self, A):
        self.A = torch.FloatTensor(A)
        if next(self.parameters()).is_cuda:
            self.A = self.A.cuda()
    
    def forward(self, x):
        N, C, T, V, M = x.size()
        
        # Focus on first person
        x = x[:, :, :, :, 0]  # (N, C, T, V)
        
        # Data normalization
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, V, T)
        x = x.view(N, C * V, T)
        x = self.data_bn(x)
        x = x.view(N, C, V, T)
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, T, V)
        
        # Graph convolution layers
        # Layer 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Temporal convolution
        x = self.tcn(x)
        x = self.tcn_bn(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Global pooling
        x = F.avg_pool2d(x, x.size()[2:])  # (N, 128, 1, 1)
        x = x.view(N, -1)  # (N, 128)
        
        # Classification
        x = self.fc(x)
        
        return x

class NewIntegratedSystem:
    def __init__(self):
        print("ğŸš€ ìƒˆë¡œìš´ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
        
        # YOLO ëª¨ë¸ë“¤
        self.person_model = YOLO('./yolov8s-seg.pt')  # ì‚¬ëŒ ê°ì§€ + ì„¸ê·¸ë©˜í…Œì´ì…˜
        self.pose_model = YOLO('../yolov8n-pose.pt')   # í¬ì¦ˆ ê°ì§€
        print("âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Shift-GCN ëª¨ë¸ ë¡œë“œ
        self.load_gesture_model()
        
        # MediaPipe ìˆ˜ì¤€ì˜ ì™„ë²½í•œ ìƒ‰ìƒ ë§¤ì¹­ ì„¤ì •
        self.people_data = {}
        self.next_id = 0
        self.match_threshold = 0.50
        self.reentry_threshold = 0.45
        self.min_detection_confidence = 0.5
        self.min_person_area = 4000
        self.max_histograms_per_person = 20
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì •
        self.gesture_frame_buffer = deque(maxlen=90)  # 90 í”„ë ˆì„
        self.gesture_prediction_buffer = deque(maxlen=5)
        self.actions = ['COME', 'NORMAL']  # 2í´ë˜ìŠ¤
        self.min_gesture_frames = 90  # 90í”„ë ˆì„ì´ ëª¨ì—¬ì•¼ íŒë‹¨ (3ì´ˆ ë‹¨ìœ„)
        self.gesture_confidence_threshold = 0.6
        
        # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ ì„¤ì •
        self.gesture_decision_interval = 90  # 90í”„ë ˆì„(3ì´ˆ)ë§ˆë‹¤ ìƒˆë¡œìš´ íŒë‹¨
        self.last_gesture_decision_frame = 0
        self.current_gesture_confidence = 0.5
        
        # ë¹ ë¥¸ NORMAL ë³µêµ¬ë¥¼ ìœ„í•œ ì„¤ì •
        self.static_detection_frames = 0
        self.static_threshold = 60  # 30 â†’ 60í”„ë ˆì„(2ì´ˆ)ìœ¼ë¡œ ì¦ê°€
        # ë³´í˜¸ê¸°ê°„ ì œê±°
        
        # ìƒì²´ ê´€ì ˆì  (YOLO Pose 17ê°œ ì¤‘ ìƒì²´ 9ê°œ) - í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ
        self.upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]  # ì½”+ì–´ê¹¨+íŒ”+ì—‰ë©ì´
        self.display_joints = [5, 6, 7, 8, 9, 10]  # í™”ë©´ í‘œì‹œìš© (ì–´ê¹¨+íŒ”ë§Œ)
        
        # ì¸ì ‘ í–‰ë ¬ ìƒì„±
        self.adjacency_matrix = self.create_adjacency_matrix()
        if hasattr(self, 'gesture_model'):
            self.gesture_model.set_adjacency_matrix(self.adjacency_matrix)
        
        # ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •
        self.setup_threading()
        
        # ëª¨ë¸ í…ŒìŠ¤íŠ¸ (COME/NORMAL êµ¬ë¶„ ëŠ¥ë ¥ í™•ì¸)
        self.test_model_performance()
        
        print("ğŸ‰ ìƒˆë¡œìš´ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def load_gesture_model(self):
        """Shift-GCN ëª¨ë¸ ë¡œë“œ"""
        # ì ˆëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •
        model_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/models/simple_shift_gcn_model.pth'
        
        if os.path.exists(model_path):
            self.gesture_model = SimpleShiftGCN(num_classes=2, num_joints=9, dropout=0.5)
            self.gesture_model.load_state_dict(torch.load(model_path, map_location=device))
            self.gesture_model = self.gesture_model.to(device)
            self.gesture_model.eval()
            self.enable_gesture_recognition = True
            print(f"âœ… Shift-GCN ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
            self.enable_gesture_recognition = False
        
    def get_person_color(self, person_id):
        """Person IDë³„ ê³ ìœ  ìƒ‰ìƒ ìƒì„±"""
        # ë‹¤ì–‘í•œ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = [
            (0, 255, 0),    # ì´ˆë¡
            (255, 0, 0),    # ë¹¨ê°•  
            (0, 0, 255),    # íŒŒë‘
            (255, 255, 0),  # ë…¸ë‘
            (255, 0, 255),  # ë§ˆì  íƒ€
            (0, 255, 255),  # ì‹œì•ˆ
            (255, 128, 0),  # ì£¼í™©
            (128, 0, 255),  # ë³´ë¼
            (0, 128, 255),  # í•˜ëŠ˜
            (255, 128, 128) # ë¶„í™
        ]
        
        # Person IDì—ì„œ ìˆ«ì ì¶”ì¶œ
        if isinstance(person_id, str) and '_' in person_id:
            try:
                id_num = int(person_id.split('_')[-1])
                return colors[id_num % len(colors)]
            except:
                pass
        
        # ê¸°ë³¸ ìƒ‰ìƒ
        return (0, 255, 0)
    
    def draw_keypoints(self, frame, keypoints, color=(0, 255, 0)):
        """ê´€ì ˆì  ì‹œê°í™” (ìƒì²´ë§Œ)"""
        if keypoints is None or len(keypoints) == 0:
            return frame
            
        # ìƒì²´ ê´€ì ˆì  ì¸ë±ìŠ¤ (YOLO Pose 17ê°œ ì¤‘)
        # 5: ì™¼ìª½ ì–´ê¹¨, 6: ì˜¤ë¥¸ìª½ ì–´ê¹¨, 7: ì™¼ìª½ íŒ”ê¿ˆì¹˜, 8: ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜
        # 9: ì™¼ìª½ ì†ëª©, 10: ì˜¤ë¥¸ìª½ ì†ëª©, 0: ì½”
        display_joints = [0, 5, 6, 7, 8, 9, 10]  # ì–¼êµ´ + ìƒì²´
        
        # ê´€ì ˆì  ì—°ê²°ì„  ì •ì˜
        connections = [
            (5, 6),   # ì–´ê¹¨ ì—°ê²°
            (5, 7),   # ì™¼ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜
            (7, 9),   # ì™¼ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
            (6, 8),   # ì˜¤ë¥¸ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜
            (8, 10),  # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
        ]
        
        # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        for joint_idx in display_joints:
            if joint_idx < len(keypoints):
                x, y, conf = keypoints[joint_idx]
                if conf > 0.3:  # ì‹ ë¢°ë„ 0.3 ì´ìƒë§Œ
                    cv2.circle(frame, (int(x), int(y)), 4, color, -1)
                    cv2.circle(frame, (int(x), int(y)), 6, (255, 255, 255), 1)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        for joint1, joint2 in connections:
            if joint1 < len(keypoints) and joint2 < len(keypoints):
                x1, y1, conf1 = keypoints[joint1]
                x2, y2, conf2 = keypoints[joint2]
                if conf1 > 0.3 and conf2 > 0.3:
                    cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        
        return frame
    
    def create_adjacency_matrix(self):
        """ì¸ì ‘ í–‰ë ¬ ìƒì„± (ìƒì²´ ê´€ì ˆì  ì—°ê²°)"""
        num_joints = len(self.upper_body_joints)
        A = np.eye(num_joints)
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜
        connections = [
            (5, 6),   # ì–´ê¹¨
            (5, 7),   # ì™¼ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜  
            (7, 9),   # ì™¼ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
            (6, 8),   # ì˜¤ë¥¸ìª½ ì–´ê¹¨-íŒ”ê¿ˆì¹˜
            (8, 10),  # ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜-ì†ëª©
            (0, 1),   # ì–¼êµ´ ì—°ê²°
            (0, 2),
            (1, 2)
        ]
        
        # ë§¤í•‘: ì›ë³¸ ê´€ì ˆì  â†’ ìƒì²´ ë°°ì—´ ì¸ë±ìŠ¤
        joint_mapping = {joint: i for i, joint in enumerate(self.upper_body_joints)}
        
        for joint1, joint2 in connections:
            if joint1 in joint_mapping and joint2 in joint_mapping:
                i, j = joint_mapping[joint1], joint_mapping[joint2]
                A[i, j] = 1
                A[j, i] = 1
        
        return A
    
    def setup_threading(self):
        """ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì •"""
        self.frame_queue = queue.Queue(maxsize=3)
        self.detection_queue = queue.Queue(maxsize=3)
        self.gesture_queue = queue.Queue(maxsize=3)
        self.result_queue = queue.Queue(maxsize=3)
        
        self.running = True
        self.executor = ThreadPoolExecutor(max_workers=3)
        self.lock = threading.Lock()
        
        # ê³µìœ  ê²°ê³¼ (í‚¤í¬ì¸íŠ¸ ì •ë³´ í¬í•¨)
        self.latest_detections = []
        self.latest_gesture = ("NORMAL", self.current_gesture_confidence, False, None)
        
        print("ğŸ”§ ë©€í‹°ìŠ¤ë ˆë”© ì„¤ì • ì™„ë£Œ")
    
    # MediaPipeì˜ ì™„ë²½í•œ ìƒ‰ìƒ ë§¤ì¹­ ë¡œì§ (ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
    def extract_histogram(self, img, mask, bins=16):
        """MediaPipe ìˆ˜ì¤€ì˜ ì™„ë²½í•œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # ì „ì²´ ë§ˆìŠ¤í¬ì—ì„œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
        h_hist = cv2.calcHist([hsv], [0], mask, [bins], [0, 180])
        s_hist = cv2.calcHist([hsv], [1], mask, [bins], [0, 256])
        v_hist = cv2.calcHist([hsv], [2], mask, [bins], [0, 256])
        
        # ìƒì²´ ë¶€ë¶„ ê°•ì¡° (MediaPipe ë°©ì‹)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            cnt = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(cnt)
            
            # ìƒì²´ ë¶€ë¶„ ë§ˆìŠ¤í¬ (ìƒìœ„ 60%)
            upper_mask = np.zeros_like(mask)
            upper_y = y + int(h * 0.6)
            upper_mask[y:upper_y, x:x+w] = mask[y:upper_y, x:x+w]
            
            # ìƒì²´ íˆìŠ¤í† ê·¸ë¨
            h_hist_upper = cv2.calcHist([hsv], [0], upper_mask, [bins], [0, 180])
            s_hist_upper = cv2.calcHist([hsv], [1], upper_mask, [bins], [0, 256])
            v_hist_upper = cv2.calcHist([hsv], [2], upper_mask, [bins], [0, 256])
        else:
            h_hist_upper = h_hist.copy()
            s_hist_upper = s_hist.copy()
            v_hist_upper = v_hist.copy()
        
        # ì •ê·œí™”
        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()
        
        h_hist_upper = cv2.normalize(h_hist_upper, h_hist_upper).flatten()
        s_hist_upper = cv2.normalize(s_hist_upper, s_hist_upper).flatten()
        v_hist_upper = cv2.normalize(v_hist_upper, v_hist_upper).flatten()
        
        # ì „ì²´ + ìƒì²´ ê²°í•© (ìƒì²´ 70% ê°€ì¤‘ì¹˜)
        combined_hist = np.concatenate([
            h_hist * 0.3, s_hist * 0.3, v_hist * 0.3,
            h_hist_upper * 0.7, s_hist_upper * 0.7, v_hist_upper * 0.7
        ])
        
        return combined_hist, h_hist, s_hist, v_hist
    
    def calculate_similarity_metrics(self, hist1, hist2):
        """ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚° (MediaPipe ë°©ì‹)"""
        bhatt_dist = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_BHATTACHARYYA)
        
        dot_product = np.dot(hist1, hist2)
        norm1 = np.linalg.norm(hist1)
        norm2 = np.linalg.norm(hist2)
        cosine_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
        
        corr = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)
        chi_square = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CHISQR)
        intersection = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_INTERSECT)
        
        return {
            'bhattacharyya': bhatt_dist,
            'cosine_similarity': cosine_sim,
            'correlation': corr,
            'chi_square': chi_square,
            'intersection': intersection
        }
    
    def find_best_match(self, current_hist, current_bbox, used_ids, elapsed_time=0.0):
        """ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ëŒ ì°¾ê¸° (MediaPipe ë°©ì‹)"""
        best_match_id = None
        best_score = 0.0
        best_metrics = {}
        
        x1, y1, x2, y2 = current_bbox
        current_center = ((x1 + x2) // 2, (y1 + y2) // 2)
        current_area = (x2 - x1) * (y2 - y1)
        
        for pid, pdata in self.people_data.items():
            if pid in used_ids:
                continue
                
            if len(pdata['histograms']) == 0:
                continue
            
            # 1. íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¥ ì¤‘ìš”)
            hist_scores = []
            for i, stored_hist in enumerate(pdata['histograms'][-self.max_histograms_per_person:]):
                metrics = self.calculate_similarity_metrics(current_hist, stored_hist)
                # Bhattacharyya ê±°ë¦¬ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ëª¨ë‘ ê³ ë ¤
                hist_score = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                hist_scores.append(hist_score)
            
            # ìµœê³  íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜ (ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ê·¸ë¨ ì‚¬ìš©)
            best_hist_score = max(hist_scores) if hist_scores else 0.0
            
            # 2. ê³µê°„ì  ìœ ì‚¬ë„ ê³„ì‚°
            latest_bbox = pdata['bboxes'][-1]
            stored_center = ((latest_bbox[0] + latest_bbox[2]) // 2, (latest_bbox[1] + latest_bbox[3]) // 2)
            stored_area = (latest_bbox[2] - latest_bbox[0]) * (latest_bbox[3] - latest_bbox[1])
            
            # ì¤‘ì‹¬ì  ê±°ë¦¬
            center_distance = np.sqrt((current_center[0] - stored_center[0])**2 + 
                                     (current_center[1] - stored_center[1])**2)
            max_distance = np.sqrt(640**2 + 480**2)
            spatial_score = 1.0 - (center_distance / max_distance)
            
            # 3. í¬ê¸° ìœ ì‚¬ë„ (ì‚¬ëŒì´ ê°‘ìê¸° í¬ê²Œ ë³€í•˜ì§€ ì•ŠìŒ)
            area_ratio = min(current_area, stored_area) / max(current_area, stored_area)
            size_score = area_ratio
            
            # 4. ì‹œê°„ì  ì—°ì†ì„± (ìµœê·¼ì— ë³¸ ì‚¬ëŒì—ê²Œ ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            if pdata['timestamps']:
                time_since_last_seen = elapsed_time - pdata['timestamps'][-1]
                time_score = max(0.0, 1.0 - (time_since_last_seen / 10.0))  # 10ì´ˆ ë‚´ì— ë³¸ ì‚¬ëŒì—ê²Œ ë†’ì€ ì ìˆ˜
            else:
                time_score = 0.0
            
            # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì¡°ì •)
            total_score = (
                0.7 * best_hist_score +      # íˆìŠ¤í† ê·¸ë¨ (ê°€ì¥ ì¤‘ìš”)
                0.15 * spatial_score +       # ê³µê°„ì  ìœ„ì¹˜
                0.1 * size_score +           # í¬ê¸° ìœ ì‚¬ë„
                0.05 * time_score            # ì‹œê°„ì  ì—°ì†ì„±
            )
            
            # 6. ìµœì†Œ ì„ê³„ê°’ ê²€ì‚¬ (íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸)
            if best_hist_score < 0.35:  # 0.3 â†’ 0.35ë¡œ ì¦ê°€ (ë” ì—„ê²©í•œ íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­)
                continue
            
            # 7. ë” ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë§¤ì¹­ ì„ íƒ
            if total_score > best_score:
                best_score = total_score
                best_match_id = pid
                best_metrics = {
                    'hist_score': best_hist_score,
                    'spatial_score': spatial_score,
                    'size_score': size_score,
                    'time_score': time_score,
                    'hist_scores': hist_scores
                }
        
        return best_match_id, best_score, best_metrics
    
    def person_detection_worker(self):
        """ì‚¬ëŒ ê°ì§€ ì›Œì»¤ (MediaPipe ë§¤ì¹­ ë¡œì§ ì‚¬ìš©)"""
        while self.running:
            try:
                frame_data = self.frame_queue.get(timeout=1.0)
                if frame_data is None:
                    break
                
                frame, frame_id, elapsed_time = frame_data
                
                # ì‚¬ëŒ ê°ì§€
                results = self.person_model(frame, classes=[0], 
                                          imgsz=256, conf=0.6, verbose=False, device=0)
                
                detections = []
                for result in results:
                    if result.masks is None:
                        continue
                        
                    for i in range(len(result.boxes)):
                        seg = result.masks.data[i]
                        box = result.boxes[i]
                        confidence = box.conf[0].item()
                        
                        if confidence < self.min_detection_confidence:
                            continue
                        
                        # ë§ˆìŠ¤í¬ ì²˜ë¦¬
                        mask = seg.cpu().numpy().astype(np.uint8) * 255
                        mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), 
                                                interpolation=cv2.INTER_NEAREST)
                        
                        # ë…¸ì´ì¦ˆ ì œê±°
                        kernel = np.ones((5,5), np.uint8)
                        mask_cleaned = cv2.morphologyEx(mask_resized, cv2.MORPH_CLOSE, kernel)
                        mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)
                        
                        # MediaPipe ìˆ˜ì¤€ íˆìŠ¤í† ê·¸ë¨
                        combined_hist, h_hist, s_hist, v_hist = self.extract_histogram(frame, mask_cleaned)
                        
                        bbox = box.xyxy[0].cpu().numpy()
                        x1, y1, x2, y2 = bbox
                        area = (x2 - x1) * (y2 - y1)
                        
                        if area < self.min_person_area:
                            continue
                        
                        detections.append({
                            'hist': combined_hist,
                            'bbox': bbox,
                            'mask': mask_cleaned,
                            'confidence': confidence,
                            'area': area
                        })
                
                # ë§¤ì¹­ ì²˜ë¦¬ (MediaPipe ë°©ì‹)
                detections.sort(key=lambda x: x['area'], reverse=True)  # í° ì‚¬ëŒë¶€í„°
                used_ids = set()
                matched_people = []
                
                for detection in detections:
                    matched_id, match_score, metrics = self.find_best_match(
                        detection['hist'], detection['bbox'], used_ids, elapsed_time)
                    
                    if matched_id is not None and match_score > self.match_threshold:
                        # ê¸°ì¡´ ì‚¬ëŒ ì—…ë°ì´íŠ¸
                        self.people_data[matched_id]['histograms'].append(detection['hist'])
                        self.people_data[matched_id]['bboxes'].append(detection['bbox'])
                        self.people_data[matched_id]['timestamps'].append(elapsed_time)
                        used_ids.add(matched_id)
                        
                        # íˆìŠ¤í† ê·¸ë¨ ë©”ëª¨ë¦¬ ê´€ë¦¬
                        if len(self.people_data[matched_id]['histograms']) > self.max_histograms_per_person:
                            self.people_data[matched_id]['histograms'].pop(0)
                            self.people_data[matched_id]['bboxes'].pop(0)
                            self.people_data[matched_id]['timestamps'].pop(0)
                        
                        person_id = matched_id
                        color = self.get_person_color(matched_id)
                        
                        # ë§¤ì¹­ ì •ë³´ ë””ë²„ê¹… (ì£¼ê¸°ì ìœ¼ë¡œ)
                        if frame_id % 120 == 0 and metrics:
                            print(f"   ğŸ” ë§¤ì¹­: {matched_id} (ì ìˆ˜: {match_score:.3f})")
                            print(f"      - íˆìŠ¤í† ê·¸ë¨: {metrics['hist_score']:.3f}")
                            print(f"      - ê³µê°„ì : {metrics['spatial_score']:.3f}")
                            print(f"      - í¬ê¸°: {metrics['size_score']:.3f}")
                            print(f"      - ì‹œê°„: {metrics['time_score']:.3f}")
                    else:
                        # ìƒˆë¡œìš´ ì‚¬ëŒ
                        new_id = f"Person_{self.next_id}"
                        self.people_data[new_id] = {
                            'histograms': [detection['hist']],
                            'bboxes': [detection['bbox']],
                            'timestamps': [elapsed_time]
                        }
                        self.next_id += 1
                        used_ids.add(new_id)
                        
                        person_id = new_id
                        color = self.get_person_color(new_id)
                        
                        if frame_id % 120 == 0:
                            print(f"   ğŸ†• ìƒˆ ì‚¬ëŒ: {new_id} (ë§¤ì¹­ ì‹¤íŒ¨)")
                    
                    matched_people.append({
                        'id': person_id,
                        'bbox': detection['bbox'],
                        'confidence': detection['confidence'],
                        'color': color,
                        'match_score': match_score if matched_id else 0.0
                    })
                
                # ê²°ê³¼ ì €ì¥
                with self.lock:
                    self.latest_detections = matched_people
                
                self.frame_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ì‚¬ëŒ ê°ì§€ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                break
    
    def normalize_keypoints(self, keypoints):
        """í‚¤í¬ì¸íŠ¸ ì •ê·œí™” (í•™ìŠµ ì‹œì™€ ë™ì¼)"""
        if keypoints.shape[0] == 0:
            return keypoints
        
        valid_mask = keypoints[:, :, 2] >= 0.3  # ì‹ ë¢°ë„ 0.3 ì´ìƒ
        
        for frame_idx in range(keypoints.shape[0]):
            frame_kpts = keypoints[frame_idx]
            valid_joints = valid_mask[frame_idx]
            
            if np.any(valid_joints):
                valid_coords = frame_kpts[valid_joints][:, :2]
                
                # ì¤‘ì‹¬ì ê³¼ ìŠ¤ì¼€ì¼ ê³„ì‚°
                center = np.mean(valid_coords, axis=0)
                distances = np.linalg.norm(valid_coords - center, axis=1)
                scale = np.mean(distances) + 1e-6
                
                # ì •ê·œí™” ì ìš©
                frame_kpts[:, :2] = (frame_kpts[:, :2] - center) / scale
                keypoints[frame_idx] = frame_kpts
        
        return keypoints
    
    def detect_static_pose(self, current_keypoints, previous_keypoints):
        """ì •ì  ìì„¸ ê°ì§€ (ì›€ì§ì„ì´ ê±°ì˜ ì—†ìœ¼ë©´ True)"""
        if previous_keypoints is None:
            return False
        
        # ì£¼ìš” ê´€ì ˆì ë“¤ì˜ ì›€ì§ì„ ê³„ì‚°
        movement_threshold = 25.0  # 10.0 â†’ 25.0ìœ¼ë¡œ ì¦ê°€ (ëœ ë¯¼ê°í•˜ê²Œ)
        total_movement = 0.0
        valid_joints = 0
        
        for joint_idx in [5, 6, 7, 8, 9, 10]:  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            if (current_keypoints[joint_idx][2] > 0.3 and 
                previous_keypoints[joint_idx][2] > 0.3):
                
                curr_pos = current_keypoints[joint_idx][:2]
                prev_pos = previous_keypoints[joint_idx][:2]
                movement = np.linalg.norm(curr_pos - prev_pos)
                total_movement += movement
                valid_joints += 1
        
        if valid_joints == 0:
            return False
        
        avg_movement = total_movement / valid_joints
        is_static = avg_movement < movement_threshold
        
        # ë””ë²„ê¹…ìš© (30í”„ë ˆì„ë§ˆë‹¤ ì¶œë ¥)
        if hasattr(self, 'debug_frame_count') and self.debug_frame_count % 30 == 0:
            print(f"   ì›€ì§ì„ ë¶„ì„: í‰ê·  {avg_movement:.1f}px, ì„ê³„ê°’ {movement_threshold}px â†’ {'ì •ì ' if is_static else 'ë™ì '}")
        
        return is_static

    def gesture_recognition_worker(self):
        """ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ (ê°€ì¥ í° ì‚¬ëŒì˜ poseë§Œ ê°ì§€)"""
        previous_keypoints = None
        self.debug_frame_count = 0  # ë””ë²„ê¹…ìš© ì¹´ìš´íŠ¸
        
        while self.running:
            try:
                frame_data = self.gesture_queue.get(timeout=1.0)
                if frame_data is None:
                    break
                
                frame, frame_id, elapsed_time = frame_data
                self.debug_frame_count = frame_id  # í”„ë ˆì„ ì¹´ìš´íŠ¸ ë™ê¸°í™”
                
                if not self.enable_gesture_recognition:
                    self.gesture_queue.task_done()
                    continue
                
                # ê¸°ë³¸ê°’ ìœ ì§€ (ìƒˆë¡œìš´ íŒë‹¨ì´ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€)
                with self.lock:
                    prediction, confidence, keypoints_detected, current_keypoints = self.latest_gesture
                    latest_detections = self.latest_detections.copy()
        
                # ê°€ì¥ í° ì‚¬ëŒ(ì£¼ìš” ëŒ€ìƒ) ì°¾ê¸°
                target_person = None
                if latest_detections:
                    # ë©´ì ì´ ê°€ì¥ í° ì‚¬ëŒ ì„ íƒ
                    target_person = max(latest_detections, key=lambda p: 
                        (p['bbox'][2] - p['bbox'][0]) * (p['bbox'][3] - p['bbox'][1]))
                    
                    if frame_id % 120 == 0:
                        print(f"ğŸ¯ ì£¼ìš” ëŒ€ìƒ: {target_person['id']} (ë©´ì : {(target_person['bbox'][2] - target_person['bbox'][0]) * (target_person['bbox'][3] - target_person['bbox'][1]):.0f})")
                
                # YOLO Poseë¡œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (ê°€ì¥ í° ì‚¬ëŒë§Œ)
                if target_person is not None:
                    # ì£¼ìš” ëŒ€ìƒì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ROI ì„¤ì •
                    x1, y1, x2, y2 = map(int, target_person['bbox'])
                    
                    # ROI í™•ì¥ (ë” ë„“ì€ ì˜ì—­ì—ì„œ pose ê°ì§€)
                    margin = 50
                    x1 = max(0, x1 - margin)
                    y1 = max(0, y1 - margin)
                    x2 = min(frame.shape[1], x2 + margin)
                    y2 = min(frame.shape[0], y2 + margin)
                    
                    # ROI ì¶”ì¶œ
                    roi = frame[y1:y2, x1:x2]
                    
                    if roi.size > 0:  # ROIê°€ ìœ íš¨í•œ ê²½ìš°
                        # ROIì—ì„œ pose ê°ì§€
                        results = self.pose_model(roi, imgsz=256, conf=0.3, 
                                                verbose=False, device=0)
                        
                        keypoints_data = []
                        
                        for result in results:
                            if result.keypoints is not None:
                                keypoints = result.keypoints.data.cpu().numpy()  # (N, 17, 3)
                                
                                for person_idx, person_kpts in enumerate(keypoints):
                                    # ìƒì²´ ê´€ì ˆì ë§Œ ì¶”ì¶œ
                                    upper_body_kpts = person_kpts[self.upper_body_joints]  # (9, 3)
                                    
                                    # ì‹ ë¢°ë„ ì²´í¬
                                    valid_joints = upper_body_kpts[:, 2] >= 0.2
                                    valid_count = np.sum(valid_joints)
                                    
                                    if frame_id % 120 == 0:
                                        print(f"   ì£¼ìš” ëŒ€ìƒ {target_person['id']}: {valid_count}/9 í‚¤í¬ì¸íŠ¸ (ì–´ê¹¨:{person_kpts[5][2]:.2f}/{person_kpts[6][2]:.2f})")
                                    
                                    if valid_count >= 3:
                                        # ROI ì¢Œí‘œë¥¼ ì „ì²´ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                                        person_kpts[:, 0] += x1
                                        person_kpts[:, 1] += y1
                                        
                                        keypoints_data.append(upper_body_kpts)
                                        keypoints_detected = True
                                        current_keypoints = person_kpts  # ì „ì²´ 17ê°œ í‚¤í¬ì¸íŠ¸ ì €ì¥ (ì‹œê°í™”ìš©)
                                        break  # ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ ì‚¬ìš©
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ë©´ ì²˜ë¦¬
                        if len(keypoints_data) > 0:
                            current_keypoints_data = keypoints_data[0]
                            # ì´ì „ í‚¤í¬ì¸íŠ¸ ì €ì¥
                            previous_keypoints = current_keypoints.copy()
                        else:
                            # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì´ì „ í”„ë ˆì„ ì‚¬ìš© (íŒ¨ë”©)
                            if hasattr(self, 'last_valid_keypoints') and self.last_valid_keypoints is not None:
                                current_keypoints_data = self.last_valid_keypoints
                                if frame_id % 120 == 0:
                                    print(f"   âš ï¸ ì£¼ìš” ëŒ€ìƒ í‚¤í¬ì¸íŠ¸ ì—†ìŒ - ì´ì „ í”„ë ˆì„ ì‚¬ìš©")
                            else:
                                # ì•„ì˜ˆ ì²˜ìŒì´ë©´ ê¸°ë³¸ê°’ ìƒì„±
                                current_keypoints_data = np.zeros((9, 3))
                                if frame_id % 120 == 0:
                                    print(f"   âš ï¸ ì£¼ìš” ëŒ€ìƒ í‚¤í¬ì¸íŠ¸ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
                        
                        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ ì €ì¥
                        if len(keypoints_data) > 0:
                            self.last_valid_keypoints = keypoints_data[0]
                    else:
                        # ROIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
                        if hasattr(self, 'last_valid_keypoints') and self.last_valid_keypoints is not None:
                            current_keypoints_data = self.last_valid_keypoints
                        else:
                            current_keypoints_data = np.zeros((9, 3))
                        keypoints_detected = False
                        current_keypoints = None
                else:
                    # ê°ì§€ëœ ì‚¬ëŒì´ ì—†ëŠ” ê²½ìš°
                    if hasattr(self, 'last_valid_keypoints') and self.last_valid_keypoints is not None:
                        current_keypoints_data = self.last_valid_keypoints
                    else:
                        current_keypoints_data = np.zeros((9, 3))
                    keypoints_detected = False
                    current_keypoints = None
                
                # ì •ìƒì ì¸ ì œìŠ¤ì²˜ ë¶„ì„ (í•­ìƒ ì‹¤í–‰ - íŒ¨ë”©ëœ ë°ì´í„°ë¼ë„ ì²˜ë¦¬)
                self.gesture_frame_buffer.append(current_keypoints_data)
                
                # 3ì´ˆ(90í”„ë ˆì„) ë‹¨ìœ„ë¡œ íŒë‹¨
                frames_since_last_decision = frame_id - self.last_gesture_decision_frame
                
                if (len(self.gesture_frame_buffer) >= self.min_gesture_frames and 
                    frames_since_last_decision >= self.gesture_decision_interval):
                    
                    print(f"ğŸ¯ [Frame {frame_id}] 3ì´ˆ ë‹¨ìœ„ ì œìŠ¤ì²˜ íŒë‹¨ ì‹œì‘!")
                    
                    try:
                        # í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ì „ì²˜ë¦¬
                        keypoints_sequence = list(self.gesture_frame_buffer)
                        keypoints_array = np.array(keypoints_sequence)  # (T, 9, 3)
                    
                        # ì •ê·œí™”
                        normalized_keypoints = self.normalize_keypoints(keypoints_array)
                    
                        # ì •í™•íˆ 90í”„ë ˆì„ìœ¼ë¡œ ë§ì¶”ê¸°
                        T, V, C = normalized_keypoints.shape
                        target_frames = 90
                    
                        if T < target_frames:
                            # íŒ¨ë”©
                            padding_frames = target_frames - T
                            last_frame = normalized_keypoints[-1:].repeat(padding_frames, axis=0)
                            normalized_keypoints = np.concatenate([normalized_keypoints, last_frame], axis=0)
                        elif T > target_frames:
                            # ìµœì‹  90í”„ë ˆì„ ì‚¬ìš©
                            normalized_keypoints = normalized_keypoints[-target_frames:]
                    
                        # Shift-GCN ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜: (C, T, V, M)
                        shift_gcn_data = np.zeros((C, target_frames, V, 1))
                        shift_gcn_data[:, :, :, 0] = normalized_keypoints.transpose(2, 0, 1)
                    
                        # ëª¨ë¸ ì˜ˆì¸¡
                        input_tensor = torch.FloatTensor(shift_gcn_data).unsqueeze(0).to(device)
                    
                        with torch.no_grad():
                            outputs = self.gesture_model(input_tensor)
                            probabilities = torch.softmax(outputs, dim=1)
                            predicted_class = torch.argmax(outputs, dim=1).item()
                            confidence = probabilities[0][predicted_class].item()
                    
                        prediction = self.actions[predicted_class]
                        self.current_gesture_confidence = confidence
                        
                        # ê²°ì • ê¸°ë¡
                        self.last_gesture_decision_frame = frame_id
                        
                        # ë²„í¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ 3ì´ˆ êµ¬ê°„ ì‹œì‘)
                        self.gesture_frame_buffer.clear()
                        self.static_detection_frames = 0  # ì •ì  ì¹´ìš´í„° ë¦¬ì…‹
                        
                        # COME íŒë‹¨ ì‹œ ë³´í˜¸ ê¸°ê°„ ì„¤ì •
                        if prediction == "COME":
                            print(f"   ğŸ›¡ï¸ COME íŒë‹¨ - ë³´í˜¸ ê¸°ê°„ ì—†ìŒ")
                        
                        print(f"ğŸ¯ [3ì´ˆ íŒë‹¨] {prediction} ({confidence:.3f}) | Raw: [{outputs[0][0]:.2f}, {outputs[0][1]:.2f}]")
                        
                        # COME ì œìŠ¤ì²˜ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                        if prediction == "COME":
                            print(f"   ğŸ” COME ê°ì§€! ìƒì„¸ ë¶„ì„:")
                            print(f"      - Raw outputs: [{outputs[0][0]:.4f}, {outputs[0][1]:.4f}]")
                            print(f"      - Softmax probs: [{probabilities[0][0]:.4f}, {probabilities[0][1]:.4f}]")
                            print(f"      - Predicted class: {predicted_class} -> {prediction}")
                        elif prediction == "NORMAL" and confidence < 0.7:  # ë‚®ì€ ì‹ ë¢°ë„ì¼ ë•Œ
                            print(f"   ğŸ” ë‚®ì€ ì‹ ë¢°ë„ NORMAL ê°ì§€:")
                            print(f"      - Raw outputs: [{outputs[0][0]:.4f}, {outputs[0][1]:.4f}]")
                            print(f"      - Softmax probs: [{probabilities[0][0]:.4f}, {probabilities[0][1]:.4f}]")
                            print(f"      - Predicted class: {predicted_class} -> {prediction}")
                        
                    except Exception as e:
                        print(f"âŒ ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                        prediction = "NORMAL"
                        confidence = 0.5
                
                # ê²°ê³¼ ì €ì¥ (í‚¤í¬ì¸íŠ¸ ì •ë³´ í¬í•¨)
                # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if keypoints_detected and current_keypoints is not None:
                    # ìƒˆë¡œìš´ íŒë‹¨ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì €ì¥
                    with self.lock:
                        self.latest_gesture = (prediction, confidence, True, current_keypoints)
                # í‚¤í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ì „ ê°’ ìœ ì§€ (ì—…ë°ì´íŠ¸ ì•ˆ í•¨)
                
                self.gesture_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                break
            
    def warmup_models(self, frame):
        """ëª¨ë¸ ì›Œë°ì—… (ì²« ì¶”ë¡  ì†ë„ ê°œì„ )"""
        print("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì‹œì‘...")
        
        # YOLO ëª¨ë¸ë“¤ ì›Œë°ì—…
        try:
            _ = self.person_model(frame, classes=[0], imgsz=256, conf=0.6, verbose=False, device=0)
            _ = self.pose_model(frame, imgsz=256, conf=0.3, verbose=False, device=0)  # 320â†’256ìœ¼ë¡œ ì¶•ì†Œ
            print("âœ… YOLO ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ YOLO ì›Œë°ì—… ì˜¤ë¥˜: {e}")
        
        # Shift-GCN ëª¨ë¸ ì›Œë°ì—…
        if self.enable_gesture_recognition:
            try:
                dummy_input = torch.randn(1, 3, 90, 9, 1).to(device)
                with torch.no_grad():
                    _ = self.gesture_model(dummy_input)
                print("âœ… Shift-GCN ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ Shift-GCN ì›Œë°ì—… ì˜¤ë¥˜: {e}")
        
        print("ğŸš€ ëª¨ë“  ëª¨ë¸ ì›Œë°ì—… ì™„ë£Œ!")

    def wait_for_workers_ready(self, timeout=5.0):
        """ì›Œì»¤ë“¤ì´ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        print("â³ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¤€ë¹„ ëŒ€ê¸°...")
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            with self.lock:
                # ì›Œì»¤ê°€ ìµœì†Œ í•œ ë²ˆì€ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if len(self.latest_detections) > 0 or self.latest_gesture[2]:  # detections ìˆê±°ë‚˜ keypoints ê°ì§€ë¨
                    print("âœ… ì›Œì»¤ ìŠ¤ë ˆë“œ ì¤€ë¹„ ì™„ë£Œ!")
                    return True
            time.sleep(0.1)
        
        print("âš ï¸ ì›Œì»¤ ì¤€ë¹„ íƒ€ì„ì•„ì›ƒ (ì •ìƒ ì§„í–‰)")
        return False
    
    def run_system(self, camera_device="/dev/video0"):
        """ğŸš€ ìƒˆë¡œìš´ í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „ ìƒˆë¡œìš´ í†µí•© ì‹œìŠ¤í…œ ì‹œì‘!")
        print(f"ğŸ“¹ ì¹´ë©”ë¼: {camera_device}")
        print("âš¡ MediaPipe ë§¤ì¹­ + YOLO Pose + Shift-GCN + ë©€í‹°ìŠ¤ë ˆë”©")
        print("ğŸ”§ ì´ˆê¸° ê°ì§€ ì§€ì—° í•´ê²° ë²„ì „!")
        
        cap = cv2.VideoCapture(camera_device)
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {camera_device}")
            return
        
        # ì°½ ì„¤ì • ì¶”ê°€ (ë¬¸ì œ í•´ê²°)
        window_name = "ğŸš€ NEW Integrated System"
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)
        cv2.resizeWindow(window_name, 800, 600)  # ì ë‹¹í•œ ì°½ í¬ê¸°
        cv2.moveWindow(window_name, 50, 50)
        print("ğŸ–¼ï¸  ì°½ ì„¤ì • ì™„ë£Œ!")
        
        # ì¹´ë©”ë¼ í•´ìƒë„ë¥¼ VGAë¡œ ì„¤ì • (ì„±ëŠ¥ ìš°ì„ )
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ í¬ê¸° ìµœì†Œí™”
        cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)  # ìë™ ë…¸ì¶œ ì„¤ì •
        
        # ì‹¤ì œ ì„¤ì •ëœ í•´ìƒë„ í™•ì¸
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"ğŸ“¹ ì¹´ë©”ë¼ í•´ìƒë„: {actual_width}x{actual_height} (ì„±ëŠ¥ ìµœì í™”)")
        print("ğŸ“¹ ì¹´ë©”ë¼ ì„¤ì • ìµœì í™” ì™„ë£Œ!")
        
        # ì²« í”„ë ˆì„ìœ¼ë¡œ ì°½ í…ŒìŠ¤íŠ¸
        ret, test_frame = cap.read()
        if ret:
            cv2.putText(test_frame, "ğŸš€ System Starting...", (50, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
            cv2.imshow(window_name, test_frame)
            cv2.waitKey(100)  # 100ms ëŒ€ê¸°
            print("ğŸ–¼ï¸  ì´ˆê¸° ì°½ í‘œì‹œ ì™„ë£Œ!")
            
            # ëª¨ë¸ ì›Œë°ì—… (ì´ˆê¸° ì§€ì—° í•´ê²°)
            self.warmup_models(test_frame)
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        workers = []
        workers.append(self.executor.submit(self.person_detection_worker))
        workers.append(self.executor.submit(self.gesture_recognition_worker))
        print("ğŸ”¥ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘!")
        
        # ì›Œì»¤ ì¤€ë¹„ ëŒ€ê¸° (ì´ˆê¸° ê°ì§€ ì§€ì—° í•´ê²°)
        workers_ready = self.wait_for_workers_ready(timeout=3.0)
        if not workers_ready:
            print("âš ï¸ ì›Œì»¤ ì¤€ë¹„ ì™„ë£Œ ì „ì— ì§„í–‰ (ì¼ë¶€ ì´ˆê¸° í”„ë ˆì„ì—ì„œ ê°ì§€ ì•ˆ ë  ìˆ˜ ìˆìŒ)")
        
        frame_count = 0
        start_time = datetime.now()
        fps_times = deque(maxlen=30)
        
        current_gesture = "NORMAL"
        current_confidence = self.current_gesture_confidence
        
        try:
            while cap.isOpened():
                frame_start = time.time()
                
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                elapsed_time = (datetime.now() - start_time).total_seconds()
                
                # í”„ë ˆì„ì„ ì›Œì»¤ë“¤ì—ê²Œ ì „ë‹¬
                frame_data = (frame.copy(), frame_count, elapsed_time)
                
                # ì‚¬ëŒ ê°ì§€ ì›Œì»¤ì— í”„ë ˆì„ ì „ë‹¬ (2í”„ë ˆì„ë§ˆë‹¤ - ì„±ëŠ¥ ìµœì í™”)
                if frame_count % 2 == 0:
                    try:
                        self.frame_queue.put_nowait(frame_data)
                    except queue.Full:
                        pass
                
                # ì œìŠ¤ì²˜ ì¸ì‹ ì›Œì»¤ì— í”„ë ˆì„ ì „ë‹¬ (ë§¤ í”„ë ˆì„ - í•™ìŠµê³¼ ë™ì¼)
                try:
                    self.gesture_queue.put_nowait(frame_data)
                except queue.Full:
                    pass
                
                # ìµœì‹  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                with self.lock:
                    latest_detections = self.latest_detections.copy()
                    latest_gesture = self.latest_gesture
                
                # ì œìŠ¤ì²˜ ê²°ê³¼ ì—…ë°ì´íŠ¸
                gesture_prediction, gesture_confidence, keypoints_detected, current_keypoints = latest_gesture
                if gesture_prediction != current_gesture and gesture_confidence > 0.1:  # 0.3 â†’ 0.1ìœ¼ë¡œ ë” ë‚®ì¶¤
                    current_gesture = gesture_prediction
                    current_confidence = gesture_confidence
                    print(f"âœ‹ ì œìŠ¤ì²˜ ë³€í™”: {gesture_prediction} ({gesture_confidence:.2f})")
                
                # í™”ë©´ êµ¬ì„± (í•­ìƒ ì›ë³¸ í”„ë ˆì„ ê¸°ë°˜)
                annotated = frame.copy()
                
                # ì‚¬ëŒ ì‹œê°í™” (ì›Œì»¤ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                if latest_detections:
                    for person in latest_detections:
                        x1, y1, x2, y2 = map(int, person['bbox'])
                        color = person['color']
                        
                        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                        cv2.putText(annotated, person['id'], (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                        cv2.putText(annotated, f"Conf: {person['confidence']:.2f}", 
                                   (x1, y2+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                        if person['match_score'] > 0:
                            cv2.putText(annotated, f"Match: {person['match_score']:.2f}", 
                                       (x1, y2+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ê´€ì ˆì  ì‹œê°í™” (í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°)
                if keypoints_detected and current_keypoints is not None:
                    # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ ìƒ‰ìƒ ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
                    keypoint_color = latest_detections[0]['color'] if latest_detections else (0, 255, 255)
                    annotated = self.draw_keypoints(annotated, current_keypoints, keypoint_color)
                    
                    # ì œìŠ¤ì²˜ ì¸ì‹ìš© í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í‘œì‹œ (9ê°œ ê¸°ì¤€)
                    gesture_keypoints = current_keypoints[self.upper_body_joints]  # 9ê°œ ì¶”ì¶œ
                    valid_gesture_keypoints = np.sum(gesture_keypoints[:, 2] > 0.3)
                    cv2.putText(annotated, f"Gesture KPts: {valid_gesture_keypoints}/9", 
                               (10, 250), cv2.FONT_HERSHEY_SIMPLEX, 0.6, keypoint_color, 2)
                
                # FPS ê³„ì‚°
                frame_time = time.time() - frame_start
                fps_times.append(frame_time)
                fps = 1.0 / (sum(fps_times) / len(fps_times)) if fps_times else 0
                
                # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ (í¬ê²Œ í‘œì‹œ)
                cv2.putText(annotated, f"ğŸš€ NEW System FPS: {fps:.1f}", (10, 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                cv2.putText(annotated, f"People: {len(latest_detections)}", (10, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                cv2.putText(annotated, f"Gesture: {current_gesture}", (10, 100), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 0, 255), 3)
                cv2.putText(annotated, f"Confidence: {current_confidence:.2f}", (10, 130), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 255), 2)
                cv2.putText(annotated, f"Keypoints: {'OK' if keypoints_detected else 'NONE'}", 
                           (10, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
                           (0, 255, 0) if keypoints_detected else (0, 0, 255), 2)
                cv2.putText(annotated, f"Frame: {frame_count}", (10, 190), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # 3ì´ˆ ë‹¨ìœ„ íŒë‹¨ ì •ë³´ í‘œì‹œ
                frames_to_next_decision = self.gesture_decision_interval - (frame_count - self.last_gesture_decision_frame)
                if frames_to_next_decision > 0:
                    cv2.putText(annotated, f"Next Decision: {frames_to_next_decision}f", 
                               (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                else:
                    cv2.putText(annotated, f"Ready for Decision", 
                               (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # ì›Œì»¤ ìƒíƒœ í‘œì‹œ
                worker_status = f"Workers Ready: {workers_ready}" if frame_count < 100 else "Workers: Running"
                cv2.putText(annotated, worker_status, (10, 220), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # í™”ë©´ í‘œì‹œ (ë§¤ í”„ë ˆì„, ì°½ì€ ì´ë¯¸ ì„¤ì •ë¨)
                cv2.imshow(window_name, annotated)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                if frame_count % 120 == 0:  # 60â†’120í”„ë ˆì„ë§ˆë‹¤ë§Œ ì¶œë ¥
                    print(f"ğŸ“Š FPS: {fps:.1f} | People: {len(latest_detections)} | Gesture: {current_gesture}")
                    print(f"   í™”ë©´ í™•ì¸: ë°ê¸° {np.mean(annotated):.1f}, í¬ê¸° {annotated.shape}")
                    print(f"   ì œìŠ¤ì²˜ ìƒíƒœ: ë²„í¼ {len(self.gesture_frame_buffer)}/90, ë‹¤ìŒ íŒë‹¨ê¹Œì§€ {frames_to_next_decision}í”„ë ˆì„")
        
        except KeyboardInterrupt:
            print("ğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨")
        
        finally:
            # ì •ë¦¬
            self.running = False
            
            # ì¢…ë£Œ ì‹ í˜¸
            try:
                self.frame_queue.put_nowait(None)
                self.gesture_queue.put_nowait(None)
            except queue.Full:
                pass
            
            # ì›Œì»¤ ëŒ€ê¸°
            for worker in workers:
                try:
                    worker.result(timeout=2)
                except:
                    pass
            
            self.executor.shutdown(wait=True)
            cap.release()
            cv2.destroyAllWindows()
            
            print(f"\nğŸ‰ ì‹œìŠ¤í…œ ì¢…ë£Œ")
            print(f"   - ì´ í”„ë ˆì„: {frame_count}")
            print(f"   - ìµœì¢… FPS: {fps:.1f}")
            print(f"   - ê°ì§€ëœ ì‚¬ëŒ: {len(self.people_data)}")

    def test_model_performance(self):
        """í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        if not self.enable_gesture_recognition:
            print("âŒ ì œìŠ¤ì²˜ ì¸ì‹ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        try:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            come_data_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/shift_gcn_data/come_pose_data.npy'
            normal_data_path = '/home/ckim/ros-repo-4/deeplearning/gesture_recognition/shift_gcn_data/normal_pose_data.npy'
            
            if os.path.exists(come_data_path) and os.path.exists(normal_data_path):
                come_data = np.load(come_data_path)
                normal_data = np.load(normal_data_path)
                
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: COME {come_data.shape}, NORMAL {normal_data.shape}")
                
                # ê° í´ë˜ìŠ¤ì—ì„œ ëª‡ ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
                test_samples = 5
                come_correct = 0
                normal_correct = 0
                
                # COME ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
                for i in range(min(test_samples, len(come_data))):
                    sample = come_data[i]  # (3, 90, 9, 1)
                    input_tensor = torch.FloatTensor(sample).unsqueeze(0).to(device)
                    
                    with torch.no_grad():
                        outputs = self.gesture_model(input_tensor)
                        probabilities = torch.softmax(outputs, dim=1)
                        predicted_class = torch.argmax(outputs, dim=1).item()
                        confidence = probabilities[0][predicted_class].item()
                    
                    prediction = self.actions[predicted_class]
                    if prediction == "COME":
                        come_correct += 1
                    
                    print(f"   COME ìƒ˜í”Œ {i+1}: {prediction} ({confidence:.3f}) | Raw: [{outputs[0][0]:.2f}, {outputs[0][1]:.2f}]")
                
                # NORMAL ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
                for i in range(min(test_samples, len(normal_data))):
                    sample = normal_data[i]  # (3, 90, 9, 1)
                    input_tensor = torch.FloatTensor(sample).unsqueeze(0).to(device)
                    
                    with torch.no_grad():
                        outputs = self.gesture_model(input_tensor)
                        probabilities = torch.softmax(outputs, dim=1)
                        predicted_class = torch.argmax(outputs, dim=1).item()
                        confidence = probabilities[0][predicted_class].item()
                    
                    prediction = self.actions[predicted_class]
                    if prediction == "NORMAL":
                        normal_correct += 1
                    
                    print(f"   NORMAL ìƒ˜í”Œ {i+1}: {prediction} ({confidence:.3f}) | Raw: [{outputs[0][0]:.2f}, {outputs[0][1]:.2f}]")
                
                come_acc = come_correct / test_samples * 100
                normal_acc = normal_correct / test_samples * 100
                
                print(f"ğŸ“Š ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
                print(f"   - COME ì •í™•ë„: {come_acc:.1f}% ({come_correct}/{test_samples})")
                print(f"   - NORMAL ì •í™•ë„: {normal_acc:.1f}% ({normal_correct}/{test_samples})")
                
                if come_acc < 60 or normal_acc < 60:
                    print("âš ï¸ ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    print("âœ… ëª¨ë¸ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
                    
            else:
                print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    system = NewIntegratedSystem()
    system.run_system(camera_device="/dev/video0")
"""
ê³µìœ  ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë“€ì–¼ ì¹´ë©”ë¼ í†µí•© ì‹œìŠ¤í…œ
AI ì„œë²„ì—ì„œ ê³µìœ  ë©”ëª¨ë¦¬ì— ì €ì¥í•œ í”„ë ˆì„ì„ ì½ì–´ì™€ì„œ ë”¥ëŸ¬ë‹ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import cv2
import time
import os
import threading
import numpy as np
from collections import deque
import sys
import requests # ì¶”ê°€ëœ ì„í¬íŠ¸
import gc # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì¶”ê°€
from flask import Flask, request, jsonify  # Flask ì¶”ê°€

# CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • (ë””ë²„ê·¸ ì‹œì—ë§Œ ê°•ì œ ë™ê¸°í™”/ìºì‹œ ë¹„í™œì„±í™”)
if os.environ.get('GPU_DEBUG', '0') == '1':
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:32,garbage_collection_threshold:0.4'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['CUDA_CACHE_DISABLE'] = '1'
    os.environ['TORCH_CUDNN_V8_API_DISABLED'] = '1'
    os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
# ê¸°ë³¸ ë©”ëª¨ë¦¬ ì œí•œë§Œ ìœ ì§€
os.environ['CUDA_MEMORY_FRACTION'] = '0.7'  # GPU ë©”ëª¨ë¦¬ì˜ 70%ë§Œ ì‚¬ìš©

# GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì œì–´
def setup_gpu_memory_allocation():
    """GPU ë©”ëª¨ë¦¬ í• ë‹¹ì„ ì œì–´í•˜ì—¬ PersonTrackerì™€ GestureRecognizerê°€ ë‚˜ëˆ ì„œ ì‚¬ìš©"""
    try:
        import torch
        if torch.cuda.is_available():
            # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì œí•œ ì„¤ì •
            total_memory = torch.cuda.get_device_properties(0).total_memory
            
            # PersonTrackerìš© ë©”ëª¨ë¦¬ (40%)
            person_tracker_memory = int(total_memory * 0.4)
            # GestureRecognizerìš© ë©”ëª¨ë¦¬ (30%)
            gesture_recognizer_memory = int(total_memory * 0.3)
            # ë‚˜ë¨¸ì§€ 30%ëŠ” ì—¬ìœ ë¶„
            
            print(f"ğŸ® GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì„¤ì •:")
            print(f"  ğŸ“Š ì´ ë©”ëª¨ë¦¬: {total_memory / 1024**3:.1f}GB")
            print(f"  ğŸ‘¥ PersonTracker: {person_tracker_memory / 1024**3:.1f}GB (40%)")
            print(f"  ğŸ¤² GestureRecognizer: {gesture_recognizer_memory / 1024**3:.1f}GB (30%)")
            print(f"  ğŸ“¦ ì—¬ìœ ë¶„: {(total_memory - person_tracker_memory - gesture_recognizer_memory) / 1024**3:.1f}GB (30%)")
            
            # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ ì œí•œ ì„¤ì •
            torch.cuda.set_per_process_memory_fraction(0.7, 0)  # 70% ì œí•œ
            
            return True
        else:
            print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return False
    except Exception as e:
        print(f"âŒ GPU ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

# GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì„¤ì • ì‹¤í–‰
GPU_MEMORY_SETUP = setup_gpu_memory_allocation()

# SlidingShiftGCN ëª¨ë¸ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/ckim/ros-repo-4/deeplearning/gesture_recognition/src')

# ëª¨ë“ˆ import
from person_tracker import PersonTracker
from gesture_recognizer import GestureRecognizer
from shared_memory_reader import DualCameraSharedMemoryReader

# ê°€ë²¼ìš´ ëª¨ë¸ ì„¤ì •
USE_LIGHTWEIGHT_MODELS = True  # ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš©
if USE_LIGHTWEIGHT_MODELS:
    pass

# ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì¶”ê°€
PERFORMANCE_MODE = True  # ì„±ëŠ¥ ìµœì í™” ëª¨ë“œ
if PERFORMANCE_MODE:
    # í”„ë ˆì„ ì²˜ë¦¬ ì£¼ê¸° ì¡°ì •
    FRAME_PROCESS_INTERVAL = 1  # ë§¤í”„ë ˆì„ ì²˜ë¦¬
    # ë©”ëª¨ë¦¬ ì •ë¦¬ ì£¼ê¸° ì™„í™”
    MEMORY_CLEANUP_INTERVAL = 60  # 60ì´ˆë§ˆë‹¤
    # PersonTracker ì´ˆê¸°í™” ì£¼ê¸° ì™„í™”
    TRACKER_RESET_INTERVAL = 300  # 300í”„ë ˆì„ë§ˆë‹¤ (ì•½ 10ì´ˆ@30FPS)
else:
    FRAME_PROCESS_INTERVAL = 1
    MEMORY_CLEANUP_INTERVAL = 60
    TRACKER_RESET_INTERVAL = 300

# GUI/GStreamer ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['OPENCV_VIDEOIO_PRIORITY_GSTREAMER'] = '0'

# ë¡œì»¬ AI ì„œë²„ ì£¼ì†Œ
AI_SERVER_BASE = "http://localhost:5006"
CAMERA_HFOV_DEGREES = 60.0 # ì¹´ë©”ë¼ ìˆ˜í‰ í™”ê° (ê°€ì •ì¹˜)

# CPU ëª¨ë“œ ê°•ì œ ì„¤ì • (CUDA ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°ìš©)
FORCE_CPU_MODE = os.environ.get("FORCE_CPU_MODE", "false").lower() == "true"
HYBRID_MODE = False  # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì™„ì „ ë¹„í™œì„±í™”

if FORCE_CPU_MODE:
    os.environ['CUDA_VISIBLE_DEVICES'] = ''  # GPU ë¹„í™œì„±í™”
    print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (CUDA ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°)")
else:
    print("ğŸš€ GPU ìµœì í™” ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    try:
        import torch
        if torch.cuda.is_available():
            # ëª¨ë“  CUDA ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            gc.collect()
    except Exception as e:
        print(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


class SharedPersonTracker:
    """ì „ë©´/í›„ë©´ ì¹´ë©”ë¼ ê°„ ì‚¬ëŒ ë§¤ì¹­ì„ ìœ„í•œ ê³µìœ  ì¶”ì ê¸° (ë„í”Œê°±ì–´ ë°©ì§€)"""
    def __init__(self):
        print("ğŸ”„ ê³µìœ  ì‚¬ëŒ ì¶”ì ê¸° ì´ˆê¸°í™”")
        
        # í†µí•©ëœ ì‚¬ëŒ ë°ì´í„° (ë„í”Œê°±ì–´ ë°©ì§€)
        self.unified_people = {}
        self.next_unified_id = 0
        
        # ìµœëŒ€ ê¸°ì–µí•  ì‚¬ëŒ ìˆ˜ ì œí•œ
        self.max_people = 15
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ID í’€ (0-14)
        self.available_ids = set(range(15))
        
        # ì¹´ë©”ë¼ë³„ ìµœì‹  ê°ì§€ ê²°ê³¼ (ì›ë³¸)
        self.camera_raw_detections = {
            'front': [],
            'back': []
        }
        
        # ë§¤ì¹­ ì„¤ì •
        self.cross_camera_match_threshold = 0.7
        self.match_timeout = 15.0
        
        self.lock = threading.Lock()
        print("âœ… ê³µìœ  ì‚¬ëŒ ì¶”ì ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
    def add_frame(self, frame, frame_id, elapsed_time, camera_name):
        """íŠ¹ì • ì¹´ë©”ë¼ì˜ í”„ë ˆì„ ì¶”ê°€ (ì›ë³¸ ê°ì§€ ê²°ê³¼ë§Œ ì €ì¥)"""
        # ì›ë³¸ PersonTrackerë¡œ ê°ì§€ (GPU ëª¨ë“œ)
        if camera_name == 'front':
            if not hasattr(self, 'front_tracker'):
                self.front_tracker = PersonTracker()
            self.front_tracker.add_frame(frame, frame_id, elapsed_time)
            self.camera_raw_detections['front'] = self.front_tracker.get_latest_detections()
        else:
            if not hasattr(self, 'back_tracker'):
                self.back_tracker = PersonTracker()
            self.back_tracker.add_frame(frame, frame_id, elapsed_time)
            self.camera_raw_detections['back'] = self.back_tracker.get_latest_detections()
        
        if frame_id % MEMORY_CLEANUP_INTERVAL == 0:
            self.cleanup_old_mappings(elapsed_time)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ PersonTracker ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if frame_id % 1800 == 0:  # 300 â†’ 1800ìœ¼ë¡œ ëŠ˜ë¦¼ (30ì´ˆ â†’ 3ë¶„)
            print(f"ğŸ”„ {camera_name} ì¹´ë©”ë¼ PersonTracker ì´ˆê¸°í™”")
            if camera_name == 'front':
                self.front_tracker = PersonTracker()
            else:
                self.back_tracker = PersonTracker()
        
    def get_latest_detections(self, camera_name, elapsed_time):
        """íŠ¹ì • ì¹´ë©”ë¼ì˜ ìµœì‹  ê°ì§€ ê²°ê³¼ ë°˜í™˜ (ë„í”Œê°±ì–´ ë°©ì§€)"""
        with self.lock:
            # 1ë‹¨ê³„: í˜„ì¬ ì¹´ë©”ë¼ì˜ ì›ë³¸ ê°ì§€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            raw_detections = self.camera_raw_detections.get(camera_name, [])
            
            # ë„í”Œê°±ì–´ ë°©ì§€: ê°™ì€ í”„ë ˆì„ì—ì„œ ì¤‘ë³µ ID ì œê±°
            seen_unified_ids = set()
            
            # 2ë‹¨ê³„: ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„
            unified_detections = []
            used_unified_ids = set()
            
            for detection in raw_detections:
                person_id = detection['id']
                bbox = detection['bbox']
                
                # ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„
                matched_unified_id = self._find_existing_match(detection, camera_name, elapsed_time)
                
                if matched_unified_id and matched_unified_id not in seen_unified_ids:
                    # ê¸°ì¡´ í†µí•© ID ì‚¬ìš©
                    unified_detection = detection.copy()
                    unified_detection['id'] = matched_unified_id
                    unified_detections.append(unified_detection)
                    seen_unified_ids.add(matched_unified_id)
                    used_unified_ids.add(matched_unified_id)
                    
                    # ë§¤ì¹­ ì •ë³´ ì—…ë°ì´íŠ¸
                    self.unified_people[matched_unified_id]['last_seen'][camera_name] = elapsed_time
                    self.unified_people[matched_unified_id]['bbox'][camera_name] = bbox
                elif matched_unified_id is None:
                    # ìƒˆë¡œìš´ í†µí•© ID í• ë‹¹ (ë„í”Œê°±ì–´ ë°©ì§€)
                    new_unified_id = self._assign_new_unified_id(detection, camera_name, elapsed_time)
                    if new_unified_id not in seen_unified_ids:
                        unified_detection = detection.copy()
                        unified_detection['id'] = new_unified_id
                        unified_detections.append(unified_detection)
                        seen_unified_ids.add(new_unified_id)
                        used_unified_ids.add(new_unified_id)
                # else: ì´ë¯¸ ì‚¬ìš©ëœ IDëŠ” ê±´ë„ˆëœ€ (ë„í”Œê°±ì–´ ë°©ì§€)
            
            return unified_detections
    
    def _find_existing_match(self, detection, camera_name, elapsed_time):
        """ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„"""
        person_id = detection['id']
        bbox = detection['bbox']
        
        for unified_id, data in self.unified_people.items():
            if camera_name in data['camera_ids'] and data['camera_ids'][camera_name] == person_id:
                return unified_id
        
        return None
    
    def _assign_new_unified_id(self, detection, camera_name, elapsed_time):
        """ìƒˆë¡œìš´ í†µí•© ID í• ë‹¹ (ë„í”Œê°±ì–´ ë°©ì§€)"""
        # ìµœëŒ€ ì¸ì› ì œí•œ í™•ì¸
        if len(self.unified_people) >= self.max_people:
            oldest_id = min(self.unified_people.keys(), key=lambda x: self.unified_people[x]['created_time'])
            removed_person = self.unified_people.pop(oldest_id)
            removed_id_num = int(oldest_id.split('_')[1])
            self.available_ids.add(removed_id_num)
            print(f"ğŸ—‘ï¸ ìµœëŒ€ ì¸ì› ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ ì‚¬ëŒ ì œê±°: {oldest_id}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ID ì¤‘ ê°€ì¥ ì‘ì€ ë²ˆí˜¸ ì„ íƒ
        if self.available_ids:
            next_id = min(self.available_ids)
            self.available_ids.remove(next_id)
        else:
            next_id = 0
            print(f"âš ï¸ ëª¨ë“  IDê°€ ì‚¬ìš© ì¤‘, 0ë¶€í„° ì¬ì‹œì‘")
        
        # ìƒˆë¡œìš´ í†µí•© ID ìƒì„±
        unified_id = f"Person_{next_id}"
        
        self.unified_people[unified_id] = {
            'camera_ids': {camera_name: detection['id']},
            'last_seen': {camera_name: elapsed_time},
            'bbox': {camera_name: detection['bbox']},
            'created_time': elapsed_time
        }
        
        print(f"ğŸ†• ìƒˆë¡œìš´ í†µí•© ID ìƒì„±: {detection['id']} â†’ {unified_id}")
        return unified_id
    
    def cleanup_old_mappings(self, elapsed_time):
        """ì˜¤ë˜ëœ ë§¤í•‘ ì •ë¦¬ (ìµœëŒ€ ì¸ì› ì œí•œ ê³ ë ¤)"""
        with self.lock:
            people_to_remove = []
            for unified_id, data in self.unified_people.items():
                # ëª¨ë“  ì¹´ë©”ë¼ì—ì„œ ì¼ì • ì‹œê°„ ì´ìƒ ê°ì§€ë˜ì§€ ì•Šì€ ì‚¬ëŒ ì œê±°
                all_old = True
                for camera_name, last_seen in data['last_seen'].items():
                    if elapsed_time - last_seen < self.match_timeout:
                        all_old = False
                        break
                
                if all_old:
                    people_to_remove.append(unified_id)
            
            # ì˜¤ë˜ëœ ì‚¬ëŒë“¤ ì œê±°
            for unified_id in people_to_remove:
                removed_person = self.unified_people.pop(unified_id)
                # ì œê±°ëœ IDë¥¼ ì‚¬ìš© ê°€ëŠ¥í•œ ID í’€ì— ì¶”ê°€
                removed_id_num = int(unified_id.split('_')[1])
                self.available_ids.add(removed_id_num)


class SingleCameraProcessor:
    """ë‹¨ì¼ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê¸° (ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš©)"""
    def __init__(self, name, shared_tracker):
        self.name = name
        self.shared_tracker = shared_tracker
        self.gesture_recognizer = GestureRecognizer()
        
        self.frame_count = 0
        self.start_time = time.time()
        
        # í”„ë ˆì„ ê°„ ì§€ì—°ì‹œê°„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜
        self.last_frame_time = None
        self.current_delay = 0.0
        
        # predict_webcam_realtime.pyì²˜ëŸ¼ ì•ˆì •ì ì¸ ì´ˆê¸°ê°’ ì„¤ì •
        self.current_gesture = "NORMAL"
        self.current_confidence = 0.5
        self.last_gesture_update_frame = 0  # ë§ˆì§€ë§‰ ì œìŠ¤ì²˜ ì—…ë°ì´íŠ¸ í”„ë ˆì„
        self.gesture_changed = False  # ì œìŠ¤ì²˜ ë³€ê²½ í”Œë˜ê·¸
        self.first_prediction_received = False  # ì²« ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì‹  í”Œë˜ê·¸
        
        self.person_tracker_skip = 1
        self.gesture_recognizer_skip = 2
        self.color_palette = [
            (0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)
        ]
        self.last_tracking_update_time = 0.0 # ì¶”ì  ì—…ë°ì´íŠ¸ ì£¼ê¸° ì œì–´
        self.last_come_sent_ts = 0.0
        self.last_come_person_id = None
        
        # come ì œìŠ¤ì²˜ ìƒíƒœ ê´€ë¦¬ (ai_serverì™€ ë™ê¸°í™”)
        self.come_gesture_active = False
        self.last_come_gesture_time = 0.0

    # ë¹„ë™ê¸° POST ìœ í‹¸
    def _post_async(self, url, payload, timeout=0.2):
        def _worker():
            try:
                requests.post(url, json=payload, timeout=timeout)
            except Exception:
                pass
        t = threading.Thread(target=_worker, daemon=True)
        t.start()

    def process_frame(self, frame):
        if frame is None:
            return None
            
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        current_time = time.time()
        
        # í”„ë ˆì„ ê°„ ì§€ì—°ì‹œê°„ ê³„ì‚°
        if self.last_frame_time is not None:
            self.current_delay = current_time - self.last_frame_time
        self.last_frame_time = current_time

        # AI ëª¨ë¸ìš©ìœ¼ë¡œ í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ (640x480)
        ai_frame = cv2.resize(frame, (640, 480))
        camera_name = 'front' if 'Front' in self.name else 'back'
        if self.frame_count % self.person_tracker_skip == 0:
            self.shared_tracker.add_frame(ai_frame.copy(), self.frame_count, elapsed_time, camera_name)
            latest_detections = self.shared_tracker.get_latest_detections(camera_name, elapsed_time)
        else:
            latest_detections = self.shared_tracker.get_latest_detections(camera_name, elapsed_time)

        # GestureRecognizer: 2í”„ë ˆì„ë§ˆë‹¤ ì‹¤í–‰
        if self.frame_count % self.gesture_recognizer_skip == 0:
            self.gesture_recognizer.add_frame(ai_frame.copy(), self.frame_count, elapsed_time, latest_detections)
            gesture_prediction, gesture_confidence, keypoints_detected, current_keypoints = self.gesture_recognizer.get_latest_gesture()
            
            # predict_webcam_realtime.pyì™€ ë™ì¼: 30í”„ë ˆì„ ìœˆë„ìš°ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ì´ ë‚˜ì™”ì„ ë•Œë§Œ UI ì—…ë°ì´íŠ¸
            # GestureRecognizer ë‚´ë¶€ì—ì„œ 30í”„ë ˆì„ ìŒ“ì˜€ì„ ë•Œë§Œ ì˜ˆì¸¡í•˜ë¯€ë¡œ, ê·¸ë•Œë§Œ UI ë³€ê²½
            if (keypoints_detected and current_keypoints is not None and 
                gesture_confidence > 0.0 and 
                (not self.first_prediction_received or gesture_prediction != self.current_gesture)):
                # predict_webcam_realtime.pyì™€ ë™ì¼: ì œìŠ¤ì²˜ê°€ ì‹¤ì œë¡œ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
                if not self.first_prediction_received:
                    print(f"[{self.name}] ğŸ¯ ì²« ì œìŠ¤ì²˜ ì˜ˆì¸¡: {gesture_prediction} (ì‹ ë¢°ë„: {gesture_confidence:.3f})")
                    self.first_prediction_received = True
                else:
                    print(f"[{self.name}] ğŸ¯ ì œìŠ¤ì²˜ ë³€ê²½: {self.current_gesture} â†’ {gesture_prediction} (ì‹ ë¢°ë„: {gesture_confidence:.3f})")
                self.current_gesture = gesture_prediction
                self.current_confidence = gesture_confidence
                self.last_gesture_update_frame = self.frame_count
                self.gesture_changed = True
            # ê·¸ ì™¸ì˜ ê²½ìš°: UI ê°’ ë³€ê²½ ì—†ìŒ (ì•ˆì •ì  í‘œì‹œ)
        # ìŠ¤í‚µëœ í”„ë ˆì„ì—ì„œëŠ” ì´ì „ ì œìŠ¤ì²˜ ê²°ê³¼ ìœ ì§€
        
        annotated = frame.copy()
        
        if latest_detections:
            for i, person in enumerate(latest_detections):
                # AI ëª¨ë¸ ê²°ê³¼ë¥¼ ì›ë³¸ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
                x1, y1, x2, y2 = map(int, person['bbox'])
                # 640x480 â†’ ì›ë³¸ í•´ìƒë„ ìŠ¤ì¼€ì¼ë§
                scale_x = frame.shape[1] / 640
                scale_y = frame.shape[0] / 480
                x1 = int(x1 * scale_x)
                y1 = int(y1 * scale_y)
                x2 = int(x2 * scale_x)
                y2 = int(y2 * scale_y)
                
                person_id = person['id']
                color = self.color_palette[int(person_id.split('_')[-1]) % len(self.color_palette)]
                cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                cv2.putText(annotated, f"ID:{person_id}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        # í˜„ì¬ í”„ë ˆì„ì˜ ê°€ì¥ í° ì‚¬ëŒ ID ê³„ì‚° (tracking/updateìš©)
        largest_person_id = None
        if latest_detections:
            largest_area = -1
            for det in latest_detections:
                x1, y1, x2, y2 = det['bbox']
                area = (x2 - x1) * (y2 - y1)
                if area > largest_area:
                    largest_area = area
                    largest_person_id = det['id']
        
        if current_time - self.last_tracking_update_time > 1.0:
            largest_person_visible = bool(largest_person_id is not None)
            self._post_async(f"{AI_SERVER_BASE}/tracking/update", {"person_visible": largest_person_visible, "person_id": largest_person_id}, timeout=0.2)
            self.last_tracking_update_time = current_time

        # í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (GestureRecognizerê°€ ì‹¤í–‰ëœ í”„ë ˆì„ì—ì„œë§Œ)
        if self.frame_count % self.gesture_recognizer_skip == 0 and keypoints_detected and current_keypoints is not None:
            # í‚¤í¬ì¸íŠ¸ë„ ì›ë³¸ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
            scaled_keypoints = current_keypoints.copy()
            scale_x = frame.shape[1] / 640
            scale_y = frame.shape[0] / 480
            scaled_keypoints[:, 0] *= scale_x
            scaled_keypoints[:, 1] *= scale_y
            annotated = self.gesture_recognizer.draw_visualization(annotated, scaled_keypoints, self.current_gesture, self.current_confidence)

            # COME ì¸ì‹ ì‹œ ê°ë„ ê³„ì‚° ë° ì „ì†¡
            if self.current_gesture == "COME" and self.current_confidence >= 0.8:
                # come ì œìŠ¤ì²˜ê°€ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if self.come_gesture_active:
                    # return_commandê°€ ì˜¬ ë•Œê¹Œì§€ ì–´ë–¤ ì‚¬ëŒì´ë“  come ì œìŠ¤ì²˜ë¥¼ ì¤‘ì•™ì— ë³´ë‚´ì§€ ì•Šë„ë¡ ìˆ˜ì •í•˜ê³ , continue ì˜¤ë¥˜ë„ ìˆ˜ì •í•©ë‹ˆë‹¤.
                    print(f"[{self.name}] come ì œìŠ¤ì²˜ ì´ë¯¸ í™œì„±í™”ë¨, return_command ëŒ€ê¸° ì¤‘")
                    return annotated
                
                # ê°€ì¥ í° ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê°€ì§„ ì‚¬ëŒì˜ ID ì‚¬ìš©
                person_id = None
                bbox = None
                if latest_detections:
                    # ê°€ì¥ í° ë°”ìš´ë”© ë°•ìŠ¤ ì°¾ê¸°
                    largest_area = 0
                    for det in latest_detections:
                        x1, y1, x2, y2 = det['bbox']
                        area = (x2 - x1) * (y2 - y1)
                        if area > largest_area:
                            largest_area = area
                            person_id = det['id']
                            bbox = det['bbox']
                
                if bbox:
                    # 5ì´ˆ ì¿¨ë‹¤ìš´
                    if current_time - self.last_come_sent_ts > 5.0 or self.last_come_person_id != person_id:
                        left_angle, right_angle = self._calculate_person_angles(bbox, ai_frame.shape[1])
                        payload = {
                            "robot_id": 3,
                            "person_id": person_id,
                            "left_angle": f"{left_angle:.1f}",
                            "right_angle": f"{right_angle:.1f}",
                            "timestamp": int(current_time)
                        }
                        # ë¹„ë™ê¸° ì „ì†¡
                        self._post_async(f"{AI_SERVER_BASE}/gesture/come_local", payload, timeout=0.2)
                        self.last_come_sent_ts = current_time
                        self.last_come_person_id = person_id
                        
                        # come ì œìŠ¤ì²˜ í™œì„±í™” ìƒíƒœ ì„¤ì •
                        self.come_gesture_active = True
                        self.last_come_gesture_time = current_time

        # ê°„ë‹¨í•œ ì •ë³´ í‘œì‹œ: ë”œë ˆì´, ì œìŠ¤ì²˜, confidence
        cv2.putText(annotated, f"Delay: {self.current_delay*1000:.0f}ms", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # ì œìŠ¤ì²˜ ìƒ‰ìƒ: COMEì€ ë¹¨ê°„ìƒ‰, NORMALì€ ì´ˆë¡ìƒ‰
        gesture_color = (0, 0, 255) if self.current_gesture == "COME" else (0, 255, 0)
        cv2.putText(annotated, f"Gesture: {self.current_gesture}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, gesture_color, 2)
        cv2.putText(annotated, f"Conf: {self.current_confidence:.2f}", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, gesture_color, 2)

        # ë””ë²„ê·¸: 30í”„ë ˆì„ ì˜ˆì¸¡ ìƒíƒœ í‘œì‹œ
        frames_since_update = self.frame_count - self.last_gesture_update_frame
        cv2.putText(annotated, f"Last Update: {frames_since_update}f ago", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        return annotated

    def _calculate_person_angles(self, bbox, frame_width):
        """Bboxë¥¼ ì‚¬ìš©í•´ ì¹´ë©”ë¼ ì¤‘ì‹¬ ê¸°ì¤€ ì‚¬ëŒì˜ ì¢Œìš° ê°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if bbox is None:
            return 0.0, 0.0
        camera_center_x = frame_width / 2.0
        degrees_per_pixel = CAMERA_HFOV_DEGREES / frame_width
        x1, _, x2, _ = bbox
        left_angle = (x1 - camera_center_x) * degrees_per_pixel
        right_angle = (x2 - camera_center_x) * degrees_per_pixel
        return left_angle, right_angle


class DualCameraSystemShared:
    def __init__(self):
        # ê³µìœ  ë©”ëª¨ë¦¬ ë¦¬ë” ì´ˆê¸°í™”
        self.shared_memory_reader = DualCameraSharedMemoryReader()
        
        # ê³µìœ  ì¶”ì ê¸° ìƒì„±
        self.shared_tracker = SharedPersonTracker()
        
        # ê³µìœ  ì¶”ì ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ì„œë“¤
        self.front_processor = SingleCameraProcessor("Front", self.shared_tracker)
        self.back_processor = SingleCameraProcessor("Back", self.shared_tracker)
        self.latest_angles = {'front': 180.0, 'back': 180.0}
        self.last_come_sent_ts = 0.0
        self.last_come_person_id = None
        
        # Flask ì„œë²„ ì´ˆê¸°í™” (return_command ìˆ˜ì‹ ìš©)
        self.app = Flask(__name__)
        self.setup_flask_routes()
        
        # Flask ì„œë²„ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        self.flask_thread = threading.Thread(target=self.run_flask_server, daemon=True)
        self.flask_thread.start()
        
    def setup_flask_routes(self):
        """Flask ë¼ìš°íŠ¸ ì„¤ì •"""
        @self.app.route("/gesture/return_command", methods=["POST"])
        def gesture_return_command():
            """return_command ìˆ˜ì‹  ì‹œ come ì œìŠ¤ì²˜ ìƒíƒœ ë¦¬ì…‹"""
            try:
                data = request.get_json(force=True)
            except Exception:
                return jsonify({"status_code": 400, "error": "invalid_json"}), 400

            robot_id = data.get("robot_id")
            if robot_id is None:
                return jsonify({"status_code": 400, "error": "robot_id_required"}), 400
            self.front_processor.come_gesture_active = False
            self.front_processor.last_come_person_id = None
            self.back_processor.come_gesture_active = False
            self.back_processor.last_come_person_id = None
            return jsonify({"status_code": 200})
            
        @self.app.route("/health", methods=["GET"])
        def health():
            """ìƒíƒœ í™•ì¸"""
            return jsonify({
                "status": "ok",
                "front_come_active": self.front_processor.come_gesture_active,
                "back_come_active": self.back_processor.come_gesture_active,
                "front_last_person": self.front_processor.last_come_person_id,
                "back_last_person": self.back_processor.last_come_person_id,
                "server_type": "dual_camera_system_shared"
            })
    
    def run_flask_server(self):
        """Flask ì„œë²„ ì‹¤í–‰"""
        try:
            self.app.run(host="0.0.0.0", port=5008, debug=False)
        except Exception as e:
            print(f"Flask ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    def run_system(self):
        if not self.shared_memory_reader.is_available():
            print("ğŸ›‘ ê³µìœ  ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ AI ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return

        window_front = "Front Camera (Shared Memory)"
        window_back = "Back Camera (Shared Memory)"
        cv2.namedWindow(window_front, cv2.WINDOW_NORMAL)
        cv2.namedWindow(window_back, cv2.WINDOW_NORMAL)
        cv2.moveWindow(window_front, 50, 50)
        cv2.moveWindow(window_back, 700, 50)
        
        # ì°½ í¬ê¸° ì„¤ì •
        cv2.resizeWindow(window_front, 960, 540)
        cv2.resizeWindow(window_back, 960, 540)
        while True:
            # ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ í”„ë ˆì„ ì½ê¸° (ìµœì í™”)
            try:
                front_frame, back_frame = self.shared_memory_reader.read_frames()
            except Exception as e:
                print(f"ê³µìœ ë©”ëª¨ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}")
                time.sleep(0.01)
                continue

            if front_frame is not None:
                annotated_front = self.front_processor.process_frame(front_frame)
                if annotated_front is not None:
                    cv2.imshow(window_front, annotated_front)
            
            if back_frame is not None:
                annotated_back = self.back_processor.process_frame(back_frame)
                if annotated_back is not None:
                    cv2.imshow(window_back, annotated_back)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            time.sleep(0.01)
        self.stop()

    def stop(self):
        self.shared_memory_reader.close()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    dual_system = DualCameraSystemShared()
    dual_system.run_system() 
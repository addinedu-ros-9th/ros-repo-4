"""
ë“€ì–¼ ì¹´ë©”ë¼ì— í†µí•© ì‹œìŠ¤í…œ(Person Tracking + Gesture Recognition)ì„ ì ìš©í•©ë‹ˆë‹¤.

- WebcamStream: ê° ì¹´ë©”ë¼ì˜ í”„ë ˆì„ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì½ì–´ì˜¤ëŠ” ìŠ¤ë ˆë“œ (I/O ë¸”ë¡œí‚¹ ë°©ì§€)
- SingleCameraProcessor: ë‹¨ì¼ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì— ëŒ€í•œ ëª¨ë“  ì²˜ë¦¬ ë¡œì§(ì¶”ì , ì¸ì‹, ì‹œê°í™”)ì„ ìº¡ìŠí™”
- DualCameraSystem: ë‘ ê°œì˜ WebcamStreamê³¼ ë‘ ê°œì˜ SingleCameraProcessorë¥¼ ê´€ë¦¬í•˜ì—¬ ì „ì²´ ì‹œìŠ¤í…œì„ ìš´ì˜
- SharedPersonTracker: ì „ë©´/í›„ë©´ ì¹´ë©”ë¼ ê°„ ì‚¬ëŒ ë§¤ì¹­ì„ ìœ„í•œ ê³µìœ  ì¶”ì ê¸°
"""
import cv2
import time
import os
import threading
import numpy as np
from collections import deque
import sys

# SlidingShiftGCN ëª¨ë¸ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/ckim/ros-repo-4/deeplearning/gesture_recognition/src')

# ëª¨ë“ˆ import
from person_tracker import PersonTracker
from gesture_recognizer import GestureRecognizer

# GUI/GStreamer ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['OPENCV_VIDEOIO_PRIORITY_GSTREAMER'] = '0'


class SharedPersonTracker:
    """ì „ë©´/í›„ë©´ ì¹´ë©”ë¼ ê°„ ì‚¬ëŒ ë§¤ì¹­ì„ ìœ„í•œ ê³µìœ  ì¶”ì ê¸° (ë„í”Œê°±ì–´ ë°©ì§€)"""
    def __init__(self):
        print("ğŸ”„ ê³µìœ  ì‚¬ëŒ ì¶”ì ê¸° ì´ˆê¸°í™”")
        
        # í†µí•©ëœ ì‚¬ëŒ ë°ì´í„° (ë„í”Œê°±ì–´ ë°©ì§€)
        self.unified_people = {}
        self.next_unified_id = 0
        
        # ìµœëŒ€ ê¸°ì–µí•  ì‚¬ëŒ ìˆ˜ ì œí•œ
        self.max_people = 10
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ID í’€ (0-9)
        self.available_ids = set(range(10))
        
        # ì¹´ë©”ë¼ë³„ ìµœì‹  ê°ì§€ ê²°ê³¼ (ì›ë³¸)
        self.camera_raw_detections = {
            'front': [],
            'back': []
        }
        
        # ë§¤ì¹­ ì„¤ì •
        self.cross_camera_match_threshold = 0.4
        self.match_timeout = 8.0
        
        self.lock = threading.Lock()
        print("âœ… ê³µìœ  ì‚¬ëŒ ì¶”ì ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        
    def add_frame(self, frame, frame_id, elapsed_time, camera_name):
        """íŠ¹ì • ì¹´ë©”ë¼ì˜ í”„ë ˆì„ ì¶”ê°€ (ì›ë³¸ ê°ì§€ ê²°ê³¼ë§Œ ì €ì¥)"""
        # ì›ë³¸ PersonTrackerë¡œ ê°ì§€
        if camera_name == 'front':
            if not hasattr(self, 'front_tracker'):
                self.front_tracker = PersonTracker()
            self.front_tracker.add_frame(frame, frame_id, elapsed_time)
            self.camera_raw_detections['front'] = self.front_tracker.get_latest_detections()
        else:
            if not hasattr(self, 'back_tracker'):
                self.back_tracker = PersonTracker()
            self.back_tracker.add_frame(frame, frame_id, elapsed_time)
            self.camera_raw_detections['back'] = self.back_tracker.get_latest_detections()
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì˜¤ë˜ëœ ë§¤í•‘ ì •ë¦¬
        if frame_id % 60 == 0:
            self.cleanup_old_mappings(elapsed_time)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ PersonTracker ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if frame_id % 300 == 0:
            print(f"ğŸ”„ {camera_name} ì¹´ë©”ë¼ PersonTracker ì´ˆê¸°í™”")
            if camera_name == 'front':
                self.front_tracker = PersonTracker()
            else:
                self.back_tracker = PersonTracker()
        
    def get_latest_detections(self, camera_name, elapsed_time):
        """íŠ¹ì • ì¹´ë©”ë¼ì˜ ìµœì‹  ê°ì§€ ê²°ê³¼ ë°˜í™˜ (ë„í”Œê°±ì–´ ë°©ì§€)"""
        with self.lock:
            # 1ë‹¨ê³„: í˜„ì¬ ì¹´ë©”ë¼ì˜ ì›ë³¸ ê°ì§€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            raw_detections = self.camera_raw_detections.get(camera_name, [])
            
            # 2ë‹¨ê³„: ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„
            unified_detections = []
            used_unified_ids = set()  # í˜„ì¬ ì¹´ë©”ë¼ì—ì„œ ì‚¬ìš©ëœ í†µí•© ID ì¶”ì 
            
            for detection in raw_detections:
                person_id = detection['id']
                bbox = detection['bbox']
                
                # ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„
                matched_unified_id = self._find_existing_match(detection, camera_name, elapsed_time)
                
                if matched_unified_id:
                    # ê¸°ì¡´ í†µí•© ID ì‚¬ìš©
                    unified_detection = detection.copy()
                    unified_detection['id'] = matched_unified_id
                    unified_detections.append(unified_detection)
                    used_unified_ids.add(matched_unified_id)
                    
                    # ë§¤ì¹­ ì •ë³´ ì—…ë°ì´íŠ¸
                    self.unified_people[matched_unified_id]['last_seen'][camera_name] = elapsed_time
                    self.unified_people[matched_unified_id]['bbox'][camera_name] = bbox
                else:
                    # ìƒˆë¡œìš´ í†µí•© ID í• ë‹¹ (ë„í”Œê°±ì–´ ë°©ì§€)
                    new_unified_id = self._assign_new_unified_id(detection, camera_name, elapsed_time)
                    unified_detection = detection.copy()
                    unified_detection['id'] = new_unified_id
                    unified_detections.append(unified_detection)
                    used_unified_ids.add(new_unified_id)
            
            # 3ë‹¨ê³„: ë‹¤ë¥¸ ì¹´ë©”ë¼ì—ì„œ í˜„ì¬ ì¹´ë©”ë¼ë¡œ ì´ë™í•œ ì‚¬ëŒ í™•ì¸
            other_camera = 'back' if camera_name == 'front' else 'front'
            other_detections = self.camera_raw_detections.get(other_camera, [])
            
            for other_detection in other_detections:
                other_person_id = other_detection['id']
                other_bbox = other_detection['bbox']
                
                # ì´ë¯¸ í˜„ì¬ ì¹´ë©”ë¼ì—ì„œ ì‚¬ìš©ëœ í†µí•© IDëŠ” ì œì™¸
                for unified_id, data in self.unified_people.items():
                    if (unified_id not in used_unified_ids and 
                        other_camera in data['camera_ids'] and 
                        data['camera_ids'][other_camera] == other_person_id):
                        
                        # ì‹œê°„ ì°¨ì´ í™•ì¸
                        other_last_seen = data['last_seen'].get(other_camera, 0)
                        time_diff = elapsed_time - other_last_seen
                        
                        if time_diff <= self.match_timeout:
                            # í˜„ì¬ ì¹´ë©”ë¼ì˜ ì›ë³¸ ê°ì§€ ê²°ê³¼ì—ì„œ ë§¤ì¹­í•  ëŒ€ìƒ ì°¾ê¸°
                            for current_detection in raw_detections:
                                current_bbox = current_detection['bbox']
                                # ê³µê°„ì  ë§¤ì¹­ ì‹œë„
                                score = self._calculate_cross_camera_similarity(other_bbox, current_bbox, other_camera, camera_name)
                                if score > self.cross_camera_match_threshold:
                                    # ë§¤ì¹­ ì„±ê³µ - ê¸°ì¡´ í†µí•© ID ì‚¬ìš©
                                    unified_detection = current_detection.copy()
                                    unified_detection['id'] = unified_id
                                    unified_detections.append(unified_detection)
                                    used_unified_ids.add(unified_id)
                                    
                                    # ë§¤ì¹­ ì •ë³´ ì—…ë°ì´íŠ¸
                                    data['camera_ids'][camera_name] = current_detection['id']
                                    data['last_seen'][camera_name] = elapsed_time
                                    data['bbox'][camera_name] = current_detection['bbox']
                                    
                                    print(f"âœ… ì¹´ë©”ë¼ ê°„ ì´ë™ ê°ì§€: {other_camera} â†’ {camera_name} ({unified_id})")
                                    break
            
            return unified_detections
    
    def _find_existing_match(self, detection, camera_name, elapsed_time):
        """ê¸°ì¡´ í†µí•© IDì™€ ë§¤ì¹­ ì‹œë„"""
        person_id = detection['id']
        bbox = detection['bbox']
        
        for unified_id, data in self.unified_people.items():
            if camera_name in data['camera_ids'] and data['camera_ids'][camera_name] == person_id:
                return unified_id
        
        return None
    
    def _assign_new_unified_id(self, detection, camera_name, elapsed_time):
        """ìƒˆë¡œìš´ í†µí•© ID í• ë‹¹ (ë„í”Œê°±ì–´ ë°©ì§€)"""
        # ìµœëŒ€ ì¸ì› ì œí•œ í™•ì¸
        if len(self.unified_people) >= self.max_people:
            # ê°€ì¥ ì˜¤ë˜ëœ ì‚¬ëŒ ì œê±°
            oldest_id = min(self.unified_people.keys(), 
                          key=lambda x: self.unified_people[x]['created_time'])
            removed_person = self.unified_people.pop(oldest_id)
            removed_id_num = int(oldest_id.split('_')[1])
            self.available_ids.add(removed_id_num)
            print(f"ğŸ—‘ï¸ ìµœëŒ€ ì¸ì› ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ ì‚¬ëŒ ì œê±°: {oldest_id}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ID ì¤‘ ê°€ì¥ ì‘ì€ ë²ˆí˜¸ ì„ íƒ
        if self.available_ids:
            next_id = min(self.available_ids)
            self.available_ids.remove(next_id)
        else:
            next_id = 0
            print(f"âš ï¸ ëª¨ë“  IDê°€ ì‚¬ìš© ì¤‘, 0ë¶€í„° ì¬ì‹œì‘")
        
        # ìƒˆë¡œìš´ í†µí•© ID ìƒì„±
        unified_id = f"Person_{next_id}"
        
        self.unified_people[unified_id] = {
            'camera_ids': {camera_name: detection['id']},
            'last_seen': {camera_name: elapsed_time},
            'bbox': {camera_name: detection['bbox']},
            'created_time': elapsed_time
        }
        
        print(f"ğŸ†• ìƒˆë¡œìš´ í†µí•© ID ìƒì„±: {detection['id']} â†’ {unified_id}")
        return unified_id
    
    def _calculate_cross_camera_similarity(self, bbox1, bbox2, camera1, camera2):
        """ë‘ ì¹´ë©”ë¼ ê°„ ë°”ìš´ë”© ë°•ìŠ¤ ìœ ì‚¬ë„ ê³„ì‚° (ì ë‹¹í•œ ê¸°ì¤€)"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        
        # ë©´ì  ë¹„ìœ¨ ìœ ì‚¬ë„ (ì ë‹¹í•˜ê²Œ)
        area_ratio = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 0
        
        # ë©´ì  ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ë§¤ì¹­ ê±°ë¶€ (ë” ê´€ëŒ€í•˜ê²Œ)
        if area_ratio < 0.1:  # ë©´ì  ì°¨ì´ê°€ 90% ì´ìƒì´ë©´ ë§¤ì¹­ ê±°ë¶€ (0.3 â†’ 0.1ë¡œ ë” ê´€ëŒ€í•˜ê²Œ)
            return 0.0
        
        # ìœ„ì¹˜ ìœ ì‚¬ë„ (ì¹´ë©”ë¼ë³„ íŠ¹ì„± ê³ ë ¤)
        if camera1 == 'front' and camera2 == 'back':
            # ì „ë©´-í›„ë©´: ì „ë©´ì—ì„œëŠ” ìƒë‹¨, í›„ë©´ì—ì„œëŠ” í•˜ë‹¨ì— ìˆì„ ê°€ëŠ¥ì„±
            front_center_y = (y1_1 + y2_1) / 2
            back_center_y = (y1_2 + y2_2) / 2
            height_ratio = 480  # í”„ë ˆì„ ë†’ì´
            
            # ì „ë©´ ìƒë‹¨ â†” í›„ë©´ í•˜ë‹¨ ë§¤ì¹­
            front_normalized = front_center_y / height_ratio
            back_normalized = (height_ratio - back_center_y) / height_ratio
            position_similarity = 1.0 - abs(front_normalized - back_normalized)
            
            # ìœ„ì¹˜ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ë§¤ì¹­ ê±°ë¶€ (ë” ê´€ëŒ€í•˜ê²Œ)
            if position_similarity < 0.2:  # ìœ„ì¹˜ ì°¨ì´ê°€ 80% ì´ìƒì´ë©´ ë§¤ì¹­ ê±°ë¶€ (0.4 â†’ 0.2ë¡œ ë” ê´€ëŒ€í•˜ê²Œ)
                return 0.0
        else:
            position_similarity = 0.5  # ê¸°ë³¸ê°’
        
        # ì¢…í•© ì ìˆ˜ (ë©´ì  ë¹„ìœ¨ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜, ì ë‹¹í•˜ê²Œ)
        total_score = (area_ratio * 0.6 + position_similarity * 0.4)
        
        # ë””ë²„ê¹… ì •ë³´ (15í”„ë ˆì„ë§ˆë‹¤ - ë” ìì£¼ ì¶œë ¥)
        if hasattr(self, 'debug_counter') and self.debug_counter % 15 == 0:
            print(f"ğŸ” ì¹´ë©”ë¼ ê°„ ë§¤ì¹­ ì‹œë„: {camera1} â†” {camera2}")
            print(f"   - ë©´ì  ë¹„ìœ¨: {area_ratio:.3f}")
            print(f"   - ìœ„ì¹˜ ìœ ì‚¬ë„: {position_similarity:.3f}")
            print(f"   - ì¢…í•© ì ìˆ˜: {total_score:.3f}")
            print(f"   - ì„ê³„ê°’: {self.cross_camera_match_threshold}")
            if total_score < self.cross_camera_match_threshold:
                print(f"   âŒ ë§¤ì¹­ ê±°ë¶€: ì ìˆ˜ ë¶€ì¡±")
            else:
                print(f"   âœ… ë§¤ì¹­ ê°€ëŠ¥: ì ìˆ˜ ì¶©ë¶„")
        
        return total_score
    
    def cleanup_old_mappings(self, elapsed_time):
        """ì˜¤ë˜ëœ ë§¤í•‘ ì •ë¦¬ (ìµœëŒ€ ì¸ì› ì œí•œ ê³ ë ¤)"""
        with self.lock:
            people_to_remove = []
            for unified_id, data in self.unified_people.items():
                # ëª¨ë“  ì¹´ë©”ë¼ì—ì„œ ì¼ì • ì‹œê°„ ì´ìƒ ê°ì§€ë˜ì§€ ì•Šì€ ì‚¬ëŒ ì œê±°
                all_old = True
                for camera_name, last_seen in data['last_seen'].items():
                    if elapsed_time - last_seen < self.match_timeout:
                        all_old = False
                        break
                
                if all_old:
                    people_to_remove.append(unified_id)
            
            # ì˜¤ë˜ëœ ì‚¬ëŒë“¤ ì œê±°
            for unified_id in people_to_remove:
                removed_person = self.unified_people.pop(unified_id)
                # ì œê±°ëœ IDë¥¼ ì‚¬ìš© ê°€ëŠ¥í•œ ID í’€ì— ì¶”ê°€
                removed_id_num = int(unified_id.split('_')[1])
                self.available_ids.add(removed_id_num)
                print(f"ğŸ—‘ï¸ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì˜¤ë˜ëœ ì‚¬ëŒ ì œê±°: {unified_id}")
                print(f"   - ì œê±°ëœ ì‚¬ëŒ ì •ë³´: {removed_person['camera_ids']}")
            
            # í˜„ì¬ ì¸ì› ìˆ˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            if len(self.unified_people) > 0:
                print(f"ğŸ“Š í˜„ì¬ ì¶”ì  ì¤‘ì¸ ì‚¬ëŒ: {len(self.unified_people)}/{self.max_people}ëª…")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ID í’€: {sorted(self.available_ids)}")


class WebcamStream:
    """íŠ¹ì • ì¹´ë©”ë¼ë¥¼ ìœ„í•œ ë¹„ë™ê¸° í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ"""
    def __init__(self, src, name):
        print(f"ğŸ”„ {name} ({src}) ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì‹œë„...")
        self.stream = cv2.VideoCapture(src, cv2.CAP_V4L2)
        if not self.stream.isOpened():
            raise IOError(f"ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {name} at {src}")

        self.name = name
        fps = 15 if "Back" in self.name else 30
        self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        self.stream.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        self.stream.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        self.stream.set(cv2.CAP_PROP_FPS, fps)
        print(f"   - {name} FPS ì„¤ì •: {fps}")

        (self.ret, self.frame) = self.stream.read()
        if not self.ret:
            raise IOError(f"ì´ˆê¸° í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {name}")
        
        print(f"âœ… {name} ({src}) ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì„±ê³µ!")

        self.stopped = False
        self.lock = threading.Lock()
        self.thread = threading.Thread(target=self.update, args=())
        self.thread.daemon = True

    def start(self):
        self.stopped = False
        self.thread.start()
        return self

    def update(self):
        while not self.stopped:
            (ret, frame) = self.stream.read()
            with self.lock:
                self.ret = ret
                self.frame = frame
        self.stream.release()

    def read(self):
        with self.lock:
            if self.frame is None:
                return False, None
            return self.ret, self.frame.copy()

    def stop(self):
        self.stopped = True
        if self.thread.is_alive():
            self.thread.join(timeout=1)

class SingleCameraProcessor:
    """ë‹¨ì¼ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ê¸°"""
    def __init__(self, name, shared_tracker):
        print(f"ğŸš€ {name} Processor ì´ˆê¸°í™”")
        self.name = name
        self.shared_tracker = shared_tracker  # ê³µìœ  ì¶”ì ê¸° ì‚¬ìš©
        self.gesture_recognizer = GestureRecognizer()
        
        self.frame_count = 0
        self.start_time = time.time()
        
        # í”„ë ˆì„ ê°„ ì§€ì—°ì‹œê°„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜
        self.last_frame_time = None
        self.current_delay = 0.0
        
        self.current_gesture = "NORMAL"
        self.current_confidence = 0.5
        
        self.color_palette = [
            (0, 255, 0), (255, 0, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128), (255, 165, 0)
        ]
        print(f"âœ… {name} Processor ì´ˆê¸°í™” ì™„ë£Œ")

    def process_frame(self, frame):
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        current_time = time.time()
        
        # í”„ë ˆì„ ê°„ ì§€ì—°ì‹œê°„ ê³„ì‚°
        if self.last_frame_time is not None:
            self.current_delay = current_time - self.last_frame_time
        self.last_frame_time = current_time

        # AI ëª¨ë¸ìš©ìœ¼ë¡œ í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ (640x480)
        ai_frame = cv2.resize(frame, (640, 480))

        # ê³µìœ  ì¶”ì ê¸° ì‚¬ìš©
        camera_name = 'front' if 'Front' in self.name else 'back'
        self.shared_tracker.add_frame(ai_frame.copy(), self.frame_count, elapsed_time, camera_name)
        latest_detections = self.shared_tracker.get_latest_detections(camera_name, elapsed_time)

        self.gesture_recognizer.add_frame(ai_frame.copy(), self.frame_count, elapsed_time, latest_detections)
        gesture_prediction, gesture_confidence, keypoints_detected, current_keypoints = self.gesture_recognizer.get_latest_gesture()

        # ì œìŠ¤ì²˜ì™€ confidence ì—…ë°ì´íŠ¸
        if keypoints_detected and current_keypoints is not None:
            # gesture_predictionì´ ì´ë¯¸ "COME" ë˜ëŠ” "NORMAL" ë¬¸ìì—´
            self.current_gesture = gesture_prediction
            self.current_confidence = gesture_confidence
        else:
            self.current_gesture = "NORMAL"
            self.current_confidence = 0.0  # keypointsê°€ ì—†ìœ¼ë©´ confidence 0
        
        annotated = frame.copy()  # ì›ë³¸ í•´ìƒë„ë¡œ í‘œì‹œ
        
        if latest_detections:
            for i, person in enumerate(latest_detections):
                # AI ëª¨ë¸ ê²°ê³¼ë¥¼ ì›ë³¸ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
                x1, y1, x2, y2 = map(int, person['bbox'])
                # 640x480 â†’ 1280x720 ìŠ¤ì¼€ì¼ë§
                scale_x = frame.shape[1] / 640
                scale_y = frame.shape[0] / 480
                x1 = int(x1 * scale_x)
                y1 = int(y1 * scale_y)
                x2 = int(x2 * scale_x)
                y2 = int(y2 * scale_y)
                
                person_id = person['id']
                color = self.color_palette[int(person_id.split('_')[-1]) % len(self.color_palette)]
                cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                cv2.putText(annotated, f"ID:{person_id}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

        if keypoints_detected and current_keypoints is not None:
            # í‚¤í¬ì¸íŠ¸ë„ ì›ë³¸ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
            scaled_keypoints = current_keypoints.copy()
            scale_x = frame.shape[1] / 640
            scale_y = frame.shape[0] / 480
            scaled_keypoints[:, 0] *= scale_x
            scaled_keypoints[:, 1] *= scale_y
            annotated = self.gesture_recognizer.draw_visualization(annotated, scaled_keypoints, self.current_gesture, self.current_confidence)

        # ê°„ë‹¨í•œ ì •ë³´ í‘œì‹œ: ë”œë ˆì´, ì œìŠ¤ì²˜, confidence
        cv2.putText(annotated, f"Delay: {self.current_delay*1000:.0f}ms", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # ì œìŠ¤ì²˜ ìƒ‰ìƒ: COMEì€ ë¹¨ê°„ìƒ‰, NORMALì€ ì´ˆë¡ìƒ‰
        gesture_color = (0, 0, 255) if self.current_gesture == "COME" else (0, 255, 0)
        cv2.putText(annotated, f"Gesture: {self.current_gesture}", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, gesture_color, 2)
        cv2.putText(annotated, f"Conf: {self.current_confidence:.2f}", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, gesture_color, 2)

        return annotated


class DualCameraSystem:
    def __init__(self):
        print("ğŸš€ ì´ì¤‘ ì¹´ë©”ë¼ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        self.front_stream = None
        self.back_stream = None
        
        # ê³µìœ  ì¶”ì ê¸° ìƒì„±
        self.shared_tracker = SharedPersonTracker()
        
        # ê³µìœ  ì¶”ì ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ì„œë“¤
        self.front_processor = SingleCameraProcessor("Front", self.shared_tracker)
        self.back_processor = SingleCameraProcessor("Back", self.shared_tracker)

    def run_system(self):
        try:
            self.front_stream = WebcamStream(src="/dev/video0", name="Front Camera").start()
            time.sleep(1.0)
            self.back_stream = WebcamStream(src="/dev/video2", name="Back Camera").start()
        except IOError as e:
            print(f"ğŸ›‘ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.stop()
            return

        window_front = "Front Camera"
        window_back = "Back Camera"
        cv2.namedWindow(window_front, cv2.WINDOW_NORMAL)
        cv2.namedWindow(window_back, cv2.WINDOW_NORMAL)
        cv2.moveWindow(window_front, 50, 50)
        cv2.moveWindow(window_back, 700, 50)
        
        # ì°½ í¬ê¸° ì„¤ì • (1280x720 í•´ìƒë„ì— ë§ì¶° ë” í¬ê²Œ)
        cv2.resizeWindow(window_front, 960, 540)  # 16:9 ë¹„ìœ¨ë¡œ í¬ê²Œ
        cv2.resizeWindow(window_back, 960, 540)   # 16:9 ë¹„ìœ¨ë¡œ í¬ê²Œ

        print("\nğŸš€ ì–‘ìª½ í†µí•© ì‹œìŠ¤í…œ ê°€ë™ ì‹œì‘. 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
        print("ğŸ“ ì´ì œ ì „ë©´/í›„ë©´ ì¹´ë©”ë¼ì—ì„œ ê°™ì€ ì‚¬ëŒì´ ê°™ì€ IDë¡œ ì¸ì‹ë©ë‹ˆë‹¤!")
        
        while True:
            if self.front_stream.stopped or self.back_stream.stopped:
                break

            ret_front, frame_front = self.front_stream.read()
            ret_back, frame_back = self.back_stream.read()

            if ret_front:
                annotated_front = self.front_processor.process_frame(frame_front)
                cv2.imshow(window_front, annotated_front)
            
            if ret_back:
                annotated_back = self.back_processor.process_frame(frame_back)
                cv2.imshow(window_back, annotated_back)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            # ë©”ì¸ ë£¨í”„ì— íœ´ì‹ì„ ì£¼ì–´ CPU ê³¼ë¶€í•˜ ë°©ì§€ (ì¹´ë©”ë¼ FPSì— ë§ì¶° ì¡°ì •)
            time.sleep(0.01)  # 10ms ëŒ€ê¸° (ì•½ 100 FPS ì œí•œ)
        
        self.stop()

    def stop(self):
        print("ğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì‹œì‘...")
        if self.front_stream: self.front_stream.stop()
        if self.back_stream: self.back_stream.stop()
        cv2.destroyAllWindows()
        print("âœ… ì‹œìŠ¤í…œ ì •ë¦¬ ì™„ë£Œ.")


if __name__ == "__main__":
    dual_system = DualCameraSystem()
    dual_system.run_system() 
"""
í†µí•© ì‹œìŠ¤í…œ ë©”ì¸ ëª¨ë“ˆ
- Person Trackerì™€ Gesture Recognizerë¥¼ í†µí•©
- ì“°ë ˆë“œ ê´€ë¦¬ ë° UI í‘œì‹œ
- ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì¹´ë©”ë¼ ì¥ì¹˜ ì„¤ì • ê°€ëŠ¥
"""

import cv2
import numpy as np
from collections import deque
import time
from datetime import datetime
import os
import argparse

# ëª¨ë“ˆ import
from person_tracker import PersonTracker
from gesture_recognizer import GestureRecognizer

# Qt ì˜¤ë¥˜ ë°©ì§€
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'

class IntegratedSystem:
    """í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        print("ğŸš€ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        
        # ëª¨ë“ˆ ì´ˆê¸°í™”
        self.person_tracker = PersonTracker()
        self.gesture_recognizer = GestureRecognizer()
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.running = True
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.fps_times = deque(maxlen=30)
        
        print("âœ… í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_system(self, camera_device=None, system_name="Integrated System"):
        """ì‹œìŠ¤í…œ ì‹¤í–‰"""
        if camera_device is None:
            print("âŒ ì¹´ë©”ë¼ ì¥ì¹˜ê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   ì‚¬ìš©ë²•: python integrated_system.py --camera /dev/videoX --name 'ì‹œìŠ¤í…œëª…'")
            return
            
        print("ğŸš€ í†µí•© ì‹œìŠ¤í…œ ì‹œì‘!")
        print(f"ğŸ“¹ ì¹´ë©”ë¼: {camera_device}")
        print(f"ğŸ·ï¸ ì‹œìŠ¤í…œ ì´ë¦„: {system_name}")
        print("âš¡ ëª¨ë“ˆí™”ëœ ì‹œìŠ¤í…œ: Person Tracker + Gesture Recognizer")
        
        # ì¹´ë©”ë¼ ì„¤ì •
        cap = cv2.VideoCapture(camera_device, cv2.CAP_V4L2)  # V4L2 ë°±ì—”ë“œ ëª…ì‹œì  ì‚¬ìš©
        
        # ê°™ì€ í•˜ë“œì›¨ì–´ ì¹´ë©”ë¼ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ íŠ¹ë³„ ì„¤ì •
        if "video4" in camera_device:
            # video4ëŠ” video2ì™€ ê°™ì€ í•˜ë“œì›¨ì–´ì´ë¯€ë¡œ ë” ê¸´ ëŒ€ê¸° ì‹œê°„
            time.sleep(2)  # 5ì´ˆì—ì„œ 2ì´ˆë¡œ ë‹¨ì¶•
        
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {camera_device}")
            return
        
        # ì¹´ë©”ë¼ ì—°ê²° í™•ì¸
        ret, test_frame = cap.read()
        if not ret:
            print(f"âŒ ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨: {camera_device}")
            cap.release()
            return
        
        print(f"âœ… ì¹´ë©”ë¼ ì—°ê²° ë° í”„ë ˆì„ ì½ê¸° ì„±ê³µ: {camera_device}")
        
        # ì°½ ì„¤ì •
        window_name = f"ğŸš€ {system_name}"
        
        try:
            # Back ì°½ì˜ ê²½ìš° ë” ê°•ë ¥í•œ ì°½ ì„¤ì •
            if "Back" in system_name:
                # ê°„ë‹¨í•œ ì°½ ì´ë¦„ìœ¼ë¡œ ì‹œë„
                window_name = "Back Camera"
                cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
                cv2.resizeWindow(window_name, 800, 600)
                cv2.moveWindow(window_name, 900, 50)
                cv2.setWindowProperty(window_name, cv2.WND_PROP_TOPMOST, 1)
                cv2.waitKey(1)
                time.sleep(0.5)
                print(f"ğŸ“ Back ì°½ ìƒì„±: {window_name}")
            else:
                cv2.namedWindow(window_name, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)
                cv2.resizeWindow(window_name, 800, 600)
                
                # ì°½ ìœ„ì¹˜ ì„¤ì • (ì‹œìŠ¤í…œ ì´ë¦„ì— ë”°ë¼)
                if "Front" in system_name:
                    cv2.moveWindow(window_name, 50, 50)   # ì™¼ìª½ ìœ„
                    print(f"ğŸ“ Front ì°½ ìœ„ì¹˜: (50, 50)")
                else:
                    cv2.moveWindow(window_name, 50, 50)    # ê¸°ë³¸ ìœ„ì¹˜
                
                # ì°½ ìƒì„± ì•ˆì •ì„±ì„ ìœ„í•œ ëŒ€ê¸°
                cv2.waitKey(1)
                time.sleep(1.0)  # 1ì´ˆ ëŒ€ê¸°
                
                # ì°½ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                cv2.waitKey(1)
            
            print("ğŸ–¼ï¸ ì°½ ì„¤ì • ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ì°½ ì„¤ì • ì‹¤íŒ¨: {e}")
            # Back ì°½ì˜ ê²½ìš° ëŒ€ì•ˆ ì°½ ì´ë¦„ ì‹œë„
            if "Back" in system_name:
                try:
                    window_name = "Back Camera"
                    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
                    cv2.resizeWindow(window_name, 800, 600)
                    cv2.moveWindow(window_name, 500, 100)
                    cv2.waitKey(1)
                    print("ğŸ”„ ëŒ€ì•ˆ Back ì°½ ìƒì„± ì„±ê³µ!")
                except Exception as e2:
                    print(f"âŒ ëŒ€ì•ˆ ì°½ë„ ì‹¤íŒ¨: {e2}")
                    window_name = None
            else:
                print("ğŸ”„ ì°½ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤...")
                window_name = None
        
        # ì¹´ë©”ë¼ í•´ìƒë„ ì„¤ì • (ê°™ì€ í•˜ë“œì›¨ì–´ ì¹´ë©”ë¼ ì¶©ëŒ ë°©ì§€)
        if "video4" in camera_device:
            # video4ëŠ” ë” ë‚®ì€ í•´ìƒë„ë¡œ ì„¤ì •
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        else:
            # video2ëŠ” ê¸°ë³¸ í•´ìƒë„
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)
        
        # ë¹„ë””ì˜¤ í¬ë§· ì„¤ì • (ê°™ì€ í•˜ë“œì›¨ì–´ ì¹´ë©”ë¼ ì¶©ëŒ ë°©ì§€)
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        cap.set(cv2.CAP_PROP_CONVERT_RGB, True)
        
        # Back ì‹œìŠ¤í…œì˜ FPSë¥¼ ë‚®ì¶° ëŒ€ì—­í­ í™•ë³´
        if "Back" in system_name:
            cap.set(cv2.CAP_PROP_FPS, 15)
            print(f"ğŸ“‰ {system_name} FPSë¥¼ 15ë¡œ ë‚®ì¶° ëŒ€ì—­í­ì„ í™•ë³´í•©ë‹ˆë‹¤.")
        else:
            cap.set(cv2.CAP_PROP_FPS, 30)
        
        cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)
        
        # ì¹´ë©”ë¼ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸° (ê°™ì€ í•˜ë“œì›¨ì–´ ì¹´ë©”ë¼ ì¶©ëŒ ë°©ì§€)
        if "video4" in camera_device:
            time.sleep(3)  # video4ëŠ” ë” ê¸´ ëŒ€ê¸° ì‹œê°„
        else:
            time.sleep(2)  # video2ëŠ” ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„
        
        # ì‹¤ì œ ì„¤ì •ëœ í•´ìƒë„ í™•ì¸
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"ğŸ“¹ ì¹´ë©”ë¼ í•´ìƒë„: {actual_width}x{actual_height}")
        
        # ì²« í”„ë ˆì„ìœ¼ë¡œ ì°½ í…ŒìŠ¤íŠ¸
        ret, test_frame = cap.read()
        if ret:
            cv2.putText(test_frame, "ğŸš€ System Starting...", (50, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
            cv2.imshow(window_name, test_frame)
            cv2.waitKey(100)
            print("ğŸ–¼ï¸ ì´ˆê¸° ì°½ í‘œì‹œ ì™„ë£Œ!")
        else:
            print(f"âŒ ì´ˆê¸° í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨: {camera_device}")
            cap.release()
            return
        
        # ì¶”ê°€ í”„ë ˆì„ í…ŒìŠ¤íŠ¸ (ê²€ì€ìƒ‰ í™”ë©´ ë°©ì§€)
        for i in range(5):
            ret, test_frame = cap.read()
            if ret and test_frame is not None and test_frame.size > 0:
                # í”„ë ˆì„ì´ ê²€ì€ìƒ‰ì¸ì§€ í™•ì¸
                if np.mean(test_frame) > 10:  # í‰ê·  ë°ê¸°ê°€ 10 ì´ìƒì´ë©´ ì •ìƒ
                    print(f"âœ… í”„ë ˆì„ {i+1} ì •ìƒ: í‰ê·  ë°ê¸° {np.mean(test_frame):.1f}")
                    break
                else:
                    print(f"âš ï¸ í”„ë ˆì„ {i+1} ê²€ì€ìƒ‰: í‰ê·  ë°ê¸° {np.mean(test_frame):.1f}")
            else:
                print(f"âŒ í”„ë ˆì„ {i+1} ì½ê¸° ì‹¤íŒ¨")
            time.sleep(0.5)
        
        # ëª¨ë“ˆ ì‹œì‘ (ì´ë¯¸ __init__ì—ì„œ ìë™ ì‹œì‘ë¨)
        print("ğŸ”¥ ëª¨ë“  ëª¨ë“ˆ ìë™ ì‹œì‘ë¨!")
        
        # ë©”ì¸ ë£¨í”„
        frame_count = 0
        start_time = datetime.now()
        
        current_gesture = "NORMAL"
        current_confidence = 0.5
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (ì‚¬ëŒë³„ ìƒ‰ìƒ í• ë‹¹ìš©)
        color_palette = [
            (0, 255, 0),    # ì´ˆë¡ìƒ‰
            (255, 0, 0),    # íŒŒë€ìƒ‰  
            (0, 0, 255),    # ë¹¨ê°„ìƒ‰
            (255, 255, 0),  # ì²­ë¡ìƒ‰
            (255, 0, 255),  # ìí™ìƒ‰
            (0, 255, 255),  # ë…¸ë€ìƒ‰
            (128, 0, 128),  # ë³´ë¼ìƒ‰
            (255, 165, 0),  # ì£¼í™©ìƒ‰
        ]
        
        try:
            while cap.isOpened():
                frame_start = time.time()
                
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                elapsed_time = (datetime.now() - start_time).total_seconds()
                
                # í”„ë ˆì„ì„ ëª¨ë“ˆë“¤ì—ê²Œ ì „ë‹¬
                self.person_tracker.add_frame(frame.copy(), frame_count, elapsed_time)
                
                # ì‚¬ëŒ ê°ì§€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                latest_detections = self.person_tracker.get_latest_detections()
                
                # ì œìŠ¤ì²˜ ì¸ì‹ê¸°ì— í”„ë ˆì„ ì „ë‹¬ (ì‚¬ëŒ ê°ì§€ ê²°ê³¼ í¬í•¨)
                self.gesture_recognizer.add_frame(frame.copy(), frame_count, elapsed_time, latest_detections)
                
                # ì œìŠ¤ì²˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                gesture_prediction, gesture_confidence, keypoints_detected, current_keypoints = self.gesture_recognizer.get_latest_gesture()
                
                # ì œìŠ¤ì²˜ ê²°ê³¼ ì—…ë°ì´íŠ¸ (í†µí•©ëœ ë¡œì§)
                # 1. í‚¤í¬ì¸íŠ¸ ë¶€ì¡± ì‹œ NORMALë¡œ ë¦¬ì…‹
                if not keypoints_detected or current_keypoints is None:
                    if current_gesture != "NORMAL":
                        current_gesture = "NORMAL"
                        current_confidence = 0.5
                        print(f"ğŸ”„ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±ìœ¼ë¡œ NORMALë¡œ ë¦¬ì…‹")
                else:
                    # 2. í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í™•ì¸ (ìµœì†Œ 7ê°œ í•„ìš”)
                    upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]
                    gesture_keypoints = current_keypoints[upper_body_joints]
                    valid_gesture_keypoints = np.sum(gesture_keypoints[:, 2] > 0.1)
                    
                    if valid_gesture_keypoints < 7:  # ìµœì†Œ 7ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš”
                        if current_gesture != "NORMAL":
                            current_gesture = "NORMAL"
                            current_confidence = 0.5
                            print(f"ğŸ”„ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±({valid_gesture_keypoints}/7)ìœ¼ë¡œ NORMALë¡œ ë¦¬ì…‹")
                    else:
                        # 3. í‚¤í¬ì¸íŠ¸ê°€ ì¶©ë¶„í•˜ë©´ ëª¨ë¸ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë°˜ì˜
                        if gesture_prediction != current_gesture:
                            current_gesture = gesture_prediction
                            current_confidence = gesture_confidence
                            print(f"ğŸ¯ ëª¨ë¸ ê²°ê³¼ ë°˜ì˜: {gesture_prediction} ({gesture_confidence:.2f})")
                        elif abs(gesture_confidence - current_confidence) > 0.1:
                            # ì‹ ë¢°ë„ê°€ í¬ê²Œ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
                            current_gesture = gesture_prediction
                            current_confidence = gesture_confidence
                            print(f"ğŸ¯ ì‹ ë¢°ë„ ë³€í™”ë¡œ ì—…ë°ì´íŠ¸: {gesture_prediction} ({gesture_confidence:.2f})")
                
                # ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ê²°ê³¼ ë””ë²„ê¹… (ë§¤ 30í”„ë ˆì„ë§ˆë‹¤)
                if frame_count % 30 == 0:
                    print(f"ğŸ”„ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ê²°ê³¼:")
                    print(f"   - gesture_prediction: {gesture_prediction}")
                    print(f"   - gesture_confidence: {gesture_confidence:.3f}")
                    print(f"   - current_gesture: {current_gesture}")
                    print(f"   - current_confidence: {current_confidence:.3f}")
                    print(f"   - ë³€í™” ì—¬ë¶€: {gesture_prediction != current_gesture}")
                    
                    # í‚¤í¬ì¸íŠ¸ ìƒíƒœë„ í•¨ê»˜ ì¶œë ¥
                    if keypoints_detected and current_keypoints is not None:
                        upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]
                        gesture_keypoints = current_keypoints[upper_body_joints]
                        valid_gesture_keypoints = np.sum(gesture_keypoints[:, 2] > 0.1)
                        print(f"   - í‚¤í¬ì¸íŠ¸ ìƒíƒœ: {valid_gesture_keypoints}/7")
                    else:
                        print(f"   - í‚¤í¬ì¸íŠ¸ ìƒíƒœ: ì—†ìŒ")
                
                # ê°•ì œ ì—…ë°ì´íŠ¸ (ë””ë²„ê¹…ìš©) - í‚¤í¬ì¸íŠ¸ê°€ ì¶©ë¶„í•  ë•Œë§Œ
                if frame_count % 30 == 0 and keypoints_detected and current_keypoints is not None:
                    # í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ ì¬í™•ì¸
                    upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]
                    gesture_keypoints = current_keypoints[upper_body_joints]
                    valid_gesture_keypoints = np.sum(gesture_keypoints[:, 2] > 0.1)
                    
                    if valid_gesture_keypoints >= 7:  # ì¶©ë¶„í•œ í‚¤í¬ì¸íŠ¸ê°€ ìˆì„ ë•Œë§Œ
                        # ëª¨ë¸ ê²°ê³¼ì™€ í˜„ì¬ ìƒíƒœê°€ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
                        if gesture_prediction != current_gesture:
                            current_gesture = gesture_prediction
                            current_confidence = gesture_confidence
                            print(f"ğŸ”„ ê°•ì œ ì œìŠ¤ì²˜ ì—…ë°ì´íŠ¸: {gesture_prediction} ({gesture_confidence:.2f})")
                        else:
                            print(f"ğŸ”„ ëª¨ë¸ ê²°ê³¼ì™€ í˜„ì¬ ìƒíƒœ ë™ì¼: {gesture_prediction} ({gesture_confidence:.2f})")
                    else:
                        print(f"ğŸ”„ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±({valid_gesture_keypoints}/7)ìœ¼ë¡œ ê°•ì œ ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
                
                # í™”ë©´ êµ¬ì„±
                annotated = frame.copy()
                
                # ì‚¬ëŒ ì‹œê°í™” (PersonTrackerì˜ ì‹¤ì œ ë°˜í™˜ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                if latest_detections:
                    for i, person in enumerate(latest_detections):
                        x1, y1, x2, y2 = map(int, person['bbox'])
                        person_id = person['id']
                        confidence = person['confidence']
                        score = person['score']
                        
                        # ìƒ‰ìƒ í• ë‹¹ (ë¬¸ìì—´ IDì—ì„œ ìˆ«ì ì¶”ì¶œ)
                        if isinstance(person_id, str) and 'Person_' in person_id:
                            # "Person_0" â†’ 0 ì¶”ì¶œ
                            person_num = int(person_id.split('_')[1])
                        else:
                            # ì •ìˆ˜ IDì¸ ê²½ìš°
                            person_num = int(person_id) if isinstance(person_id, (int, float)) else i
                        
                        color = color_palette[person_num % len(color_palette)]
                        
                        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                        cv2.putText(annotated, f"ID:{person_id}", (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                        cv2.putText(annotated, f"Conf: {confidence:.2f}", 
                                   (x1, y2+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                        if score > 0:
                            cv2.putText(annotated, f"Score: {score:.2f}", 
                                       (x1, y2+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ê´€ì ˆì  ì‹œê°í™” (í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ëœ ê²½ìš°)
                if keypoints_detected and current_keypoints is not None:
                    # ê°€ì¥ í° ë°”ìš´ë”© ë°•ìŠ¤ ì‚¬ëŒ ì°¾ê¸°
                    largest_person = None
                    largest_area = 0
                    largest_person_color = (0, 255, 255)  # ê¸°ë³¸ ìƒ‰ìƒ
                    
                    if latest_detections:
                        for person in latest_detections:
                            x1, y1, x2, y2 = map(int, person['bbox'])
                            area = (x2 - x1) * (y2 - y1)
                            if area > largest_area:
                                largest_area = area
                                largest_person = person
                                
                                # ìƒ‰ìƒ í• ë‹¹ (ë¬¸ìì—´ IDì—ì„œ ìˆ«ì ì¶”ì¶œ)
                                person_id = person['id']
                                if isinstance(person_id, str) and 'Person_' in person_id:
                                    person_num = int(person_id.split('_')[1])
                                else:
                                    person_num = int(person_id) if isinstance(person_id, (int, float)) else 0
                                
                                largest_person_color = color_palette[person_num % len(color_palette)]
                    
                    # ê°€ì¥ í° ì‚¬ëŒì˜ ê´€ì ˆì ë§Œ ì‹œê°í™” (í•´ë‹¹ ì‚¬ëŒ ìƒ‰ìƒìœ¼ë¡œ)
                    annotated = self.gesture_recognizer.draw_visualization(annotated, current_keypoints, current_gesture, current_confidence)
                    
                    # ì œìŠ¤ì²˜ ì¸ì‹ìš© í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í‘œì‹œ (9ê°œ ê¸°ì¤€)
                    upper_body_joints = [0, 5, 6, 7, 8, 9, 10, 11, 12]
                    gesture_keypoints = current_keypoints[upper_body_joints]  # 9ê°œ ì¶”ì¶œ
                    valid_gesture_keypoints = np.sum(gesture_keypoints[:, 2] > 0.1)  # 0.3 â†’ 0.1ë¡œ ë‚®ì¶¤
                    cv2.putText(annotated, f"Gesture KPts: {valid_gesture_keypoints}/9", 
                               (10, 250), cv2.FONT_HERSHEY_SIMPLEX, 0.6, largest_person_color, 2)
                    
                    # í‚¤í¬ì¸íŠ¸ ë””ë²„ê¹… ì •ë³´ (60í”„ë ˆì„ë§ˆë‹¤)
                    if frame_count % 60 == 0:
                        print(f"ğŸ¯ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì •ë³´:")
                        print(f"   - keypoints_detected: {keypoints_detected}")
                        print(f"   - current_keypoints shape: {current_keypoints.shape if current_keypoints is not None else 'None'}")
                        print(f"   - ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸: {valid_gesture_keypoints}/9")
                        if largest_person:
                            print(f"   - ê°€ì¥ í° ì‚¬ëŒ: {largest_person['id']} (ë©´ì : {largest_area})")
                        if current_keypoints is not None:
                            print(f"   - ì–´ê¹¨ ì‹ ë¢°ë„: {current_keypoints[5][2]:.2f}/{current_keypoints[6][2]:.2f}")
                            print(f"   - íŒ”ê¿ˆì¹˜ ì‹ ë¢°ë„: {current_keypoints[7][2]:.2f}/{current_keypoints[8][2]:.2f}")
                            print(f"   - ì†ëª© ì‹ ë¢°ë„: {current_keypoints[9][2]:.2f}/{current_keypoints[10][2]:.2f}")
                else:
                    # í‚¤í¬ì¸íŠ¸ê°€ ê°ì§€ë˜ì§€ ì•Šì„ ë•Œ ë””ë²„ê¹… (60í”„ë ˆì„ë§ˆë‹¤)
                    if frame_count % 60 == 0:
                        print(f"âŒ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” ì‹¤íŒ¨:")
                        print(f"   - keypoints_detected: {keypoints_detected}")
                        print(f"   - current_keypoints: {current_keypoints is not None}")
                        print(f"   - gesture_prediction: {gesture_prediction}")
                        print(f"   - gesture_confidence: {gesture_confidence}")
                
                # FPS ê³„ì‚°
                frame_time = time.time() - frame_start
                self.fps_times.append(frame_time)
                fps = 1.0 / (sum(self.fps_times) / len(self.fps_times)) if self.fps_times else 0
                
                # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
                cv2.putText(annotated, f"ğŸš€ Modular System FPS: {fps:.1f}", (10, 40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                cv2.putText(annotated, f"People: {len(latest_detections)}", (10, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                # ì œìŠ¤ì²˜ í‘œì‹œ (ìƒ‰ìƒ ë³€ê²½: COMEì€ ë¹¨ê°„ìƒ‰, NORMALì€ ì´ˆë¡ìƒ‰)
                gesture_color = (0, 0, 255) if current_gesture == "COME" else (0, 255, 0)  # COMEì€ ë¹¨ê°„ìƒ‰, NORMALì€ ì´ˆë¡ìƒ‰
                cv2.putText(annotated, f"Gesture: {current_gesture}", (10, 100), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, gesture_color, 3)
                cv2.putText(annotated, f"Confidence: {current_confidence:.2f}", (10, 130), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, gesture_color, 2)
                
                # ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì •ë³´ ì¶”ê°€
                cv2.putText(annotated, f"Real-time: {gesture_prediction}", (10, 160), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                cv2.putText(annotated, f"Real-conf: {gesture_confidence:.2f}", (10, 180), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                cv2.putText(annotated, f"Keypoints: {'OK' if keypoints_detected else 'NONE'}", 
                           (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
                           (0, 255, 0) if keypoints_detected else (0, 0, 255), 2)
                cv2.putText(annotated, f"Frame: {frame_count}", (10, 220), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # 1ì´ˆ ë‹¨ìœ„ íŒë‹¨ ì •ë³´ í‘œì‹œ (SlidingShiftGCN ëª¨ë¸ì— ë§ì¶¤)
                frames_to_next_decision = self.gesture_recognizer.gesture_decision_interval - (frame_count - self.gesture_recognizer.last_gesture_decision_frame)
                if frames_to_next_decision > 0:
                    cv2.putText(annotated, f"Next Decision: {frames_to_next_decision}f", 
                               (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                else:
                    cv2.putText(annotated, f"Ready for Decision", 
                               (10, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # ëª¨ë“ˆ ìƒíƒœ í‘œì‹œ
                cv2.putText(annotated, "Modules: Running", (10, 220), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
                
                # í™”ë©´ í‘œì‹œ
                if window_name:
                    try:
                        cv2.imshow(window_name, annotated)
                        cv2.waitKey(1)
                        
                        # Back ì°½ì¸ ê²½ìš° ê°•ì œë¡œ ì•ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                        if "Back" in system_name or "Back Camera" in window_name:
                            cv2.setWindowProperty(window_name, cv2.WND_PROP_TOPMOST, 1)
                            cv2.waitKey(1)
                            # ì°½ì„ ë‹¤ì‹œ í‘œì‹œ
                            cv2.imshow(window_name, annotated)
                            cv2.waitKey(1)
                            
                    except Exception as e:
                        print(f"âŒ í™”ë©´ í‘œì‹œ ì‹¤íŒ¨: {e}")
                        window_name = None
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                if frame_count % 120 == 0:
                    print(f"ğŸ“Š FPS: {fps:.1f} | People: {len(latest_detections)} | Gesture: {current_gesture}")
                    print(f"   í™”ë©´ í™•ì¸: ë°ê¸° {np.mean(annotated):.1f}, í¬ê¸° {annotated.shape}")
                    print(f"   ì œìŠ¤ì²˜ ìƒíƒœ: ë²„í¼ {len(self.gesture_recognizer.gesture_frame_buffer)}/30, ë‹¤ìŒ íŒë‹¨ê¹Œì§€ {frames_to_next_decision}í”„ë ˆì„")
                    print(f"   í‚¤í¬ì¸íŠ¸ ìƒíƒœ: ê°ì§€={keypoints_detected}, ë°ì´í„°={current_keypoints is not None}")
                    
                    # íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ ë””ë²„ê¹…
                    if latest_detections:
                        print(f"ğŸ” íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­ ë””ë²„ê¹…:")
                        for person in latest_detections:
                            person_id = person['id']
                            score = person['score']
                            print(f"   - {person_id}: ë§¤ì¹­ ì ìˆ˜ = {score:.3f}")
                            
                            # PersonTrackerì—ì„œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            if person_id in self.person_tracker.people_data:
                                pdata = self.person_tracker.people_data[person_id]
                                hist_count = len(pdata['histograms'])
                                print(f"     ì €ì¥ëœ íˆìŠ¤í† ê·¸ë¨: {hist_count}ê°œ")
                                if hist_count > 0:
                                    latest_hist = pdata['histograms'][-1]
                                    hist_std = np.std(latest_hist)
                                    hist_mean = np.mean(latest_hist)
                                    print(f"     ìµœì‹  íˆìŠ¤í† ê·¸ë¨ - í‰ê· : {hist_mean:.3f}, í‘œì¤€í¸ì°¨: {hist_std:.3f}")
                    
                    # ì „ì²´ ì‚¬ëŒ ë°ì´í„° ìƒíƒœ
                    total_people = len(self.person_tracker.people_data)
                    print(f"   ì „ì²´ ë“±ë¡ëœ ì‚¬ëŒ: {total_people}ëª…")
                    for pid, pdata in self.person_tracker.people_data.items():
                        hist_count = len(pdata['histograms'])
                        print(f"     {pid}: {hist_count}ê°œ íˆìŠ¤í† ê·¸ë¨")
        
        except KeyboardInterrupt:
            print("ğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨")
        
        finally:
            # ì •ë¦¬
            self.running = False
            
            # ëª¨ë“ˆ ì¤‘ì§€
            self.person_tracker.stop()
            self.gesture_recognizer.stop()
            
            # ì¹´ë©”ë¼ í•´ì œ
            cap.release()
            cv2.destroyAllWindows()
            
            # fps ë³€ìˆ˜ ì•ˆì „ ì²˜ë¦¬
            if 'fps' not in locals():
                fps = 0.0
            
            print(f"\nğŸ‰ ì‹œìŠ¤í…œ ì¢…ë£Œ")
            print(f"   - ì´ í”„ë ˆì„: {frame_count}")
            print(f"   - ìµœì¢… FPS: {fps:.1f}")
            print(f"   - ê°ì§€ëœ ì‚¬ëŒ: {len(self.person_tracker.people_data)}")

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    parser = argparse.ArgumentParser(description='í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰')
    parser.add_argument('--camera', type=str, required=True,
                       help='ì¹´ë©”ë¼ ì¥ì¹˜ (í•„ìˆ˜)')
    parser.add_argument('--name', type=str, default="Integrated System", 
                       help='ì‹œìŠ¤í…œ ì´ë¦„ (ê¸°ë³¸ê°’: Integrated System)')
    
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ì‹¤í–‰
    system = IntegratedSystem()
    system.run_system(camera_device=args.camera, system_name=args.name) 
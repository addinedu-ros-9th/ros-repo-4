#!/usr/bin/env python3
"""
ê°•ì˜ì‹¤ í™˜ê²½ìš© ê°ì²´ì¸ì‹ ì‹œìŠ¤í…œ (ì‚¬ëŒë§Œ ì¸ì‹)
- ì‚¬ëŒ ì¸ì‹ë§Œ ìˆ˜í–‰
- íŠ¸ë˜í‚¹ì„ ìœ„í•œ ê¸°ë³¸ ê°ì§€ ê¸°ëŠ¥
"""

import cv2
import torch
from ultralytics import YOLO
import numpy as np
import time

class PersonDetector:
    def __init__(self, model_path="yolov8n.pt"):
        """
        ì‚¬ëŒ ì¸ì‹ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: YOLO ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸: yolov8n.pt)
        """
        print("ğŸš€ ì‚¬ëŒ ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # YOLOv8 ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(model_path)
        print(f"âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        
        # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ (COCO ë°ì´í„°ì…‹ ê¸°ì¤€)
        self.person_class_id = 0  # COCOì—ì„œ personì€ 0ë²ˆ
        
        print(f"ğŸ¯ íƒ€ê²Ÿ í´ë˜ìŠ¤: person")
        
        # ì„±ëŠ¥ í†µê³„
        self.frame_count = 0
        self.start_time = time.time()
    
    def detect_people(self, image):
        """
        ì´ë¯¸ì§€ì—ì„œ ì‚¬ëŒë“¤ì„ ì¸ì‹
        
        Args:
            image: OpenCV ì´ë¯¸ì§€ (BGR)
            
        Returns:
            list: ê°ì§€ëœ ì‚¬ëŒë“¤ì˜ ì •ë³´ [{confidence, bbox, center}, ...]
        """
        # YOLO ì¶”ë¡ 
        results = self.model(image, verbose=False)
        
        detected_people = []
        
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # í´ë˜ìŠ¤ IDì™€ ì‹ ë¢°ë„
                    class_id = int(box.cls[0])
                    confidence = float(box.conf[0])
                    
                    # ì‚¬ëŒë§Œ ê°ì§€ (class_id == 0)
                    if class_id == self.person_class_id and confidence > 0.5:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                        x1, y1, x2, y2 = box.xyxy[0].tolist()
                        
                        person_info = {
                            'confidence': confidence,
                            'bbox': [int(x1), int(y1), int(x2), int(y2)],
                            'center': [int((x1+x2)/2), int((y1+y2)/2)],
                            'area': (int(x2)-int(x1)) * (int(y2)-int(y1))
                        }
                        detected_people.append(person_info)
        
        return detected_people
    
    def draw_detections(self, image, detections):
        """
        ê°ì§€ëœ ì‚¬ëŒë“¤ì„ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            detections: detect_people()ì—ì„œ ë°˜í™˜ëœ ì‚¬ëŒ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            image: ë°”ìš´ë”©ë°•ìŠ¤ê°€ ê·¸ë ¤ì§„ ì´ë¯¸ì§€
        """
        result_image = image.copy()
        
        # ì‚¬ëŒ ìƒ‰ìƒ (ì´ˆë¡ìƒ‰)
        person_color = (0, 255, 0)
        
        for person in detections:
            confidence = person['confidence']
            x1, y1, x2, y2 = person['bbox']
            area = person['area']
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(result_image, (x1, y1), (x2, y2), person_color, 2)
            
            # ë¼ë²¨ í…ìŠ¤íŠ¸
            label = f"Person: {confidence:.2f}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
            cv2.rectangle(result_image, (x1, y1-label_h-10), (x1+label_w, y1), person_color, -1)
            
            # í…ìŠ¤íŠ¸
            cv2.putText(result_image, label, (x1, y1-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            
            # ë©´ì  ì •ë³´
            area_text = f"Area: {area}"
            cv2.putText(result_image, area_text, (x1, y2+20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, person_color, 1)
        
        # í†µê³„ ì •ë³´
        stats_text = f"People: {len(detections)}"
        cv2.putText(result_image, stats_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # FPS ì •ë³´ í‘œì‹œ
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        if elapsed_time > 0:
            fps = self.frame_count / elapsed_time
            fps_text = f"FPS: {fps:.1f}"
            cv2.putText(result_image, fps_text, (10, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return result_image

def main():
    """
    ì›¹ìº ì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ì‚¬ëŒ ì¸ì‹ í…ŒìŠ¤íŠ¸
    """
    print("ğŸ¥ ì›¹ìº  ì‹¤ì‹œê°„ ì‚¬ëŒ ì¸ì‹ ì‹œì‘...")
    
    # ì‚¬ëŒ ì¸ì‹ê¸° ì´ˆê¸°í™”
    detector = PersonDetector()
    
    # ì›¹ìº  ì—°ê²° (ì—¬ëŸ¬ ì¸ë±ìŠ¤ ì‹œë„)
    cap = None
    for cam_id in [0, 1]:
        cap = cv2.VideoCapture(cam_id)
        if cap.isOpened():
            print(f"ğŸ“¹ ì›¹ìº  {cam_id} ì—°ê²° ì„±ê³µ!")
            break
        cap.release()
    
    if cap is None or not cap.isOpened():
        print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ì›¹ìº  í•´ìƒë„ ì„¤ì •
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    print("ğŸ“– ì‚¬ìš©ë²•:")
    print("  - q: ì¢…ë£Œ")
    print("  - s: ìŠ¤í¬ë¦°ìƒ· ì €ì¥")
    print("  - d: ì‚¬ëŒ ê°ì§€ ê²°ê³¼ ìƒì„¸ ì¶œë ¥ í† ê¸€")
    
    # ìƒíƒœ ë³€ìˆ˜
    detailed_output = False
    screenshot_count = 0
    
    # ìœˆë„ìš° ìƒì„± ë° ì„¤ì •
    window_name = "ì‚¬ëŒ ì¸ì‹ ì‹œìŠ¤í…œ"
    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
    
    try:
        while True:
            # í”„ë ˆì„ ì½ê¸°
            ret, frame = cap.read()
            if not ret:
                print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
                break
            
            # ì‚¬ëŒ ì¸ì‹
            detections = detector.detect_people(frame)
            
            # ê²°ê³¼ ì‹œê°í™”
            result_frame = detector.draw_detections(frame, detections)
            
            # ê°ì§€ëœ ì‚¬ëŒ ì •ë³´ ì¶œë ¥ (ì˜µì…˜)
            if detections and detailed_output:
                print(f"ğŸ‘¥ ê°ì§€ëœ ì‚¬ëŒ: {len(detections)}ëª…")
                for i, person in enumerate(detections):
                    print(f"  - Person {i+1}: {person['confidence']:.2f} at {person['center']}, Area: {person['area']}")
            
            # í™”ë©´ì— í‘œì‹œ
            cv2.imshow(window_name, result_frame)
            
            # í‚¤ ì…ë ¥ ì²˜ë¦¬
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("ğŸ›‘ ì‚¬ìš©ìê°€ ì¢…ë£Œë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤")
                break
            elif key == ord('s'):
                # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                filename = f"person_detection_{screenshot_count:03d}.jpg"
                cv2.imwrite(filename, result_frame)
                print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
                screenshot_count += 1
            elif key == ord('d'):
                # ìƒì„¸ ì¶œë ¥ í† ê¸€
                detailed_output = not detailed_output
                status = "ì¼œì§" if detailed_output else "êº¼ì§"
                print(f"ğŸ” ìƒì„¸ ì¶œë ¥: {status}")
                
    except KeyboardInterrupt:
        print("ğŸ›‘ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        # ìµœì¢… í†µê³„
        elapsed_time = time.time() - detector.start_time
        avg_fps = detector.frame_count / elapsed_time if elapsed_time > 0 else 0
        print(f"âœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print(f"ğŸ“Š í†µê³„: {detector.frame_count}í”„ë ˆì„, í‰ê·  FPS: {avg_fps:.1f}")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
ğŸš€ Advanced Person Tracking System
- YOLOv8 + OSNet + BoT-SORT + FAISS ì¡°í•©
- ì‹¤ì‹œê°„ ê³ ì„±ëŠ¥ ì‚¬ëŒ ì¶”ì  ë° ì¬ì‹ë³„
- í”„ë ˆì„ ë°– ì¬ë“±ì¥ ì‹œ ID ìœ ì§€
"""

import cv2
import numpy as np
import sys
import os
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import faiss
import pickle
from collections import deque
import time

sys.path.append('../object_detection')
from simple_detector import PersonDetector

def normalize_embedding(embedding):
    """L2 ì •ê·œí™” í•¨ìˆ˜ - cosine similarity ìµœì í™”"""
    norm = np.linalg.norm(embedding)
    if norm == 0:
        return embedding
    return embedding / norm

def normalize_tensor(tensor):
    """PyTorch í…ì„œ L2 ì •ê·œí™”"""
    return F.normalize(tensor, p=2, dim=1)

def compute_cosine_similarity_pytorch(tensor_a, tensor_b):
    """PyTorch cosine similarity ê³„ì‚°"""
    # L2 ì •ê·œí™”
    tensor_a_norm = normalize_tensor(tensor_a)
    tensor_b_norm = normalize_tensor(tensor_b)
    
    # Cosine similarity
    similarity = F.cosine_similarity(tensor_a_norm, tensor_b_norm, dim=1)
    return similarity.item()

class BoTSORTTracker:
    """BoT-SORT ê¸°ë°˜ ì¶”ì ê¸°"""
    def __init__(self, max_disappeared=60, iou_threshold=0.3):  # 15 â†’ 60í”„ë ˆì„ìœ¼ë¡œ ì¦ê°€
        self.tracks = {}
        self.next_id = 0
        self.max_disappeared = max_disappeared  # 2ì´ˆ (30fps ê¸°ì¤€)
        self.iou_threshold = iou_threshold
        self.frame_count = 0
        
    def update(self, detections):
        """BoT-SORT ì—…ë°ì´íŠ¸"""
        self.frame_count += 1
        
        # í˜„ì¬ í”„ë ˆì„ì˜ íŠ¸ë™ ìƒíƒœ
        current_tracks = {}
        
        # ê¸°ì¡´ íŠ¸ë™ê³¼ ìƒˆë¡œìš´ ê°ì§€ ê²°ê³¼ ë§¤ì¹­
        if self.tracks:
            track_ids = list(self.tracks.keys())
            track_boxes = [self.tracks[tid]['bbox'] for tid in track_ids]
            
            # IoU ê¸°ë°˜ ë§¤ì¹­
            matches = self._hungarian_matching(track_boxes, [d['bbox'] for d in detections])
            
            # ë§¤ì¹­ëœ íŠ¸ë™ ì—…ë°ì´íŠ¸
            for track_idx, det_idx in matches:
                if det_idx is not None:
                    track_id = track_ids[track_idx]
                    detection = detections[det_idx]
                    
                    # íŠ¸ë™ ì—…ë°ì´íŠ¸
                    self.tracks[track_id].update({
                        'bbox': detection['bbox'],
                        'confidence': detection['confidence'],
                        'last_seen': self.frame_count,
                        'disappeared': 0
                    })
                    current_tracks[track_id] = self.tracks[track_id]
        
        # ìƒˆë¡œìš´ ê°ì§€ ê²°ê³¼ì— ëŒ€í•´ ìƒˆ íŠ¸ë™ ìƒì„±
        for detection in detections:
            if not self._is_tracked(detection['bbox']):
                self.tracks[self.next_id] = {
                    'bbox': detection['bbox'],
                    'confidence': detection['confidence'],
                    'last_seen': self.frame_count,
                    'disappeared': 0,
                    'created': self.frame_count
                }
                current_tracks[self.next_id] = self.tracks[self.next_id]
                self.next_id += 1
        
        # ì‚¬ë¼ì§„ íŠ¸ë™ ì²˜ë¦¬
        tracks_to_remove = []
        for track_id, track_data in self.tracks.items():
            if track_id not in current_tracks:
                track_data['disappeared'] += 1
                if track_data['disappeared'] <= self.max_disappeared:
                    current_tracks[track_id] = track_data
                else:
                    tracks_to_remove.append(track_id)
        
        for track_id in tracks_to_remove:
            del self.tracks[track_id]
        
        return current_tracks
    
    def _hungarian_matching(self, track_boxes, detection_boxes):
        """í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ë§¤ì¹­"""
        if not track_boxes or not detection_boxes:
            return []
        
        # IoU ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        cost_matrix = np.zeros((len(track_boxes), len(detection_boxes)))
        for i, track_box in enumerate(track_boxes):
            for j, det_box in enumerate(detection_boxes):
                iou = self._calculate_iou(track_box, det_box)
                cost_matrix[i, j] = 1 - iou  # ë¹„ìš©ì€ 1-IoU
        
        # í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ì ìš©
        from scipy.optimize import linear_sum_assignment
        track_indices, det_indices = linear_sum_assignment(cost_matrix)
        
        # ì„ê³„ê°’ ì´ìƒì˜ ë§¤ì¹­ë§Œ ë°˜í™˜
        matches = []
        for track_idx, det_idx in zip(track_indices, det_indices):
            if cost_matrix[track_idx, det_idx] < (1 - self.iou_threshold):
                matches.append((track_idx, det_idx))
        
        return matches
    
    def _calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # êµì§‘í•© ì˜ì—­
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def _is_tracked(self, bbox):
        """ì´ë¯¸ ì¶”ì  ì¤‘ì¸ ë°”ìš´ë”©ë°•ìŠ¤ì¸ì§€ í™•ì¸"""
        for track_data in self.tracks.values():
            if self._calculate_iou(track_data['bbox'], bbox) > self.iou_threshold:
                return True
        return False

class FAISSReIDStore:
    """FAISS ê¸°ë°˜ Re-ID íŠ¹ì§• ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ë‚´ ë¦¬ìŠ¤íŠ¸) - ê°œì„ ëœ cosine matching + L2 ì •ê·œí™” + Re-ranking"""
    def __init__(self, feature_dim=512, similarity_threshold=0.90):  # 0.95 â†’ 0.90ìœ¼ë¡œ ì¡°ì •
        self.feature_dim = feature_dim
        self.similarity_threshold = similarity_threshold
        self.features = []  # íŠ¹ì§• ë²¡í„° ë¦¬ìŠ¤íŠ¸
        self.ids = []       # ì‚¬ëŒ ID ë¦¬ìŠ¤íŠ¸
        self.index = None   # FAISS ì¸ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤
        
        # **ì„ë² ë”© í‰ê· í™”ë¥¼ ìœ„í•œ ì €ì¥ì†Œ**
        self.embedding_history = {}  # {person_id: [embeddings]}
        self.avg_embeddings = {}     # {person_id: averaged_embedding}
        self.min_frames_for_avg = 5  # í‰ê· í™”ì— í•„ìš”í•œ ìµœì†Œ í”„ë ˆì„ ìˆ˜
        
        # **Re-ranking ì„¤ì •**
        self.use_re_ranking = True
        self.top_k_candidates = 10   # Re-ranking í›„ë³´ ìˆ˜
        self.k_reciprocal = 20       # k-reciprocal íŒŒë¼ë¯¸í„°
        
        self._build_index()
        
        print(f"âœ… FAISS Re-ID Store initialized")
        print(f"   - Feature dimension: {self.feature_dim}")
        print(f"   - Similarity threshold: {self.similarity_threshold}")
        print(f"   - Index type: Inner Product (cosine similarity)")
        print(f"   - L2 normalization: Enabled")
        print(f"   - Embedding averaging: {self.min_frames_for_avg} frames")
        print(f"   - Re-ranking: {'Enabled' if self.use_re_ranking else 'Disabled'}")
        
    def _build_index(self):
        """FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        self.index = faiss.IndexFlatIP(self.feature_dim)  # Inner Product (cosine similarity)
        
    def add_person(self, person_id, features):
        """ìƒˆë¡œìš´ ì‚¬ëŒ íŠ¹ì§•ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ - ì„ë² ë”© í‰ê· í™” ì ìš©"""
        if len(features) != self.feature_dim:
            print(f"âŒ Feature dimension mismatch: {len(features)} != {self.feature_dim}")
            return False
        
        # L2 ì •ê·œí™” (ì¤‘ë³µ ì •ê·œí™” ë°©ì§€)
        features_norm = normalize_embedding(features)
        
        # ì„ë² ë”© íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        if person_id not in self.embedding_history:
            self.embedding_history[person_id] = []
        self.embedding_history[person_id].append(features_norm)
        
        # í‰ê·  ì„ë² ë”© ê³„ì‚° (ì¶©ë¶„í•œ í”„ë ˆì„ì´ ìŒ“ì˜€ì„ ë•Œ)
        if len(self.embedding_history[person_id]) >= self.min_frames_for_avg:
            avg_embedding = self._compute_average_embedding(person_id)
            self.avg_embeddings[person_id] = avg_embedding
            final_embedding = avg_embedding
            print(f"âœ… Person {person_id} using averaged embedding ({len(self.embedding_history[person_id])} frames)")
        else:
            final_embedding = features_norm
            print(f"âš ï¸ Person {person_id} using single embedding ({len(self.embedding_history[person_id])}/{self.min_frames_for_avg} frames)")
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(final_embedding.reshape(1, -1))
        self.features.append(final_embedding)
        self.ids.append(person_id)
        
        print(f"âœ… Person {person_id} added to Re-ID store")
        print(f"   - Feature norm: {np.linalg.norm(final_embedding):.6f}")
        print(f"   - Store size: {len(self.ids)}")
        
        return True
    
    def _compute_average_embedding(self, person_id):
        """íŠ¹ì • ì‚¬ëŒì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°"""
        embeddings = self.embedding_history[person_id]
        avg_embedding = np.mean(embeddings, axis=0)
        return normalize_embedding(avg_embedding)
    
    def find_similar_person(self, features, top_k=5):
        """ìœ ì‚¬í•œ ì‚¬ëŒ ì°¾ê¸° - Re-ranking ì ìš©"""
        if len(self.features) == 0:
            print("âš ï¸ Re-ID store is empty")
            return None, 0.0
        
        # L2 ì •ê·œí™” (ì¤‘ë³µ ì •ê·œí™” ë°©ì§€)
        features_norm = normalize_embedding(features)
        
        # FAISS ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´ ê²€ìƒ‰)
        search_k = max(top_k, self.top_k_candidates) if self.use_re_ranking else top_k
        similarities, indices = self.index.search(features_norm.reshape(1, -1), min(search_k, len(self.features)))
        
        # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        similarities = similarities[0].tolist()
        indices = indices[0].tolist()
        
        print(f"ğŸ” Initial cosine similarity search results:")
        for i, (sim, idx) in enumerate(zip(similarities, indices)):
            person_id = self.ids[idx]
            print(f"   - Person {person_id}: {sim:.6f}")
        
        if self.use_re_ranking and len(indices) > 1:
            # Re-ranking ì ìš©
            re_ranked_similarities, re_ranked_indices = self._apply_re_ranking(
                features_norm, similarities, indices
            )
            print(f"ğŸ”„ Re-ranking results:")
            for i, (sim, idx) in enumerate(zip(re_ranked_similarities, re_ranked_indices)):
                person_id = self.ids[idx]
                print(f"   - Person {person_id}: {sim:.6f}")
            
            # Re-ranking ê²°ê³¼ ì‚¬ìš©
            similarities = re_ranked_similarities
            indices = re_ranked_indices
        
        if len(indices) > 0 and similarities[0] > self.similarity_threshold:
            best_idx = int(indices[0])  # ì •ìˆ˜ë¡œ ë³€í™˜
            best_similarity = similarities[0]
            best_id = self.ids[best_idx]
            
            print(f"âœ… Best match: Person {best_id} (similarity: {best_similarity:.6f})")
            return best_id, best_similarity
        else:
            print(f"âŒ No match above threshold {self.similarity_threshold}")
            return None, 0.0
    
    def _apply_re_ranking(self, query_features, similarities, indices):
        """k-reciprocal Re-ranking ì ìš©"""
        if len(indices) < 2:
            return similarities, indices
        
        # í›„ë³´ ì„ë² ë”©ë“¤
        candidate_features = [self.features[int(idx)] for idx in indices]  # int() ì¶”ê°€
        candidate_features = np.array(candidate_features)
        
        # ì¿¼ë¦¬ì™€ í›„ë³´ë“¤ ê°„ì˜ ìœ ì‚¬ë„
        query_similarities = np.array(similarities)
        
        # í›„ë³´ë“¤ ê°„ì˜ ìƒí˜¸ ìœ ì‚¬ë„ ê³„ì‚°
        candidate_similarities = np.dot(candidate_features, candidate_features.T)
        
        # k-reciprocal ê³„ì‚°
        k_reciprocal_scores = self._compute_k_reciprocal_scores(
            query_similarities, candidate_similarities
        )
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì¿¼ë¦¬ ìœ ì‚¬ë„ + k-reciprocal ì ìˆ˜)
        final_scores = 0.5 * query_similarities + 0.5 * k_reciprocal_scores
        
        # ì •ë ¬
        sorted_indices = np.argsort(final_scores)[::-1]
        
        return final_scores[sorted_indices], [indices[i] for i in sorted_indices]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    def _compute_k_reciprocal_scores(self, query_similarities, candidate_similarities):
        """k-reciprocal ì ìˆ˜ ê³„ì‚°"""
        k = min(self.k_reciprocal, len(candidate_similarities))
        
        # ê° í›„ë³´ì— ëŒ€í•´ k-nearest neighbors ì°¾ê¸°
        k_nearest_indices = np.argsort(candidate_similarities, axis=1)[:, -k:]
        
        # k-reciprocal ì ìˆ˜ ê³„ì‚°
        k_reciprocal_scores = np.zeros(len(query_similarities))
        
        for i in range(len(query_similarities)):
            # í›„ë³´ iì˜ k-nearest neighbors
            neighbors = k_nearest_indices[i]
            
            # ê° neighborê°€ í›„ë³´ ië¥¼ k-nearestì— í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
            reciprocal_count = 0
            for neighbor_idx in neighbors:
                if i in k_nearest_indices[neighbor_idx]:
                    reciprocal_count += 1
            
            k_reciprocal_scores[i] = reciprocal_count / k
        
        return k_reciprocal_scores
    
    def update_person_features(self, person_id, new_features):
        """ê¸°ì¡´ ì‚¬ëŒì˜ íŠ¹ì§• ì—…ë°ì´íŠ¸ - ì„ë² ë”© í‰ê· í™” ì ìš©"""
        if person_id in self.ids:
            idx = self.ids.index(person_id)
            
            # ì„ë² ë”© íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            if person_id not in self.embedding_history:
                self.embedding_history[person_id] = []
            self.embedding_history[person_id].append(normalize_embedding(new_features))
            
            # í‰ê·  ì„ë² ë”© ì¬ê³„ì‚°
            if len(self.embedding_history[person_id]) >= self.min_frames_for_avg:
                avg_embedding = self._compute_average_embedding(person_id)
                self.avg_embeddings[person_id] = avg_embedding
                features_norm = avg_embedding
                print(f"âœ… Person {person_id} updated with averaged embedding")
            else:
                features_norm = normalize_embedding(new_features)
                print(f"âš ï¸ Person {person_id} updated with single embedding")
            
            self.features[idx] = features_norm
            
            # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            self._rebuild_index()
            
            print(f"ğŸ”„ Person {person_id} features updated")
            print(f"   - New feature norm: {np.linalg.norm(features_norm):.6f}")
    
    def _rebuild_index(self):
        """FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        self.index = faiss.IndexFlatIP(self.feature_dim)
        if self.features:
            features_array = np.array(self.features)
            self.index.add(features_array)
    
    def get_person_count(self):
        """ì €ì¥ëœ ì‚¬ëŒ ìˆ˜ ë°˜í™˜"""
        return len(self.ids)

class AdvancedOSNet:
    """ê³ ê¸‰ OSNet ëª¨ë¸ - ì‹¤ì œ ëª¨ë¸ ì‚¬ìš©"""
    def __init__(self, feature_dim=512):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.feature_dim = feature_dim
        self.model = None
        self.transform = None
        self._initialize_model()
        
    def _initialize_model(self):
        """OSNet ëª¨ë¸ ì´ˆê¸°í™” - ì§„ì§œ OSNet êµ¬ì¡° êµ¬í˜„"""
        try:
            # ì§„ì§œ OSNet êµ¬ì¡° (Omni-Scale Learning + Multi-Scale Features)
            class OSNet(nn.Module):
                def __init__(self, num_classes=751, feature_dim=512):
                    super(OSNet, self).__init__()
                    
                    # **OSNetì˜ í•µì‹¬: Omni-Scale Learning**
                    # ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ ë™ì‹œì— í•™ìŠµí•˜ëŠ” êµ¬ì¡°
                    
                    # Stage 1: ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ (7x7 conv)
                    self.conv1 = nn.Sequential(
                        nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False),
                        nn.BatchNorm2d(64),
                        nn.ReLU(inplace=True),
                        nn.MaxPool2d(3, stride=2, padding=1)
                    )
                    
                    # Stage 2: Omni-Scale Block 1 (ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§•)
                    self.os_block1 = self._make_omni_scale_block(64, 128, 2)
                    
                    # Stage 3: Omni-Scale Block 2 (ë” ì„¸ë°€í•œ íŠ¹ì§•)
                    self.os_block2 = self._make_omni_scale_block(128, 256, 2)
                    
                    # Stage 4: Omni-Scale Block 3 (ê³ ìˆ˜ì¤€ íŠ¹ì§•)
                    self.os_block3 = self._make_omni_scale_block(256, 512, 2)
                    
                    # **Multi-Scale Feature Fusion**
                    # ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ ìœµí•©í•˜ì—¬ ì„¸ë¶€ ì†ì„± ì¸ì‹ í–¥ìƒ
                    self.feature_fusion = nn.Sequential(
                        nn.AdaptiveAvgPool2d((1, 1)),
                        nn.Flatten(),
                        nn.Dropout(0.5)
                    )
                    
                    # **Attribute-Aware Feature Extractor**
                    # ì˜ë³µ, ì•¡ì„¸ì„œë¦¬ ë“± ì„¸ë¶€ ì†ì„±ì„ ì¸ì‹í•˜ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°
                    self.attribute_extractor = nn.Sequential(
                        nn.Linear(512, 1024),
                        nn.ReLU(inplace=True),
                        nn.Dropout(0.5),
                        nn.Linear(1024, feature_dim)
                    )
                    
                    # ë¶„ë¥˜ê¸° (ì‚¬ì „í›ˆë ¨ìš©)
                    self.classifier = nn.Linear(feature_dim, num_classes)
                    
                def _make_omni_scale_block(self, in_channels, out_channels, num_layers):
                    """Omni-Scale Block: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ ë™ì‹œì— í•™ìŠµ"""
                    layers = []
                    
                    for i in range(num_layers):
                        # **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì»¨ë³¼ë£¨ì…˜**
                        # 1x1, 3x3, 5x5 ë“± ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ì»¨ë³¼ë£¨ì…˜ì„ ë³‘ë ¬ë¡œ ì ìš©
                        multi_scale_conv = nn.ModuleList([
                            # 1x1 ìŠ¤ì¼€ì¼ (ì„¸ë°€í•œ íŠ¹ì§•)
                            nn.Sequential(
                                nn.Conv2d(in_channels if i == 0 else out_channels, 
                                        out_channels // 4, 1, bias=False),
                                nn.BatchNorm2d(out_channels // 4),
                                nn.ReLU(inplace=True)
                            ),
                            # 3x3 ìŠ¤ì¼€ì¼ (ì¤‘ê°„ íŠ¹ì§•)
                            nn.Sequential(
                                nn.Conv2d(in_channels if i == 0 else out_channels, 
                                        out_channels // 4, 3, padding=1, bias=False),
                                nn.BatchNorm2d(out_channels // 4),
                                nn.ReLU(inplace=True)
                            ),
                            # 5x5 ìŠ¤ì¼€ì¼ (í° íŠ¹ì§•)
                            nn.Sequential(
                                nn.Conv2d(in_channels if i == 0 else out_channels, 
                                        out_channels // 4, 5, padding=2, bias=False),
                                nn.BatchNorm2d(out_channels // 4),
                                nn.ReLU(inplace=True)
                            ),
                            # 7x7 ìŠ¤ì¼€ì¼ (ë§¤ìš° í° íŠ¹ì§•)
                            nn.Sequential(
                                nn.Conv2d(in_channels if i == 0 else out_channels, 
                                        out_channels // 4, 7, padding=3, bias=False),
                                nn.BatchNorm2d(out_channels // 4),
                                nn.ReLU(inplace=True)
                            )
                        ])
                        
                        # **Multi-Scale Feature Fusion**
                        fusion_layer = nn.Sequential(
                            nn.Conv2d(out_channels, out_channels, 1, bias=False),
                            nn.BatchNorm2d(out_channels),
                            nn.ReLU(inplace=True)
                        )
                        
                        # **Residual Connection**
                        if in_channels != out_channels:
                            shortcut = nn.Sequential(
                                nn.Conv2d(in_channels if i == 0 else out_channels, 
                                        out_channels, 1, bias=False),
                                nn.BatchNorm2d(out_channels)
                            )
                        else:
                            shortcut = nn.Identity()
                        
                        layers.append(nn.ModuleDict({
                            'multi_scale_conv': multi_scale_conv,
                            'fusion': fusion_layer,
                            'shortcut': shortcut
                        }))
                        
                        # ì²« ë²ˆì§¸ ë ˆì´ì–´ì—ì„œë§Œ ë‹¤ìš´ìƒ˜í”Œë§
                        if i == 0:
                            layers.append(nn.MaxPool2d(2, stride=2))
                    
                    return nn.ModuleList(layers)
                
                def _forward_omni_scale_block(self, x, block):
                    """Omni-Scale Blockì˜ ìˆœì „íŒŒ"""
                    for layer in block:
                        if isinstance(layer, nn.ModuleDict):
                            # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
                            multi_scale_features = []
                            for conv in layer['multi_scale_conv']:
                                multi_scale_features.append(conv(x))
                            
                            # íŠ¹ì§• ìœµí•©
                            fused_features = torch.cat(multi_scale_features, dim=1)
                            fused_features = layer['fusion'](fused_features)
                            
                            # Residual connection
                            shortcut = layer['shortcut'](x)
                            x = fused_features + shortcut
                            x = F.relu(x)
                        else:
                            # ë‹¤ìš´ìƒ˜í”Œë§
                            x = layer(x)
                    
                    return x
                
                def forward(self, x):
                    # Stage 1: ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ
                    x = self.conv1(x)
                    
                    # Stage 2-4: Omni-Scale Blocks
                    x = self._forward_omni_scale_block(x, self.os_block1)
                    x = self._forward_omni_scale_block(x, self.os_block2)
                    x = self._forward_omni_scale_block(x, self.os_block3)
                    
                    # Multi-Scale Feature Fusion
                    x = self.feature_fusion(x)
                    
                    # Attribute-Aware Feature Extraction
                    features = self.attribute_extractor(x)
                    
                    # ë¶„ë¥˜ ì¶œë ¥ (ì‚¬ì „í›ˆë ¨ìš©)
                    cls_output = self.classifier(features)
                    
                    return cls_output, features
            
            self.model = OSNet(feature_dim=self.feature_dim)
            
            # **Xavier/Glorot ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (OSNet í‘œì¤€)**
            for m in self.model.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)
            
            self.model.to(self.device)
            self.model.eval()
            
            # **OSNet í‘œì¤€ ì „ì²˜ë¦¬ (Market1501 ë°ì´í„°ì…‹ ê¸°ë°˜)**
            self.transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((256, 128)),  # OSNet í‘œì¤€ í¬ê¸°
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            print("âœ… Advanced OSNet model initialized successfully")
            print(f"   - Feature dimension: {self.feature_dim}")
            print(f"   - Device: {self.device}")
            print(f"   - Input size: 256x128")
            print("   - Omni-Scale Learning: Enabled")
            print("   - Multi-Scale Feature Fusion: Enabled")
            print("   - Attribute-Aware Features: Enabled")
            print("   - ì˜ë³µ ìŠ¤íƒ€ì¼, ì•¡ì„¸ì„œë¦¬ ì¸ì‹ ìµœì í™”")
            
        except Exception as e:
            print(f"âŒ OSNet initialization failed: {e}")
            print("   - Falling back to simple feature extraction")
            self.model = None
    
    def extract_features(self, frame, bbox):
        """OSNet íŠ¹ì§• ì¶”ì¶œ - ì„±ëŠ¥ ìµœì í™” + ì„¸ë¶€ ì†ì„± ì¸ì‹"""
        if self.model is None:
            print("âš ï¸ Using fallback feature extraction (OSNet not available)")
            return self._extract_simple_features(frame, bbox)
        
        try:
            x1, y1, x2, y2 = map(int, bbox)
            roi = frame[y1:y2, x1:x2]
            
            if roi.size == 0:
                return None
            
            # ROI í¬ê¸° ì œí•œ ë° í’ˆì§ˆ í™•ì¸
            if roi.shape[0] < 50 or roi.shape[1] < 25:
                print(f"âš ï¸ ROI too small: {roi.shape[1]}x{roi.shape[0]}")
                return None
            
            # **1. ì„±ëŠ¥ ì œì–´: ì„¸ê·¸ë©˜í…Œì´ì…˜ ìŠ¤í‚µ ì—¬ë¶€ ê²°ì •**
            use_segmentation = False
            if self.enable_segmentation:
                # ìƒˆë¡œìš´ ì‚¬ëŒì´ê±°ë‚˜ ì¼ì • í”„ë ˆì„ë§ˆë‹¤ë§Œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰
                if (self.frame_count - self.last_segmentation_frame) >= self.segmentation_skip_frames:
                    use_segmentation = True
                    self.last_segmentation_frame = self.frame_count
                else:
                    self.stats['segmentation_skipped'] += 1
            
            # **2. ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰ (ì„±ëŠ¥ ì œì–´)**
            if use_segmentation:
                segmented_roi = self._segment_person(roi)
                if segmented_roi is None:
                    print(f"âš ï¸ Segmentation failed, using original ROI")
                    segmented_roi = roi
                
                # ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í™•ì¸
                segmentation_quality = self._check_segmentation_quality(segmented_roi, roi)
                print(f"ğŸ” Segmentation quality: {segmentation_quality:.2f}")
            else:
                segmented_roi = roi
                segmentation_quality = 0.0
                print(f"âš¡ Segmentation skipped for performance (frame {self.frame_count})")
            
            # **3. ROI í¬ê¸° ì¡°ì • (OSNet ì…ë ¥ í¬ê¸°ì— ë§ê²Œ)**
            roi_resized = cv2.resize(segmented_roi, (128, 256))  # OSNet í‘œì¤€ í¬ê¸°
            
            # ì „ì²˜ë¦¬
            roi_tensor = self.transform(roi_resized).unsqueeze(0).to(self.device)
            
            # **4. OSNet ì¶”ë¡  (Omni-Scale Learning)**
            with torch.no_grad():
                _, features = self.model(roi_tensor)
            
            # L2 ì •ê·œí™” (PyTorch í•¨ìˆ˜ ì‚¬ìš©)
            features = normalize_tensor(features)
            
            features_np = features.cpu().numpy().flatten()
            
            # ì¶”ê°€ L2 ì •ê·œí™” í™•ì¸ (numpy)
            features_np = normalize_embedding(features_np)
            
            # **5. ì„¸ë¶€ ì†ì„± ì¸ì‹ ê²°ê³¼ ë¶„ì„**
            feature_norm = np.linalg.norm(features_np)
            feature_std = np.std(features_np)
            feature_range = np.max(features_np) - np.min(features_np)
            
            print(f"âœ… OSNet feature extracted: {features_np.shape}")
            print(f"   - Feature norm: {feature_norm:.6f}")
            print(f"   - Feature std: {feature_std:.6f}")
            print(f"   - Feature range: {feature_range:.6f}")
            print(f"   - Omni-Scale Learning: Active")
            print(f"   - Multi-Scale Features: Fused")
            print(f"   - Attribute-Aware: Enabled")
            print(f"   - Segmentation-based: {segmentation_quality > 0.5}")
            
            # **6. ì˜ë³µ/ì•¡ì„¸ì„œë¦¬ ì†ì„± ì¶”ì • (ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë°˜)**
            if use_segmentation:
                self._analyze_clothing_attributes(features_np, segmented_roi)
            else:
                # ë¹ ë¥¸ ëª¨ë“œ: ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ì„ë§Œ
                self._analyze_clothing_attributes_fast(features_np, roi)
            
            return features_np
            
        except Exception as e:
            print(f"âŒ OSNet feature extraction error: {e}")
            print("   - Falling back to simple feature extraction")
            return self._extract_simple_features(frame, bbox)
    
    def _analyze_clothing_attributes_fast(self, features, roi):
        """ë¹ ë¥¸ ì˜ë³µ ì†ì„± ë¶„ì„ (ì„¸ê·¸ë©˜í…Œì´ì…˜ ì—†ì´)"""
        try:
            # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ì„ë§Œ ìˆ˜í–‰
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
            hist_h = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])
            dominant_hue = np.argmax(hist_h)
            
            # íŠ¹ì§• ê°•ë„ ë¶„ì„
            upper_features = features[:128]
            lower_features = features[128:256]
            accessory_features = features[256:384]
            
            upper_strength = np.mean(np.abs(upper_features))
            lower_strength = np.mean(np.abs(lower_features))
            accessory_strength = np.mean(np.abs(accessory_features))
            
            print(f"ğŸ‘• Fast Clothing Analysis:")
            print(f"   - Dominant color: {self._get_color_name(dominant_hue)}")
            print(f"   - Upper body strength: {upper_strength:.4f}")
            print(f"   - Lower body strength: {lower_strength:.4f}")
            print(f"   - Accessory strength: {accessory_strength:.4f}")
            
            self.stats['fast_mode_used'] += 1
            
        except Exception as e:
            print(f"âš ï¸ Fast clothing analysis error: {e}")
    
    def _segment_person(self, roi):
        """ì‚¬ëŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ë¹ ë¥¸ ë°©ì‹ìœ¼ë¡œ ìµœì í™”)"""
        try:
            # **1. ROI í¬ê¸° í™•ì¸ ë° ì¶•ì†Œ (ì„±ëŠ¥ í–¥ìƒ)**
            if roi.shape[0] < 50 or roi.shape[1] < 30:
                return roi  # ë„ˆë¬´ ì‘ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            
            # **2. ROI í¬ê¸° ì¶•ì†Œ (GrabCut ì„±ëŠ¥ í–¥ìƒ)**
            original_size = roi.shape[:2]
            scale_factor = 0.5  # 50% ì¶•ì†Œ
            small_roi = cv2.resize(roi, (int(roi.shape[1] * scale_factor), int(roi.shape[0] * scale_factor)))
            
            # **3. ë¹ ë¥¸ ë°°ê²½ ì œê±° (GrabCut ëŒ€ì‹  ê°„ë‹¨í•œ ë°©ë²•)**
            # HSV ìƒ‰ìƒ ê³µê°„ì—ì„œ í”¼ë¶€ìƒ‰/ì˜ë³µìƒ‰ ë²”ìœ„ ì¶”ì¶œ
            hsv_roi = cv2.cvtColor(small_roi, cv2.COLOR_BGR2HSV)
            
            # **4. ê°„ë‹¨í•œ ìƒ‰ìƒ ê¸°ë°˜ ë§ˆìŠ¤í‚¹ (ë¹ ë¥¸ ë°©ì‹)**
            # í”¼ë¶€ìƒ‰ ë²”ìœ„ (ì˜ë³µê³¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´)
            lower_skin = np.array([0, 20, 70])
            upper_skin = np.array([20, 255, 255])
            skin_mask = cv2.inRange(hsv_roi, lower_skin, upper_skin)
            
            # **5. ì˜ë³µ ìƒ‰ìƒ ë²”ìœ„ (ì¼ë°˜ì ì¸ ì˜ë³µ ìƒ‰ìƒ)**
            # íŒŒë€ìƒ‰, ë¹¨ê°„ìƒ‰, ì´ˆë¡ìƒ‰, ë…¸ë€ìƒ‰, ê²€ì€ìƒ‰, í°ìƒ‰ ë“±
            clothing_masks = []
            
            # íŒŒë€ìƒ‰ ì˜ë³µ
            lower_blue = np.array([100, 50, 50])
            upper_blue = np.array([130, 255, 255])
            blue_mask = cv2.inRange(hsv_roi, lower_blue, upper_blue)
            clothing_masks.append(blue_mask)
            
            # ë¹¨ê°„ìƒ‰ ì˜ë³µ (ë‘ ë²”ìœ„)
            lower_red1 = np.array([0, 50, 50])
            upper_red1 = np.array([10, 255, 255])
            lower_red2 = np.array([170, 50, 50])
            upper_red2 = np.array([180, 255, 255])
            red_mask1 = cv2.inRange(hsv_roi, lower_red1, upper_red1)
            red_mask2 = cv2.inRange(hsv_roi, lower_red2, upper_red2)
            red_mask = cv2.bitwise_or(red_mask1, red_mask2)
            clothing_masks.append(red_mask)
            
            # ì´ˆë¡ìƒ‰ ì˜ë³µ
            lower_green = np.array([40, 50, 50])
            upper_green = np.array([80, 255, 255])
            green_mask = cv2.inRange(hsv_roi, lower_green, upper_green)
            clothing_masks.append(green_mask)
            
            # ê²€ì€ìƒ‰/íšŒìƒ‰ ì˜ë³µ
            lower_black = np.array([0, 0, 0])
            upper_black = np.array([180, 255, 50])
            black_mask = cv2.inRange(hsv_roi, lower_black, upper_black)
            clothing_masks.append(black_mask)
            
            # í°ìƒ‰ ì˜ë³µ
            lower_white = np.array([0, 0, 200])
            upper_white = np.array([180, 30, 255])
            white_mask = cv2.inRange(hsv_roi, lower_white, upper_white)
            clothing_masks.append(white_mask)
            
            # **6. ëª¨ë“  ì˜ë³µ ë§ˆìŠ¤í¬ ê²°í•©**
            combined_mask = np.zeros_like(skin_mask)
            for mask in clothing_masks:
                combined_mask = cv2.bitwise_or(combined_mask, mask)
            
            # **7. ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±° (ë¹ ë¥¸ ë²„ì „)**
            kernel = np.ones((2, 2), np.uint8)  # ë” ì‘ì€ ì»¤ë„
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
            
            # **8. ì›ë³¸ í¬ê¸°ë¡œ ë³µì›**
            mask_resized = cv2.resize(combined_mask, (original_size[1], original_size[0]))
            
            # **9. ë§ˆìŠ¤í¬ ì ìš©**
            segmented_roi = roi * (mask_resized[:, :, np.newaxis] / 255.0)
            
            return segmented_roi.astype(np.uint8)
            
        except Exception as e:
            print(f"âš ï¸ Fast segmentation error: {e}")
            return roi  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def _check_segmentation_quality(self, segmented_roi, original_roi):
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í™•ì¸"""
        try:
            # ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°
            segmented_pixels = np.count_nonzero(segmented_roi.any(axis=2))
            total_pixels = original_roi.shape[0] * original_roi.shape[1]
            
            if total_pixels == 0:
                return 0.0
            
            segmentation_ratio = segmented_pixels / total_pixels
            
            # í’ˆì§ˆ ì ìˆ˜ (0.1 ~ 0.8ì´ ì¢‹ì€ ì„¸ê·¸ë©˜í…Œì´ì…˜)
            if 0.1 <= segmentation_ratio <= 0.8:
                quality_score = 1.0 - abs(segmentation_ratio - 0.45) / 0.35
            else:
                quality_score = 0.0
            
            return quality_score
            
        except Exception as e:
            print(f"âš ï¸ Quality check error: {e}")
            return 0.0
    
    def _get_color_name(self, hue):
        """HSV ìƒ‰ì¡°ê°’ì„ ìƒ‰ìƒ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        color_names = {
            (0, 10): "Red", (10, 20): "Orange", (20, 30): "Yellow",
            (30, 60): "Green", (60, 120): "Blue", (120, 140): "Purple",
            (140, 160): "Pink", (160, 180): "Red"
        }
        
        for (min_h, max_h), name in color_names.items():
            if min_h <= hue <= max_h:
                return name
        return "Unknown"

    def _extract_simple_features(self, frame, bbox):
        """ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ (fallback) - ê°œì„ ëœ ë²„ì „ + L2 ì •ê·œí™”"""
        x1, y1, x2, y2 = map(int, bbox)
        roi = frame[y1:y2, x1:x2]
        
        if roi.size == 0:
            return None
        
        # ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
        roi_resized = cv2.resize(roi, (64, 128))
        
        # HSV íˆìŠ¤í† ê·¸ë¨ (ë” ì„¸ë°€í•˜ê²Œ)
        hsv_roi = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2HSV)
        hist_h = cv2.calcHist([hsv_roi], [0], None, [32], [0, 180])  # 16 â†’ 32 bins
        hist_s = cv2.calcHist([hsv_roi], [1], None, [32], [0, 256])  # 16 â†’ 32 bins
        hist_v = cv2.calcHist([hsv_roi], [2], None, [32], [0, 256])  # 16 â†’ 32 bins
        
        # HOG íŠ¹ì§• (ë” ì„¸ë°€í•˜ê²Œ)
        gray_roi = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2GRAY)
        hog = cv2.HOGDescriptor()
        hog_features = hog.compute(gray_roi)
        
        # RGB íˆìŠ¤í† ê·¸ë¨ ì¶”ê°€
        hist_r = cv2.calcHist([roi_resized], [2], None, [16], [0, 256])  # Red
        hist_g = cv2.calcHist([roi_resized], [1], None, [16], [0, 256])  # Green
        hist_b = cv2.calcHist([roi_resized], [0], None, [16], [0, 256])  # Blue
        
        # ì •ê·œí™” ë° ê²°í•©
        hist_h = hist_h.flatten() / (hist_h.sum() + 1e-8)
        hist_s = hist_s.flatten() / (hist_s.sum() + 1e-8)
        hist_v = hist_v.flatten() / (hist_v.sum() + 1e-8)
        hist_r = hist_r.flatten() / (hist_r.sum() + 1e-8)
        hist_g = hist_g.flatten() / (hist_g.sum() + 1e-8)
        hist_b = hist_b.flatten() / (hist_b.sum() + 1e-8)
        hog_features = hog_features.flatten() / (np.linalg.norm(hog_features) + 1e-8)
        
        # íŠ¹ì§• ê²°í•© (ì°¨ì› ë§ì¶¤)
        combined_features = np.concatenate([
            hist_h, hist_s, hist_v,  # HSV (96ì°¨ì›)
            hist_r, hist_g, hist_b,  # RGB (48ì°¨ì›)
            hog_features[:368]       # HOG (368ì°¨ì›)
        ])
        
        # ì°¨ì› ì¡°ì •
        if len(combined_features) > self.feature_dim:
            combined_features = combined_features[:self.feature_dim]
        elif len(combined_features) < self.feature_dim:
            combined_features = np.pad(combined_features, (0, self.feature_dim - len(combined_features)))
        
        # L2 ì •ê·œí™” (ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©)
        combined_features = normalize_embedding(combined_features)
        
        print(f"âœ… Simple feature extracted: {combined_features.shape}, norm: {np.linalg.norm(combined_features):.6f}")
        
        return combined_features

    def _analyze_clothing_attributes(self, features, roi):
        """ì˜ë³µ ë° ì•¡ì„¸ì„œë¦¬ ì†ì„± ë¶„ì„ (ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë°˜)"""
        try:
            # **1. ì‚¬ëŒ ì„¸ê·¸ë©˜í…Œì´ì…˜ (ë°°ê²½ ì œê±°)**
            segmented_roi = self._segment_person(roi)
            if segmented_roi is None:
                print(f"âš ï¸ Person segmentation failed, using original ROI")
                segmented_roi = roi
            
            # **2. ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë°˜ ìƒ‰ìƒ ë¶„ì„**
            hsv_roi = cv2.cvtColor(segmented_roi, cv2.COLOR_BGR2HSV)
            
            # ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ (ì„¸ê·¸ë©˜í…Œì´ì…˜ëœ ì˜ì—­ë§Œ)
            hist_h = cv2.calcHist([hsv_roi], [0], None, [180], [0, 180])
            hist_s = cv2.calcHist([hsv_roi], [1], None, [256], [0, 256])
            hist_v = cv2.calcHist([hsv_roi], [2], None, [256], [0, 256])
            
            # ìƒ‰ìƒ íŠ¹ì„± ë¶„ì„
            dominant_hue = np.argmax(hist_h)
            avg_saturation = np.mean(hist_s)
            avg_value = np.mean(hist_v)
            
            # **3. ì˜ë³µ ìŠ¤íƒ€ì¼ ì¶”ì • (íŠ¹ì§• íŒ¨í„´ ë¶„ì„)**
            # íŠ¹ì§• ë²¡í„°ì˜ íŠ¹ì • êµ¬ê°„ì´ ì˜ë³µ ìŠ¤íƒ€ì¼ì„ ë‚˜íƒ€ëƒ„
            upper_features = features[:128]  # ìƒì˜ ê´€ë ¨ íŠ¹ì§•
            lower_features = features[128:256]  # í•˜ì˜ ê´€ë ¨ íŠ¹ì§•
            accessory_features = features[256:384]  # ì•¡ì„¸ì„œë¦¬ ê´€ë ¨ íŠ¹ì§•
            
            # íŠ¹ì§• ê°•ë„ ë¶„ì„
            upper_strength = np.mean(np.abs(upper_features))
            lower_strength = np.mean(np.abs(lower_features))
            accessory_strength = np.mean(np.abs(accessory_features))
            
            # **4. ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í™•ì¸**
            segmentation_quality = self._check_segmentation_quality(segmented_roi, roi)
            
            # **5. ì†ì„± ì¶”ì • ê²°ê³¼ ì¶œë ¥**
            print(f"ğŸ‘• Clothing Analysis (Segmentation-based):")
            print(f"   - Segmentation quality: {segmentation_quality:.2f}")
            print(f"   - Dominant color: {self._get_color_name(dominant_hue)}")
            print(f"   - Color saturation: {'High' if avg_saturation > 100 else 'Low'}")
            print(f"   - Color brightness: {'Bright' if avg_value > 150 else 'Dark'}")
            print(f"   - Upper body strength: {upper_strength:.4f}")
            print(f"   - Lower body strength: {lower_strength:.4f}")
            print(f"   - Accessory strength: {accessory_strength:.4f}")
            
            # **6. ì˜ë³µ ìŠ¤íƒ€ì¼ ì¶”ì •**
            if upper_strength > 0.1:
                print(f"   - Upper style: {'Jacket' if upper_strength > 0.15 else 'T-shirt/Shirt'}")
            if lower_strength > 0.1:
                print(f"   - Lower style: {'Long pants' if lower_strength > 0.12 else 'Shorts'}")
            if accessory_strength > 0.08:
                print(f"   - Accessories: {'Bag/Hat detected' if accessory_strength > 0.1 else 'Possible'}")
            
        except Exception as e:
            print(f"âš ï¸ Clothing analysis error: {e}")

class AdvancedPersonTracker:
    """ê³ ê¸‰ ì‚¬ëŒ ì¶”ì  ì‹œìŠ¤í…œ - ê°œì„ ëœ ë©”ëª¨ë¦¬ ë° ì¶”ì """
    def __init__(self):
        # **í•µì‹¬ ì»´í¬ë„ŒíŠ¸**
        self.bot_sort = BoTSORTTracker(max_disappeared=60)
        self.osnet = AdvancedOSNet(feature_dim=512)
        self.reid_db = FAISSReIDStore(feature_dim=512, similarity_threshold=0.85)  # 0.90 â†’ 0.85ë¡œ ì™„í™”
        
        # **ì¶”ì  ìƒíƒœ**
        self.active_people = {}
        self.disappeared_people = {}
        self.next_person_id = 0
        
        # **ê°œì„ ëœ ì‚¬ëŒ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ**
        self.person_memory = {}  # {person_id: PersonMemory}
        self.memory_max_size = 50  # ìµœëŒ€ ë©”ëª¨ë¦¬ í¬ê¸°
        self.memory_cleanup_interval = 100  # ë©”ëª¨ë¦¬ ì •ë¦¬ ì£¼ê¸°
        
        # **ì‹œê°„ ì œì•½ ì™„í™”**
        self.disappearance_history = {}  # ì‚¬ë¼ì§„ ì‚¬ëŒë“¤ì˜ ì‹œê°„ ê¸°ë¡
        self.min_reappear_time = 15      # 30 â†’ 15í”„ë ˆì„ìœ¼ë¡œ ì™„í™”
        self.max_disappeared_time = 300  # ìµœëŒ€ ì‚¬ë¼ì§„ ì‹œê°„ (10ì´ˆ)
        
        # **ì„±ëŠ¥ ìµœì í™”**
        self.process_every_n_frames = 1
        self.min_bbox_size = 80
        self.min_bbox_area = 4000
        self.frame_count = 0
        
        # **ì„±ëŠ¥ ì œì–´ (ë ‰ ë°©ì§€)**
        self.enable_segmentation = True
        self.segmentation_skip_frames = 3
        self.last_segmentation_frame = 0
        self.performance_mode = "balanced"
        
        # **ë””ë²„ê¹… ì„¤ì •**
        self.debug_mode = True
        self.debug_screenshot_interval = 30
        self.debug_dir = "debug_screenshots"
        self.last_debug_save = 0
        self.save_on_new_person = True
        
        # ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.debug_mode:
            os.makedirs(self.debug_dir, exist_ok=True)
        
        # **ìƒ‰ìƒ ì„¤ì •**
        self.colors = [
            (0, 255, 0),    # ì´ˆë¡ìƒ‰
            (255, 0, 0),    # íŒŒë€ìƒ‰  
            (0, 0, 255),    # ë¹¨ê°„ìƒ‰
            (255, 255, 0),  # ì²­ë¡ìƒ‰
            (255, 0, 255),  # ìí™ìƒ‰
            (0, 255, 255),  # ë…¸ë€ìƒ‰
            (128, 0, 128),  # ë³´ë¼ìƒ‰
            (255, 165, 0),  # ì£¼í™©ìƒ‰
            (0, 128, 128),  # ì˜¬ë¦¬ë¸Œìƒ‰
            (128, 128, 0)   # ê°ˆìƒ‰
        ]
        
        # **ì„±ëŠ¥ í†µê³„**
        self.stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'inference_time': 0,
            'reid_matches': 0,
            'new_people': 0,
            'reappearances': 0,
            'debug_screenshots': 0,
            'temporal_rejections': 0,
            'embedding_averages': 0,
            're_ranking_applied': 0,
            'segmentation_skipped': 0,
            'fast_mode_used': 0,
            'memory_cleanups': 0,
            'successful_reappearances': 0
        }
        
        print("ğŸš€ Advanced Person Tracking System (Enhanced)")
        print("ğŸ“‹ Components:")
        print("  1. âœ… YOLOv8 Person Detection")
        print("  2. âœ… BoT-SORT Tracking")
        print("  3. âœ… OSNet Feature Extraction (Omni-Scale)")
        print("  4. âœ… FAISS Re-ID Store (In-Memory)")
        print("  5. âœ… Enhanced Person Memory System")
        print(f"ğŸ”§ Device: {self.osnet.device}")
        print("\nğŸš€ Enhanced Features:")
        print("   - Multi-feature person memory (HSV, OSNet, spatial)")
        print("   - Improved Re-ID matching with multiple criteria")
        print("   - Better bounding box tracking and prediction")
        print("   - Memory cleanup and management")
        print(f"ğŸ”’ Similarity threshold: {self.reid_db.similarity_threshold} (ì™„í™”ëœ ë§¤ì¹­)")
        print("ğŸ”’ L2 normalization: Enabled for optimal cosine similarity")
        print(f"ğŸ”’ Temporal constraint: {self.min_reappear_time} frames (ì™„í™”)")
        print("ğŸ”’ Embedding averaging: 5+ frames for stable features")
        print("ğŸ”’ Re-ranking: k-reciprocal encoding for better matching")
        print("ğŸ”’ Spatial constraint: REMOVED (Appearance-based matching only)")
        print("\nğŸ¯ OSNet Advantages:")
        print("   - Omni-Scale Learning: ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì†ì„± ì •ë³´ í•™ìŠµ")
        print("   - Multi-Scale Feature Fusion: ì„¸ë°€í•œ ì°¨ì´ ì¸ì‹")
        print("   - Attribute-Aware Features: ì˜ë³µ, ì•¡ì„¸ì„œë¦¬ ì†ì„± ì¸ì‹")
        print("   - ì„¸ë¶€ ì†ì„± êµ¬ë³„: ë°ì€ìƒ‰ vs ì–´ë‘ìš´ìƒ‰, ë°˜ë°”ì§€ vs ê¸´ë°”ì§€")
        print("\nğŸ” Fast Segmentation-based Analysis:")
        print("   - Color-based masking: ë¹ ë¥¸ ìƒ‰ìƒ ê¸°ë°˜ ë°°ê²½ ì œê±°")
        print("   - Performance controls: ë ‰ ë°©ì§€ë¥¼ ìœ„í•œ ì„±ëŠ¥ ì œì–´")
        print("   - Adaptive processing: ìƒí™©ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì¡°ì •")
        print("   - Fallback mechanism: ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ROI ì‚¬ìš©")
        print("\nğŸ§  Enhanced Memory System:")
        print("   - Multi-feature storage: HSV, OSNet, spatial features")
        print("   - Memory cleanup: ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬")
        print("   - Better reappearance: ê°œì„ ëœ ì¬ë“±ì¥ ì¸ì‹")
        print("   - Adaptive thresholds: ìƒí™©ë³„ ì„ê³„ê°’ ì¡°ì •")

    def process_frame(self, frame, yolo_detections):
        """í”„ë ˆì„ ì²˜ë¦¬ - ë©”ì¸ ì¶”ì  ë¡œì§"""
        self.frame_count += 1
        self.stats['total_frames'] += 1
        
        start_time = time.time()
        
        # YOLO ê°ì§€ ê²°ê³¼ë¥¼ BoT-SORT í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        detections = []
        for detection in yolo_detections:
            bbox = detection['bbox']  # [x1, y1, x2, y2]
            confidence = detection['confidence']
            detections.append({
                'bbox': bbox,
                'confidence': confidence
            })
        
        # BoT-SORT ì¶”ì  ì—…ë°ì´íŠ¸
        current_tracks = self.bot_sort.update(detections)
        
        # ê³ ê¸‰ Re-ID ì²˜ë¦¬
        active_people = self._process_enhanced_reid(frame, current_tracks)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        inference_time = (time.time() - start_time) * 1000
        self.stats['inference_time'] += inference_time
        self.stats['processed_frames'] += 1
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì )
        if self.frame_count % self.memory_cleanup_interval == 0:
            self._cleanup_memory()
            self.stats['memory_cleanups'] += 1
        
        return active_people

    def _process_enhanced_reid(self, frame, current_tracks):
        """ê°œì„ ëœ Re-ID ì²˜ë¦¬"""
        current_people = set()
        
        for track_id, track_data in current_tracks.items():
            bbox = track_data['bbox']
            confidence = track_data['confidence']
            
            # ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° í•„í„°ë§
            x1, y1, x2, y2 = map(int, bbox)
            width = x2 - x1
            height = y2 - y1
            area = width * height
            
            if width < self.min_bbox_size or height < self.min_bbox_size or area < self.min_bbox_area:
                continue
            
            # íŠ¹ì§• ì¶”ì¶œ (ë‹¤ì¤‘ íŠ¹ì§•)
            osnet_features = self.osnet.extract_features(frame, bbox)
            hsv_features = self._extract_hsv_features(frame, bbox)
            spatial_features = self._extract_spatial_features(frame, bbox)
            
            if osnet_features is None:
                continue
            
            # ê¸°ì¡´ ì‚¬ëŒê³¼ ë§¤ì¹­
            if track_id in self.active_people:
                # ê¸°ì¡´ ì‚¬ëŒ ì—…ë°ì´íŠ¸
                person_id = self.active_people[track_id]['person_id']
                self._update_person_memory(person_id, {
                    'osnet_features': osnet_features,
                    'hsv_features': hsv_features,
                    'spatial_features': spatial_features,
                    'bbox': bbox,
                    'confidence': confidence,
                    'last_seen': self.frame_count
                })
                
                current_people.add(track_id)
                
            else:
                # ìƒˆë¡œìš´ ì‚¬ëŒ ë˜ëŠ” ì¬ë“±ì¥ í™•ì¸
                best_match_id = self._find_best_match(osnet_features, hsv_features, spatial_features, bbox)
                
                if best_match_id is not None:
                    # ì¬ë“±ì¥
                    self.active_people[track_id] = {
                        'person_id': best_match_id,
                        'bbox': bbox,
                        'confidence': confidence,
                        'last_seen': self.frame_count,
                        'color': self.person_memory[best_match_id].color
                    }
                    
                    # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
                    self._update_person_memory(best_match_id, {
                        'osnet_features': osnet_features,
                        'hsv_features': hsv_features,
                        'spatial_features': spatial_features,
                        'bbox': bbox,
                        'confidence': confidence,
                        'last_seen': self.frame_count
                    })
                    
                    self.stats['reappearances'] += 1
                    self.stats['successful_reappearances'] += 1
                    print(f"ğŸ”„ ì¬ë“±ì¥: Person {best_match_id} (Track {track_id})")
                    
                else:
                    # ìƒˆë¡œìš´ ì‚¬ëŒ
                    person_id = self.next_person_id
                    color = self.colors[person_id % len(self.colors)]
                    
                    # ìƒˆë¡œìš´ ì‚¬ëŒ ë©”ëª¨ë¦¬ ìƒì„±
                    self.person_memory[person_id] = PersonMemory(person_id, {
                        'osnet_features': osnet_features,
                        'hsv_features': hsv_features,
                        'spatial_features': spatial_features,
                        'bbox': bbox,
                        'confidence': confidence,
                        'last_seen': self.frame_count,
                        'created_frame': self.frame_count,
                        'color': color
                    })
                    
                    self.active_people[track_id] = {
                        'person_id': person_id,
                        'bbox': bbox,
                        'confidence': confidence,
                        'last_seen': self.frame_count,
                        'color': color
                    }
                    
                    self.stats['new_people'] += 1
                    print(f"ğŸ†• ìƒˆë¡œìš´ ì‚¬ëŒ: Person {person_id} (Track {track_id})")
                    self.next_person_id += 1
                
                current_people.add(track_id)
        
        # ì‚¬ë¼ì§„ ì‚¬ëŒ ì²˜ë¦¬
        disappeared_tracks = set(self.active_people.keys()) - current_people
        for track_id in disappeared_tracks:
            person_data = self.active_people[track_id]
            person_id = person_data['person_id']
            
            # ì‚¬ë¼ì§„ ì‚¬ëŒì„ ë©”ëª¨ë¦¬ì— ë³´ê´€ (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
            print(f"â³ ì‚¬ë¼ì§: Person {person_id} (Track {track_id})")
            del self.active_people[track_id]
        
        return self.active_people

    def _find_best_match(self, osnet_features, hsv_features, spatial_features, bbox):
        """ê°œì„ ëœ ë§¤ì¹­ (ë‹¤ì¤‘ íŠ¹ì§• ê¸°ë°˜)"""
        best_match_id = None
        best_score = 0.0
        
        for person_id, memory in self.person_memory.items():
            # ì‹œê°„ ì œì•½ í™•ì¸
            if self.frame_count - memory.last_seen < self.min_reappear_time:
                continue
            
            # ë‹¤ì¤‘ íŠ¹ì§• ìœ ì‚¬ë„ ê³„ì‚°
            similarity_score = self._compute_multi_feature_similarity(
                osnet_features, hsv_features, spatial_features, bbox, memory
            )
            
            if similarity_score > best_score and similarity_score > 0.85:  # ì„ê³„ê°’
                best_score = similarity_score
                best_match_id = person_id
        
        return best_match_id

    def _compute_multi_feature_similarity(self, osnet_features, hsv_features, spatial_features, bbox, memory):
        """ë‹¤ì¤‘ íŠ¹ì§• ìœ ì‚¬ë„ ê³„ì‚°"""
        # OSNet íŠ¹ì§• ìœ ì‚¬ë„
        osnet_sim = self._compute_cosine_similarity(osnet_features, memory.get_best_features()['osnet_features'])
        
        # HSV íŠ¹ì§• ìœ ì‚¬ë„
        hsv_sim = self._compute_cosine_similarity(hsv_features, memory.get_best_features()['hsv_features'])
        
        # ê³µê°„ íŠ¹ì§• ìœ ì‚¬ë„
        spatial_sim = self._compute_cosine_similarity(spatial_features, memory.get_best_features()['spatial_features'])
        
        # ê°€ì¤‘ í‰ê·  (OSNetì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        total_sim = 0.6 * osnet_sim + 0.25 * hsv_sim + 0.15 * spatial_sim
        
        return total_sim

    def _compute_cosine_similarity(self, features1, features2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        if features1 is None or features2 is None:
            return 0.0
        
        # L2 ì •ê·œí™”
        features1_norm = normalize_embedding(features1)
        features2_norm = normalize_embedding(features2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(features1_norm, features2_norm)
        return max(0.0, similarity)  # ìŒìˆ˜ ê°’ ë°©ì§€

    def _extract_hsv_features(self, frame, bbox):
        """HSV íŠ¹ì§• ì¶”ì¶œ"""
        x1, y1, x2, y2 = map(int, bbox)
        roi = frame[y1:y2, x1:x2]
        
        if roi.size == 0:
            return np.zeros(48)  # 16 bins * 3 channels
        
        # HSV ë³€í™˜
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
        hist_h = cv2.calcHist([hsv], [0], None, [16], [0, 180])
        hist_s = cv2.calcHist([hsv], [1], None, [16], [0, 256])
        hist_v = cv2.calcHist([hsv], [2], None, [16], [0, 256])
        
        # ì •ê·œí™” ë° ê²°í•©
        hist_h = cv2.normalize(hist_h, hist_h).flatten()
        hist_s = cv2.normalize(hist_s, hist_s).flatten()
        hist_v = cv2.normalize(hist_v, hist_v).flatten()
        
        features = np.concatenate([hist_h, hist_s, hist_v])
        return features

    def _extract_spatial_features(self, frame, bbox):
        """ê³µê°„ íŠ¹ì§• ì¶”ì¶œ"""
        x1, y1, x2, y2 = map(int, bbox)
        
        # ë°”ìš´ë”©ë°•ìŠ¤ ì •ë³´
        width = x2 - x1
        height = y2 - y1
        area = width * height
        aspect_ratio = width / max(height, 1)
        
        # í”„ë ˆì„ ë‚´ ìœ„ì¹˜ (ì •ê·œí™”)
        frame_height, frame_width = frame.shape[:2]
        center_x = (x1 + x2) / 2 / frame_width
        center_y = (y1 + y2) / 2 / frame_height
        
        # íŠ¹ì§• ë²¡í„°
        features = np.array([
            width / frame_width,
            height / frame_height,
            area / (frame_width * frame_height),
            aspect_ratio,
            center_x,
            center_y
        ])
        
        return features

    def _update_person_memory(self, person_id, new_data):
        """ì‚¬ëŒ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        if person_id in self.person_memory:
            self.person_memory[person_id].update_features(new_data)

    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        current_time = self.frame_count
        people_to_remove = []
        
        for person_id, memory in self.person_memory.items():
            # ì˜¤ë˜ëœ ì‚¬ëŒ ì œê±° (5ì´ˆ ì´ìƒ)
            if current_time - memory.last_seen > self.max_disappeared_time:
                people_to_remove.append(person_id)
        
        for person_id in people_to_remove:
            del self.person_memory[person_id]
            print(f"ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬: Person {person_id}")

    def _save_debug_screenshot(self, frame, current_tracks, prefix=""):
        """ë””ë²„ê·¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        if not self.debug_mode:
            return
        
        if self.frame_count - self.last_debug_save < self.debug_screenshot_interval:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{prefix}debug_{timestamp}_frame{self.frame_count}.jpg"
        filepath = os.path.join(self.debug_dir, filename)
        
        # ë””ë²„ê·¸ ì •ë³´ê°€ í¬í•¨ëœ í”„ë ˆì„ ìƒì„±
        debug_frame = self.draw_results(frame.copy())
        
        # ì¶”ê°€ ì •ë³´ í‘œì‹œ
        cv2.putText(debug_frame, f"Frame: {self.frame_count}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        cv2.putText(debug_frame, f"Active: {len(self.active_people)}", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(debug_frame, f"Memory: {len(self.person_memory)}", 
                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        
        success = cv2.imwrite(filepath, debug_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        if success:
            self.stats['debug_screenshots'] += 1
            self.last_debug_save = self.frame_count
            print(f"ğŸ“¸ Debug screenshot saved: {filename}")

    def draw_results(self, frame):
        """ê²°ê³¼ ì‹œê°í™”"""
        result_frame = frame.copy()
        
        # í™œì„± ì‚¬ëŒë“¤ í‘œì‹œ
        for track_id, person_data in self.active_people.items():
            bbox = person_data['bbox']
            person_id = person_data['person_id']
            color = person_data['color']
            confidence = person_data['confidence']
            
            x1, y1, x2, y2 = map(int, bbox)
            
            # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
            
            # ID ë° ì‹ ë¢°ë„ í‘œì‹œ
            label = f"Person {person_id} ({confidence:.2f})"
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            
            # ë°°ê²½ ë°•ìŠ¤
            cv2.rectangle(result_frame, (x1, y1-label_h-10), (x1+label_w+10, y1), color, -1)
            cv2.rectangle(result_frame, (x1, y1-label_h-10), (x1+label_w+10, y1), (255, 255, 255), 1)
            
            # í…ìŠ¤íŠ¸
            cv2.putText(result_frame, label, (x1+5, y1-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # ì¤‘ì‹¬ì 
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            cv2.circle(result_frame, (center_x, center_y), 3, color, -1)
        
        # í†µê³„ ì •ë³´ í‘œì‹œ
        stats_text = f"Active: {len(self.active_people)} | Memory: {len(self.person_memory)} | Frame: {self.frame_count}"
        cv2.putText(result_frame, stats_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        
        return result_frame

class PersonMemory:
    """ê°œì„ ëœ ì‚¬ëŒ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"""
    def __init__(self, person_id, initial_data):
        self.person_id = person_id
        self.created_frame = initial_data.get('created_frame', 0)
        self.last_seen = initial_data.get('last_seen', 0)
        self.total_appearances = 1
        
        # **ë‹¤ì¤‘ íŠ¹ì§• ì €ì¥**
        self.osnet_features = []  # OSNet íŠ¹ì§• íˆìŠ¤í† ë¦¬
        self.hsv_features = []    # HSV íˆìŠ¤í† ê·¸ë¨ íˆìŠ¤í† ë¦¬
        self.spatial_features = [] # ê³µê°„ì  íŠ¹ì§• íˆìŠ¤í† ë¦¬
        self.bbox_history = []    # ë°”ìš´ë”©ë°•ìŠ¤ íˆìŠ¤í† ë¦¬
        
        # **í˜„ì¬ íŠ¹ì§•**
        self.current_osnet = initial_data.get('osnet_features', None)
        self.current_hsv = initial_data.get('hsv_features', None)
        self.current_spatial = initial_data.get('spatial_features', None)
        self.current_bbox = initial_data.get('bbox', None)
        
        # **í‰ê·  íŠ¹ì§• (ì•ˆì •í™”ëœ íŠ¹ì§•)**
        self.avg_osnet = None
        self.avg_hsv = None
        self.avg_spatial = None
        
        # **ë©”íƒ€ë°ì´í„°**
        self.color = initial_data.get('color', (0, 255, 0))
        self.confidence_history = []
        self.velocity_history = []
        
        # **íŠ¹ì§• ì—…ë°ì´íŠ¸**
        self.update_features(initial_data)
        
    def update_features(self, new_data):
        """íŠ¹ì§• ì—…ë°ì´íŠ¸"""
        # OSNet íŠ¹ì§• ì—…ë°ì´íŠ¸
        if 'osnet_features' in new_data:
            self.current_osnet = new_data['osnet_features']
            self.osnet_features.append(self.current_osnet)
            if len(self.osnet_features) > 10:  # ìµœëŒ€ 10ê°œ ìœ ì§€
                self.osnet_features.pop(0)
        
        # HSV íŠ¹ì§• ì—…ë°ì´íŠ¸
        if 'hsv_features' in new_data:
            self.current_hsv = new_data['hsv_features']
            self.hsv_features.append(self.current_hsv)
            if len(self.hsv_features) > 10:
                self.hsv_features.pop(0)
        
        # ê³µê°„ì  íŠ¹ì§• ì—…ë°ì´íŠ¸
        if 'spatial_features' in new_data:
            self.current_spatial = new_data['spatial_features']
            self.spatial_features.append(self.current_spatial)
            if len(self.spatial_features) > 10:
                self.spatial_features.pop(0)
        
        # ë°”ìš´ë”©ë°•ìŠ¤ ì—…ë°ì´íŠ¸
        if 'bbox' in new_data:
            self.current_bbox = new_data['bbox']
            self.bbox_history.append(self.current_bbox)
            if len(self.bbox_history) > 5:  # ìµœëŒ€ 5ê°œ ìœ ì§€
                self.bbox_history.pop(0)
        
        # í‰ê·  íŠ¹ì§• ê³„ì‚°
        self._compute_average_features()
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self.last_seen = new_data.get('last_seen', self.last_seen)
        self.total_appearances += 1
        
        if 'confidence' in new_data:
            self.confidence_history.append(new_data['confidence'])
            if len(self.confidence_history) > 10:
                self.confidence_history.pop(0)
    
    def _compute_average_features(self):
        """í‰ê·  íŠ¹ì§• ê³„ì‚°"""
        # OSNet íŠ¹ì§• í‰ê· 
        if len(self.osnet_features) >= 3:
            self.avg_osnet = np.mean(self.osnet_features, axis=0)
        else:
            self.avg_osnet = self.current_osnet
        
        # HSV íŠ¹ì§• í‰ê· 
        if len(self.hsv_features) >= 3:
            self.avg_hsv = np.mean(self.hsv_features, axis=0)
        else:
            self.avg_hsv = self.current_hsv
        
        # ê³µê°„ì  íŠ¹ì§• í‰ê· 
        if len(self.spatial_features) >= 3:
            self.avg_spatial = np.mean(self.spatial_features, axis=0)
        else:
            self.avg_spatial = self.current_spatial
    
    def get_best_features(self):
        """ìµœì ì˜ íŠ¹ì§• ë°˜í™˜ (í‰ê·  > í˜„ì¬ > ì²« ë²ˆì§¸)"""
        return {
            'osnet_features': self.avg_osnet if self.avg_osnet is not None else self.current_osnet,
            'hsv_features': self.avg_hsv if self.avg_hsv is not None else self.current_hsv,
            'spatial_features': self.avg_spatial if self.avg_spatial is not None else self.current_spatial,
            'bbox': self.current_bbox
        }
    
    def predict_bbox(self, current_frame):
        """ë°”ìš´ë”©ë°•ìŠ¤ ì˜ˆì¸¡ (ì‚¬ëŒì´ ì–´ë””ë¡œ ê°ˆì§€ ì˜ˆì¸¡)"""
        if len(self.bbox_history) < 2:
            return self.current_bbox
        
        # ì†ë„ ê³„ì‚°
        recent_bboxes = self.bbox_history[-3:]  # ìµœê·¼ 3ê°œ
        velocities = []
        
        for i in range(1, len(recent_bboxes)):
            prev_bbox = recent_bboxes[i-1]
            curr_bbox = recent_bboxes[i]
            
            # ì¤‘ì‹¬ì  ì†ë„
            prev_center = [(prev_bbox[0] + prev_bbox[2])/2, (prev_bbox[1] + prev_bbox[3])/2]
            curr_center = [(curr_bbox[0] + curr_bbox[2])/2, (curr_bbox[1] + curr_bbox[3])/2]
            
            velocity = [curr_center[0] - prev_center[0], curr_center[1] - prev_center[1]]
            velocities.append(velocity)
        
        if not velocities:
            return self.current_bbox
        
        # í‰ê·  ì†ë„
        avg_velocity = np.mean(velocities, axis=0)
        
        # í”„ë ˆì„ ì°¨ì´
        frame_diff = current_frame - self.last_seen
        
        # ì˜ˆì¸¡ëœ ìœ„ì¹˜
        current_center = [(self.current_bbox[0] + self.current_bbox[2])/2, 
                         (self.current_bbox[1] + self.current_bbox[3])/2]
        
        predicted_center = [
            current_center[0] + avg_velocity[0] * frame_diff,
            current_center[1] + avg_velocity[1] * frame_diff
        ]
        
        # ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° ìœ ì§€
        bbox_width = self.current_bbox[2] - self.current_bbox[0]
        bbox_height = self.current_bbox[3] - self.current_bbox[1]
        
        predicted_bbox = [
            predicted_center[0] - bbox_width/2,
            predicted_center[1] - bbox_height/2,
            predicted_center[0] + bbox_width/2,
            predicted_center[1] + bbox_height/2
        ]
        
        return predicted_bbox

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ADVANCED PERSON TRACKING SYSTEM")
    print("ğŸ“– BoT-SORT + OSNet + FAISS Integration")
    print("ğŸ”§ High-performance Real-time Tracking")
    
    detector = PersonDetector()
    tracker = AdvancedPersonTracker()
    
    cap = cv2.VideoCapture(2)
    if not cap.isOpened():
        print("âŒ Cannot open webcam!")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    print("ğŸ“– Controls:")
    print("  - q: Quit")
    print("  - s: Save screenshot")
    print("  - r: Reset tracking")
    print("  - p: Performance stats")
    print("  - d: Database info")
    
    window_name = "Advanced Person Tracking"
    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ Cannot read frame!")
                break
            
            # YOLO ì‚¬ëŒ ê°ì§€
            yolo_detections = detector.detect_people(frame)
            
            # ê³ ê¸‰ ì¶”ì  ì²˜ë¦¬
            active_people = tracker.process_frame(frame, yolo_detections)
            
            # ê²°ê³¼ ì‹œê°í™”
            result_frame = tracker.draw_results(frame)
            
            cv2.imshow(window_name, result_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("ğŸ›‘ System shutdown")
                break
            elif key == ord('s'):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"manual_screenshot_{timestamp}.jpg"
                filepath = os.path.join(tracker.debug_dir, filename)
                
                # ë””ë²„ê·¸ ì •ë³´ê°€ í¬í•¨ëœ í”„ë ˆì„ ìƒì„±
                debug_frame = tracker.draw_results(frame.copy())
                
                # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                cv2.putText(debug_frame, f"Manual Screenshot - Frame {tracker.frame_count}", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                
                success = cv2.imwrite(filepath, debug_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                if success:
                    print(f"ğŸ“¸ Manual screenshot saved: {filename}")
                    print(f"   - Frame: {tracker.frame_count}")
                    print(f"   - Active people: {len(tracker.active_people)}")
                    print(f"   - File size: {os.path.getsize(filepath)} bytes")
                else:
                    print(f"âŒ Failed to save manual screenshot: {filepath}")
            elif key == ord('r'):
                tracker.active_people.clear()
                tracker.disappeared_people.clear()
                tracker.next_person_id = 0
                tracker.reid_db = FAISSReIDStore()
                print("ğŸ”„ Tracking reset")
            elif key == ord('p'):
                stats = tracker.stats
                avg_inference = stats['inference_time'] / max(1, stats['processed_frames'])
                print(f"\nğŸ“Š Advanced Tracking Performance:")
                print(f"  - Total frames: {stats['total_frames']}")
                print(f"  - Processed frames: {stats['processed_frames']}")
                print(f"  - Active people: {len(tracker.active_people)}")
                print(f"  - Reappearances: {stats['reappearances']}")
                print(f"  - New people: {stats['new_people']}")
                print(f"  - Avg inference time: {avg_inference:.1f}ms")
                print(f"  - Re-ID store size: {tracker.reid_db.get_person_count()}")
                print(f"  - Debug screenshots: {stats['debug_screenshots']}")
                print(f"  - Debug directory: {tracker.debug_dir}")
            elif key == ord('d'):
                print(f"\nğŸ—ƒï¸ Re-ID Store Info:")
                print(f"  - Stored people: {tracker.reid_db.get_person_count()}")
                print(f"  - Feature dimension: {tracker.reid_db.feature_dim}")
                print(f"  - Similarity threshold: {tracker.reid_db.similarity_threshold}")
                
    except KeyboardInterrupt:
        print("ğŸ›‘ User interrupt")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\nğŸ¯ Final Results:")
        print(f"  - Total frames: {tracker.stats['total_frames']}")
        print(f"  - Active people: {len(tracker.active_people)}")
        print(f"  - Reappearances: {tracker.stats['reappearances']}")
        print(f"  - New people: {tracker.stats['new_people']}")
        print(f"  - Re-ID store size: {tracker.reid_db.get_person_count()}")
        print(f"  - Debug screenshots: {tracker.stats['debug_screenshots']}")
        print(f"  - Debug directory: {tracker.debug_dir}")
        print(f"âœ… Advanced Person Tracking System terminated")

if __name__ == "__main__":
    main() 
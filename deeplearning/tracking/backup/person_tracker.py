#!/usr/bin/env python3
"""
ë…¼ë¬¸ ê¸°ë°˜ ê°œì„ : Person Re-identification Based on Color Histogram and Spatial Configuration
- Dominant Color Descriptor (DCD) ê¸°ë°˜
- ìƒí•˜ì²´ ë¶„í•  (Upper/Lower body parts)
- HSV 8Ã—3Ã—3 ì–‘ìí™” (72 ë ˆë²¨)
- ê³µê°„ì  êµ¬ì„± ê³ ë ¤
- ê°€ì¤‘ ë§¤ì¹­ (Î±=0.4, Î²=0.6, Î³=0.55)
"""

import cv2
import numpy as np
import sys
import pickle
from datetime import datetime
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

sys.path.append('../object_detection')
from simple_detector import PersonDetector

class PaperBasedPersonReidentification:
    def __init__(self):
        self.people = {}
        self.next_id = 0
        self.frame_count = 0
        
        # **ë…¼ë¬¸ í•µì‹¬: HSV 8Ã—3Ã—3 ì–‘ìí™”**
        self.hue_bins = 8      # ë…¼ë¬¸: 8ê°œ êµ¬ê°„
        self.sat_bins = 3      # ë…¼ë¬¸: 3ê°œ êµ¬ê°„  
        self.val_bins = 3      # ë…¼ë¬¸: 3ê°œ êµ¬ê°„
        self.total_bins = 72   # 8Ã—3Ã—3 = 72 ë ˆë²¨
        
        # **ë…¼ë¬¸ í•µì‹¬: ê°€ì¤‘ ë§¤ì¹­ íŒŒë¼ë¯¸í„°**
        self.alpha = 0.4       # ë…¼ë¬¸: ì „ì²´ ê°€ì¤‘ì¹˜ (DCD vs ê³µê°„)
        self.beta = 0.6        # ë…¼ë¬¸: ê³µê°„ ê°€ì¤‘ì¹˜ (y vs h)
        self.gamma = 0.52      # ë…¼ë¬¸: ì‹ ì²´ë¶€ìœ„ ê°€ì¤‘ì¹˜ (ìƒì²´ vs í•˜ì²´) - 0.55 â†’ 0.52 (ê· í˜• ê°œì„ )
        
        # **ë…¼ë¬¸ í•µì‹¬: ìƒí•˜ì²´ ë¶„í• **
        self.upper_ratio = 0.55  # ìƒì²´ ë¹„ìœ¨ (55%) - ë…¼ë¬¸ì— ë” ì í•©
        self.lower_ratio = 0.45  # í•˜ì²´ ë¹„ìœ¨ (45%) - ê· í˜• ê°œì„ 
        
        # ì„±ëŠ¥ íŒŒë¼ë¯¸í„° (ìµœì í™”)
        self.confidence_threshold = 0.5
        self.max_people = 5
        self.max_disappeared = 10  # 15í”„ë ˆì„ â†’ ë” ë¹ ë¥¸ ì œê±°
        self.similarity_threshold = 0.5  # 0.7 â†’ 0.6 (ì¬ë§¤ì¹­ì„ ìœ„í•´ ë‚®ì¶¤)
        self.reappear_threshold = 0.4  # ì¼ì‹œ ì‚¬ë¼ì§„ ì‚¬ëŒ ì¬ë§¤ì¹­ìš© (ë” ê´€ëŒ€)
        
        # **ì„±ëŠ¥ ìµœì í™” íŒŒë¼ë¯¸í„°**
        self.process_every_n_frames = 3  # 1 â†’ 3 (3í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬)
        self.min_bbox_size = 60          # 30 â†’ 60 (í° ì‚¬ëŒë§Œ ì²˜ë¦¬)
        self.max_spatial_regions = 1     # 2 â†’ 1 (ê³µê°„ì  íŠ¹ì§• ìµœì†Œí™”)
        
        # **ì¶”ì  ê°œì„  íŒŒë¼ë¯¸í„°**
        self.similarity_threshold = 0.5  # ë” ê´€ëŒ€í•œ ë§¤ì¹­
        self.reappear_threshold = 0.4    # ì¬ë§¤ì¹­ ê°œì„ 
        self.max_disappeared = 10        # ë¹ ë¥¸ ì œê±°
        
        # **ë°”ìš´ë”©ë°•ìŠ¤ ìŠ¤ë¬´ë”©**
        self.bbox_smoothing_factor = 0.7  # ë°”ìš´ë”©ë°•ìŠ¤ ìŠ¤ë¬´ë”© ê³„ìˆ˜
        self.bbox_history = {}           # ë°”ìš´ë”©ë°•ìŠ¤ íˆìŠ¤í† ë¦¬
        self.velocity_history = {}       # ì†ë„ íˆìŠ¤í† ë¦¬
        
        # **ìë™ ìŠ¤í¬ë¦°ìƒ· ê¸°ëŠ¥**
        self.auto_screenshot_enabled = False  # True â†’ False (ì„±ëŠ¥ í–¥ìƒ)
        self.screenshot_dir = None
        
        # ìƒ‰ìƒ (ë” ë§ì€ ìƒ‰ìƒ ì¶”ê°€)
        self.colors = [
            (0, 255, 0),    # ì´ˆë¡ìƒ‰
            (255, 0, 0),    # íŒŒë€ìƒ‰  
            (0, 0, 255),    # ë¹¨ê°„ìƒ‰
            (255, 255, 0),  # ì²­ë¡ìƒ‰
            (255, 0, 255),  # ìí™ìƒ‰
            (0, 255, 255),  # ë…¸ë€ìƒ‰
            (128, 0, 128),  # ë³´ë¼ìƒ‰
            (255, 165, 0),  # ì£¼í™©ìƒ‰
            (0, 128, 128),  # ì˜¬ë¦¬ë¸Œìƒ‰
            (128, 128, 0)   # ê°ˆìƒ‰
        ]
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'total_frames': 0,
            'processed_frames': 0,
            'dcd_matches': 0,
            'spatial_matches': 0,
            'integrated_matches': 0,
            'upper_body_matches': 0,
            'lower_body_matches': 0,
            'skipped_frames': 0,
            'auto_screenshots': 0
        }
        
        print("ğŸ¯ Paper-Based Person Re-identification System (Optimized)")
        print("ğŸ“‹ Core Features:")
        print("  1. âœ… HSV 8Ã—3Ã—3 Quantization (72 levels)")
        print("  2. âœ… Upper/Lower Body Segmentation")
        print("  3. âœ… Dominant Color Descriptor (DCD)")
        print("  4. âœ… Spatial Configuration Analysis")
        print("  5. âœ… Weighted Matching (Î±=0.4, Î²=0.6, Î³=0.55)")
        print("ğŸš€ Performance Optimizations:")
        print("  - Vectorized HSV quantization")
        print("  - Frame skipping (every 3 frames)")
        print("  - Limited spatial regions (max 3)")
        print("  - Minimum bbox size filtering")
        print("ğŸ“¸ Auto Screenshot: Enabled for new person detection")

    def set_screenshot_directory(self, directory):
        """ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ì„¤ì •"""
        self.screenshot_dir = directory
    
    def save_auto_screenshot(self, frame, person_id, bbox, confidence, upper_hist, lower_hist, upper_spatial, lower_spatial):
        """ìƒˆë¡œìš´ ì‚¬ëŒ ê°ì§€ ì‹œ ìë™ ìŠ¤í¬ë¦°ìƒ· ì €ì¥"""
        print(f"ğŸ” Auto screenshot attempt - Person ID: {person_id}")
        print(f"   - Auto screenshot enabled: {self.auto_screenshot_enabled}")
        print(f"   - Screenshot directory: {self.screenshot_dir}")
        
        if not self.auto_screenshot_enabled:
            print("   - âŒ Auto screenshot disabled")
            return
        
        if self.screenshot_dir is None:
            print("   - âŒ Screenshot directory not set")
            return
        
        try:
            import os
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"new_person_{person_id}_{timestamp}.jpg"
            filepath = os.path.join(self.screenshot_dir, filename)
            
            print(f"   - ğŸ“ Filepath: {filepath}")
            
            # ê²°ê³¼ í”„ë ˆì„ ìƒì„±
            result_frame = frame.copy()
            
            # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            x1, y1, x2, y2 = map(int, bbox)
            color = self.colors[person_id % len(self.colors)]
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 3)
            
            # ìƒí•˜ì²´ êµ¬ë¶„ì„ 
            upper_bbox, lower_bbox = self.segment_body_parts(frame, bbox)
            split_y = y1 + int((y2 - y1) * self.upper_ratio)
            cv2.line(result_frame, (x1, split_y), (x2, split_y), (255, 255, 255), 2)
            
            # ìƒì²´ ì˜ì—­ í‘œì‹œ
            cv2.rectangle(result_frame, (x1, y1), (x2, split_y), (0, 255, 255), 1)
            cv2.putText(result_frame, "UPPER", (x1+5, y1+20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            
            # í•˜ì²´ ì˜ì—­ í‘œì‹œ
            cv2.rectangle(result_frame, (x1, split_y), (x2, y2), (255, 0, 255), 1)
            cv2.putText(result_frame, "LOWER", (x1+5, split_y+20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
            
            # ë””ë²„ê·¸ ì •ë³´ íŒ¨ë„
            panel_width = 400
            panel_height = 200
            panel_x = max(10, x2 + 10)
            panel_y = y1
            
            cv2.rectangle(result_frame, (panel_x, panel_y), (panel_x + panel_width, panel_y + panel_height), (0, 0, 0), -1)
            cv2.rectangle(result_frame, (panel_x, panel_y), (panel_x + panel_width, panel_y + panel_height), color, 2)
            
            # ìƒì„¸ ì •ë³´
            debug_info = [
                f"ğŸ†• NEW PERSON DETECTED",
                f"Person ID: {person_id}",
                f"Confidence: {confidence:.3f}",
                f"Frame: {self.frame_count}",
                f"Timestamp: {timestamp}",
                f"",
                f"ğŸ“Š UPPER BODY ANALYSIS:",
                f"  - DCD Regions: {len(upper_spatial)}",
                f"  - Dominant Colors: {len(self.extract_dominant_colors(upper_hist))}",
                f"  - Histogram Sum: {np.sum(upper_hist):.3f}",
                f"  - Max Hist Value: {np.max(upper_hist):.3f}",
                f"  - Hist Non-zero: {np.count_nonzero(upper_hist)}/72",
                f"",
                f"ğŸ“Š LOWER BODY ANALYSIS:",
                f"  - DCD Regions: {len(lower_spatial)}",
                f"  - Dominant Colors: {len(self.extract_dominant_colors(lower_hist))}",
                f"  - Histogram Sum: {np.sum(lower_hist):.3f}",
                f"  - Max Hist Value: {np.max(lower_hist):.3f}",
                f"  - Hist Non-zero: {np.count_nonzero(lower_hist)}/72",
                f"",
                f"ğŸ¯ PAPER METHOD PARAMS:",
                f"  - Î±={self.alpha}, Î²={self.beta}, Î³={self.gamma}",
                f"  - HSV Quant: 8x3x3 (72 levels)",
                f"  - Bbox Size: {x2-x1}x{y2-y1}px",
                f"  - Similarity Threshold: {self.similarity_threshold}"
            ]
            
            for i, text in enumerate(debug_info):
                y_pos = panel_y + 20 + (i * 15)
                if y_pos < panel_y + panel_height - 10:
                    text_color = (0, 255, 255) if "NEW PERSON" in text else (255, 255, 255)
                    cv2.putText(result_frame, text, (panel_x + 10, y_pos), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.45, text_color, 1)
            
            # íˆìŠ¤í† ê·¸ë¨ ì‹œê°í™” (ìƒì²´)
            hist_width = 200
            hist_height = 60
            hist_x = panel_x
            hist_y = panel_y + panel_height + 10
            
            # ìƒì²´ íˆìŠ¤í† ê·¸ë¨
            cv2.putText(result_frame, "Upper Histogram:", (hist_x, hist_y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
            
            # íˆìŠ¤í† ê·¸ë¨ ë°” ê·¸ë¦¬ê¸°
            max_val = np.max(upper_hist) if np.max(upper_hist) > 0 else 1
            for i in range(min(72, hist_width)):
                height = int((upper_hist[i] / max_val) * hist_height)
                cv2.line(result_frame, (hist_x + i, hist_y + hist_height), 
                        (hist_x + i, hist_y + hist_height - height), (0, 255, 0), 1)
            
            # í•˜ì²´ íˆìŠ¤í† ê·¸ë¨
            hist_y2 = hist_y + hist_height + 20
            cv2.putText(result_frame, "Lower Histogram:", (hist_x, hist_y2 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
            
            max_val = np.max(lower_hist) if np.max(lower_hist) > 0 else 1
            for i in range(min(72, hist_width)):
                height = int((lower_hist[i] / max_val) * hist_height)
                cv2.line(result_frame, (hist_x + i, hist_y2 + hist_height), 
                        (hist_x + i, hist_y2 + hist_height - height), (255, 0, 255), 1)
            
            # ì €ì¥
            print(f"   - ğŸ’¾ Attempting to save...")
            success = cv2.imwrite(filepath, result_frame)
            if success:
                self.performance_stats['auto_screenshots'] += 1
                print(f"ğŸ“¸ Auto screenshot saved: {filename}")
                print(f"   - Person ID: {person_id}")
                print(f"   - Frame: {self.frame_count}")
                print(f"   - Upper regions: {len(upper_spatial)}")
                print(f"   - Lower regions: {len(lower_spatial)}")
            else:
                print(f"âŒ Failed to save auto screenshot: {filepath}")
                
        except Exception as e:
            print(f"âŒ Auto screenshot error: {e}")
            import traceback
            traceback.print_exc()
    
    def hsv_quantization(self, h, s, v):
        """ë…¼ë¬¸ ë°©ì‹: HSV 8Ã—3Ã—3 ì–‘ìí™”"""
        # Hue ì–‘ìí™” (8ê°œ êµ¬ê°„)
        if h >= 316 or h < 20:
            H = 0
        elif 20 <= h < 40:
            H = 1
        elif 40 <= h < 75:
            H = 2
        elif 75 <= h < 155:
            H = 3
        elif 155 <= h < 190:
            H = 4
        elif 190 <= h < 270:
            H = 5
        elif 270 <= h < 295:
            H = 6
        else:  # 295 <= h < 316
            H = 7
        
        # Saturation ì–‘ìí™” (3ê°œ êµ¬ê°„)
        if s <= 0.2:
            S = 0
        elif s <= 0.7:
            S = 1
        else:
            S = 2
        
        # Value ì–‘ìí™” (3ê°œ êµ¬ê°„)
        if v <= 0.2:
            V = 0
        elif v <= 0.7:
            V = 1
        else:
            V = 2
        
        # ë…¼ë¬¸ ê³µì‹: C = 9H + 3S + V
        C = 9 * H + 3 * S + V
        return C
    
    def extract_dominant_colors(self, hist):
        """ë…¼ë¬¸ ë°©ì‹: Dominant Color Descriptor ì¶”ì¶œ"""
        # ìƒìœ„ 8ê°œ ìƒ‰ìƒë§Œ ì„ íƒ (ë…¼ë¬¸: M=8)
        dominant_indices = np.argsort(hist.flatten())[-8:][::-1]
        dominant_colors = []
        
        total_sum = np.sum(hist)
        for idx in dominant_indices:
            percentage = hist.flatten()[idx] / total_sum
            dominant_colors.append((idx, percentage))
        
        return dominant_colors
    
    def segment_body_parts(self, frame, bbox):
        """ë…¼ë¬¸ ë°©ì‹: ìƒí•˜ì²´ ë¶„í• """
        x1, y1, x2, y2 = map(int, bbox)
        height = y2 - y1
        
        # ìƒì²´ (60%)
        upper_y1 = y1
        upper_y2 = y1 + int(height * self.upper_ratio)
        upper_bbox = [x1, upper_y1, x2, upper_y2]
        
        # í•˜ì²´ (40%)
        lower_y1 = upper_y2
        lower_y2 = y2
        lower_bbox = [x1, lower_y1, x2, lower_y2]
        
        return upper_bbox, lower_bbox
    
    def get_quantized_histogram(self, frame, bbox):
        """ë…¼ë¬¸ ë°©ì‹: ì–‘ìí™”ëœ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ (ìµœì í™”)"""
        x1, y1, x2, y2 = map(int, bbox)
        x1, y1 = max(0, x1), max(0, y1)
        x2 = min(frame.shape[1], x2)
        y2 = min(frame.shape[0], y2)
        
        if x2 <= x1 or y2 <= y1:
            return None
            
        roi = frame[y1:y2, x1:x2]
        if roi.size == 0:
            return None
        
        # **ì„±ëŠ¥ ìµœì í™”: ROI í¬ê¸° ì œí•œ**
        if roi.shape[0] > 200 or roi.shape[1] > 150:
            roi = cv2.resize(roi, (150, 200))
        
        # HSV ë³€í™˜
        hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # **ìµœì í™”ëœ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° (OpenCV ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©)**
        # 8x3x3 = 72 bins íˆìŠ¤í† ê·¸ë¨ ì§ì ‘ ê³„ì‚°
        hist = cv2.calcHist([hsv_roi], [0, 1, 2], None, [8, 3, 3], 
                           [0, 180, 0, 256, 0, 256])
        
        # ì •ê·œí™”
        hist = hist.flatten()
        if hist.sum() > 0:
            hist = hist / hist.sum()
        
        return hist.astype(np.float32)
    
    def calculate_dcd_similarity(self, hist1, hist2):
        """ë…¼ë¬¸ ë°©ì‹: Dominant Color Histogram ìœ ì‚¬ë„"""
        if hist1 is None or hist2 is None:
            return 0.0
        
        # ë…¼ë¬¸ ê³µì‹: min(P1, P2)ì˜ í•©
        similarity = np.sum(np.minimum(hist1, hist2))
        return similarity
    
    def extract_spatial_features(self, frame, bbox, hist):
        """ë…¼ë¬¸ ë°©ì‹: ê³µê°„ì  êµ¬ì„± íŠ¹ì§• ì¶”ì¶œ (ìµœì í™”)"""
        x1, y1, x2, y2 = map(int, bbox)
        height = y2 - y1
        width = x2 - x1
        
        if height < 10 or width < 10:  # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ê±´ë„ˆë›°ê¸°
            return []
        
        # Dominant Color Regions ì¶”ì¶œ (ìµœì í™”)
        hsv_roi = cv2.cvtColor(frame[y1:y2, x1:x2], cv2.COLOR_BGR2HSV)
        
        spatial_features = []
        dominant_colors = self.extract_dominant_colors(hist)
        
        # ìƒìœ„ 1ê°œ ìƒ‰ìƒë§Œ ì²˜ë¦¬ (ì„±ëŠ¥ í–¥ìƒ)
        for color_idx, percentage in dominant_colors[:1]:
            if percentage < 0.2:  # 20% ë¯¸ë§Œì€ ë¬´ì‹œ (10% â†’ 20%)
                continue
            
            # ë²¡í„°í™”ëœ ë§ˆìŠ¤í¬ ìƒì„±
            h, s, v = cv2.split(hsv_roi)
            h_norm = h.astype(np.float32) * 2
            s_norm = s.astype(np.float32) / 255.0
            v_norm = v.astype(np.float32) / 255.0
            
            # ì–‘ìí™” (ë²¡í„°í™”)
            H = np.zeros_like(h_norm, dtype=np.int32)
            S = np.zeros_like(s_norm, dtype=np.int32)
            V = np.zeros_like(v_norm, dtype=np.int32)
            
            # Hue ì–‘ìí™”
            H[(h_norm >= 316) | (h_norm < 20)] = 0
            H[(h_norm >= 20) & (h_norm < 40)] = 1
            H[(h_norm >= 40) & (h_norm < 75)] = 2
            H[(h_norm >= 75) & (h_norm < 155)] = 3
            H[(h_norm >= 155) & (h_norm < 190)] = 4
            H[(h_norm >= 190) & (h_norm < 270)] = 5
            H[(h_norm >= 270) & (h_norm < 295)] = 6
            H[(h_norm >= 295) & (h_norm < 316)] = 7
            
            # Saturation & Value ì–‘ìí™”
            S[s_norm <= 0.2] = 0
            S[(s_norm > 0.2) & (s_norm <= 0.7)] = 1
            S[s_norm > 0.7] = 2
            
            V[v_norm <= 0.2] = 0
            V[(v_norm > 0.2) & (v_norm <= 0.7)] = 1
            V[v_norm > 0.7] = 2
            
            C = 9 * H + 3 * S + V
            mask = (C == color_idx).astype(np.uint8) * 255
            
            # Connected Components ì¶”ì¶œ
            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
            
            # ê° ì˜ì—­ì˜ ê³µê°„ì  íŠ¹ì§• ê³„ì‚° (ìµœëŒ€ 1ê°œë§Œ)
            region_count = 0
            for k in range(1, min(num_labels, 2)):  # ìµœëŒ€ 1ê°œ ì˜ì—­ë§Œ
                area = stats[k, cv2.CC_STAT_AREA]
                if area < 30:  # ìµœì†Œ ì˜ì—­ í¬ê¸° ì¦ê°€ (10 â†’ 30)
                    continue
                
                # ì¤‘ì‹¬ì  yì¢Œí‘œ (ì •ê·œí™”)
                center_y = centroids[k][1] / height
                
                # ë†’ì´ (ì •ê·œí™”)
                region_height = stats[k, cv2.CC_STAT_HEIGHT] / height
                
                spatial_features.append({
                    'color_idx': color_idx,
                    'percentage': percentage,
                    'center_y': center_y,
                    'height': region_height
                })
                
                region_count += 1
                if region_count >= 1:  # ìµœëŒ€ 1ê°œ ì˜ì—­ë§Œ
                    break
        
        return spatial_features
    
    def calculate_spatial_similarity(self, spatial1, spatial2):
        """ë…¼ë¬¸ ë°©ì‹: ê³µê°„ì  êµ¬ì„± ìœ ì‚¬ë„"""
        # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
        if not isinstance(spatial1, list) or not isinstance(spatial2, list):
            return 0.0
            
        if not spatial1 or not spatial2:
            return 0.0
        
        min_distances = []
        
        for region1 in spatial1:
            distances = []
            for region2 in spatial2:
                if region1['color_idx'] == region2['color_idx']:
                    # ë…¼ë¬¸ ê³µì‹: dy(u,w) = |uy - wy|/H
                    dy = abs(region1['center_y'] - region2['center_y'])
                    
                    # ë…¼ë¬¸ ê³µì‹: dh(u,w) = |uh - wh|/H  
                    dh = abs(region1['height'] - region2['height'])
                    
                    # ë…¼ë¬¸ ê³µì‹: dR(u,w) = Î²*dy + (1-Î²)*dh
                    dr = self.beta * dy + (1 - self.beta) * dh
                    distances.append(dr)
            
            if distances:
                min_distances.append(min(distances))
        
        if min_distances:
            # ë…¼ë¬¸ ê³µì‹: dDCR = Î£ min(dR)
            spatial_similarity = 1.0 - np.mean(min_distances)
            return max(0.0, spatial_similarity)
        
        return 0.0
    
    def integrated_similarity(self, person1_data, person2_data):
        """ë…¼ë¬¸ ë°©ì‹: í†µí•© ìœ ì‚¬ë„ ê³„ì‚°"""
        # ìƒì²´ ìœ ì‚¬ë„
        upper_dcd = self.calculate_dcd_similarity(
            person1_data['upper_hist'], person2_data['upper_hist'])
        upper_spatial = self.calculate_spatial_similarity(
            person1_data['upper_spatial'], person2_data['upper_spatial'])
        
        # í•˜ì²´ ìœ ì‚¬ë„
        lower_dcd = self.calculate_dcd_similarity(
            person1_data['lower_hist'], person2_data['lower_hist'])
        lower_spatial = self.calculate_spatial_similarity(
            person1_data['lower_spatial'], person2_data['lower_spatial'])
        
        # ë…¼ë¬¸ ê³µì‹: d(AU,BU) = min(P1, P2)ì˜ í•©
        upper_similarity = upper_dcd
        
        # ë…¼ë¬¸ ê³µì‹: d(AL,BL) = min(P1, P2)ì˜ í•©
        lower_similarity = lower_dcd
        
        # ë…¼ë¬¸ ê³µì‹: dDCH = Î³*d(AU,BU) + (1-Î³)*d(AL,BL)
        dcd_similarity = self.gamma * upper_similarity + (1 - self.gamma) * lower_similarity
        
        # ê³µê°„ì  ìœ ì‚¬ë„ (ìƒí•˜ì²´ í‰ê· )
        spatial_similarity = (upper_spatial + lower_spatial) / 2
        
        # ë…¼ë¬¸ ê³µì‹: d(A,B) = Î±*dDCH + (1-Î±)*dDCR
        integrated_similarity = self.alpha * dcd_similarity + (1 - self.alpha) * spatial_similarity
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.performance_stats['dcd_matches'] += 1
        self.performance_stats['spatial_matches'] += 1
        self.performance_stats['integrated_matches'] += 1
        self.performance_stats['upper_body_matches'] += 1
        self.performance_stats['lower_body_matches'] += 1
        
        return integrated_similarity
    
    def process_person_detection(self, frame, yolo_detections):
        """ë…¼ë¬¸ ë°©ì‹: ì‚¬ëŒ ê°ì§€ ì²˜ë¦¬ (ìµœì í™”)"""
        current_people = set()
        
        # í”„ë ˆì„ ìŠ¤í‚µí•‘ (ì„±ëŠ¥ ìµœì í™”)
        if self.frame_count % self.process_every_n_frames != 0:
            self.performance_stats['skipped_frames'] += 1
            # ê¸°ì¡´ ì‚¬ëŒë“¤ì˜ ìœ„ì¹˜ë¥¼ ìœ ì§€
            for person_id in self.people:
                current_people.add(person_id)
            return current_people
        
        self.performance_stats['processed_frames'] += 1
        
        for detection in yolo_detections:
            bbox = detection['bbox']
            confidence = detection['confidence']
            
            if confidence < self.confidence_threshold:
                continue
            
            # ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸° í•„í„°ë§ (ì„±ëŠ¥ ìµœì í™”)
            x1, y1, x2, y2 = map(int, bbox)
            width = x2 - x1
            height = y2 - y1
            
            if width < self.min_bbox_size or height < self.min_bbox_size:
                continue
            
            # ìƒí•˜ì²´ ë¶„í• 
            upper_bbox, lower_bbox = self.segment_body_parts(frame, bbox)
            
            # ì–‘ìí™”ëœ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
            upper_hist = self.get_quantized_histogram(frame, upper_bbox)
            lower_hist = self.get_quantized_histogram(frame, lower_bbox)
            
            if upper_hist is None or lower_hist is None:
                continue
            
            # ê³µê°„ì  íŠ¹ì§• ì¶”ì¶œ (ìµœì í™”)
            upper_spatial = self.extract_spatial_features(frame, upper_bbox, upper_hist)
            lower_spatial = self.extract_spatial_features(frame, lower_bbox, lower_hist)
            
            # ê¸°ì¡´ ì‚¬ëŒê³¼ ë§¤ì¹­ (í˜„ì¬ í”„ë ˆì„ì— ìˆëŠ” ì‚¬ëŒ + ì¼ì‹œ ì‚¬ë¼ì§„ ì‚¬ëŒë“¤)
            best_match_id = None
            best_similarity = 0.0
            
            for person_id, person_data in self.people.items():
                # í˜„ì¬ í”„ë ˆì„ì— ì´ë¯¸ ë§¤ì¹­ëœ ì‚¬ëŒì€ ê±´ë„ˆë›°ê¸°
                if person_id in current_people:
                    continue
                
                # ì¼ì‹œì ìœ¼ë¡œ ì‚¬ë¼ì§„ ì‚¬ëŒë„ ë§¤ì¹­ ëŒ€ìƒì— í¬í•¨
                frames_missing = self.frame_count - person_data['last_seen']
                if frames_missing > self.max_disappeared:
                    continue  # ë„ˆë¬´ ì˜¤ë˜ ì‚¬ë¼ì§„ ì‚¬ëŒì€ ì œì™¸
                
                # ì¼ì‹œ ì‚¬ë¼ì§„ ì‚¬ëŒì€ ë” ê´€ëŒ€í•œ ì„ê³„ê°’ ì ìš©
                current_threshold = self.reappear_threshold if frames_missing > 0 else self.similarity_threshold
                
                similarity = self.integrated_similarity(
                    {
                        'upper_hist': upper_hist,
                        'lower_hist': lower_hist,
                        'upper_spatial': upper_spatial,
                        'lower_spatial': lower_spatial
                    },
                    person_data
                )
                
                # ë””ë²„ê¹…: ìƒì„¸í•œ ë§¤ì¹­ ì •ë³´ ì¶œë ¥
                if similarity > 0.3:  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ëª¨ë“  ë§¤ì¹­ ì‹œë„ í‘œì‹œ
                    upper_dcd = self.calculate_dcd_similarity(upper_hist, person_data['upper_hist'])
                    lower_dcd = self.calculate_dcd_similarity(lower_hist, person_data['lower_hist'])
                    upper_spatial = self.calculate_spatial_similarity(upper_spatial, person_data['upper_spatial'])
                    lower_spatial = self.calculate_spatial_similarity(lower_spatial, person_data['lower_spatial'])
                    
                    status = "ì¼ì‹œ ì‚¬ë¼ì§" if frames_missing > 0 else "í™œì„±"
                    print(f"ğŸ” ë§¤ì¹­ ì‹œë„: ID {person_id} vs í˜„ì¬ ê°ì§€ ({status})")
                    print(f"   - ìƒì²´ DCD ìœ ì‚¬ë„: {upper_dcd:.3f}")
                    print(f"   - í•˜ì²´ DCD ìœ ì‚¬ë„: {lower_dcd:.3f}")
                    print(f"   - ìƒì²´ ê³µê°„ ìœ ì‚¬ë„: {upper_spatial:.3f}")
                    print(f"   - í•˜ì²´ ê³µê°„ ìœ ì‚¬ë„: {lower_spatial:.3f}")
                    print(f"   - í†µí•© ìœ ì‚¬ë„: {similarity:.3f} (ì„ê³„ê°’: {current_threshold})")
                    print(f"   - ì‚¬ë¼ì§„ í”„ë ˆì„: {frames_missing}")
                
                if similarity > current_threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_match_id = person_id
            
            if best_match_id is not None:
                # ê¸°ì¡´ ì‚¬ëŒê³¼ ë§¤ì¹­ë¨
                current_people.add(best_match_id)
                
                # ë°”ìš´ë”©ë°•ìŠ¤ ìŠ¤ë¬´ë”© ì ìš©
                smoothed_bbox = self.smooth_bbox(best_match_id, bbox)
                
                # ë°ì´í„° ì—…ë°ì´íŠ¸ (ìƒ‰ìƒ ì •ë³´ ìœ ì§€)
                existing_color = self.people[best_match_id].get('color', self.colors[best_match_id % len(self.colors)])
                self.people[best_match_id].update({
                    'upper_hist': upper_hist,
                    'lower_hist': lower_hist,
                    'upper_spatial': upper_spatial,
                    'lower_spatial': lower_spatial,
                    'bbox': smoothed_bbox,  # ìŠ¤ë¬´ë”©ëœ ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš©
                    'last_seen': self.frame_count,
                    'confidence': confidence,
                    'color': existing_color  # ê¸°ì¡´ ìƒ‰ìƒ ìœ ì§€
                })
                
                print(f"ğŸ”„ ê¸°ì¡´ ì‚¬ëŒ ë§¤ì¹­: ID {best_match_id} (ìœ ì‚¬ë„: {best_similarity:.3f})")
                
                # ì¼ì‹œì ìœ¼ë¡œ ì‚¬ë¼ì¡Œë˜ ì‚¬ëŒì¸ì§€ í™•ì¸
                frames_missing = self.frame_count - self.people[best_match_id]['last_seen']
                if frames_missing > 0:
                    print(f"   âœ… ì¼ì‹œ ì‚¬ë¼ì¡Œë˜ ì‚¬ëŒ ì¬ë°œê²¬ (ì‚¬ë¼ì§„ í”„ë ˆì„: {frames_missing})")
            else:
                # ìƒˆë¡œìš´ ì‚¬ëŒ ì¶”ê°€
                if len(self.people) < self.max_people:
                    color = self.colors[self.next_id % len(self.colors)]
                    
                    print(f"ğŸ¨ ìƒˆë¡œìš´ ì‚¬ëŒ ID {self.next_id}ì—ê²Œ ìƒ‰ìƒ í• ë‹¹: {color}")
                    
                    self.people[self.next_id] = {
                        'upper_hist': upper_hist,
                        'lower_hist': lower_hist,
                        'upper_spatial': upper_spatial,
                        'lower_spatial': lower_spatial,
                        'color': color,
                        'bbox': bbox,
                        'last_seen': self.frame_count,
                        'confidence': confidence
                    }
                    
                    # ìë™ ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                    self.save_auto_screenshot(
                        frame, self.next_id, bbox, confidence,
                        upper_hist, lower_hist, upper_spatial, lower_spatial
                    )
                    
                    current_people.add(self.next_id)
                    print(f"ğŸ†• ìƒˆë¡œìš´ ì‚¬ëŒ: ID {self.next_id}")
                    self.next_id += 1
        
        # ì‚¬ë¼ì§„ ì‚¬ëŒ ì²˜ë¦¬ (ê°œì„ : ì¦‰ì‹œ ì œê±°í•˜ì§€ ì•Šê³  ê¸°ì–µ)
        people_to_remove = []
        for person_id in list(self.people.keys()):
            if person_id not in current_people:
                # í”„ë ˆì„ì—ì„œ ì‚¬ë¼ì§„ ì‹œê°„ ê³„ì‚°
                frames_missing = self.frame_count - self.people[person_id]['last_seen']
                
                if frames_missing > self.max_disappeared:
                    # ì˜¤ë«ë™ì•ˆ ì‚¬ë¼ì§„ ê²½ìš°ì—ë§Œ ì œê±°
                    people_to_remove.append(person_id)
                    print(f"ğŸ—‘ï¸ ì‚¬ëŒ ì œê±°: ID {person_id} (ì‚¬ë¼ì§„ í”„ë ˆì„: {frames_missing})")
                else:
                    # ì ì‹œ ì‚¬ë¼ì§„ ê²½ìš° ê¸°ì–µ (ë‹¤ì‹œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ)
                    print(f"â³ ì‚¬ëŒ ì¼ì‹œ ì‚¬ë¼ì§: ID {person_id} (ì‚¬ë¼ì§„ í”„ë ˆì„: {frames_missing}/{self.max_disappeared})")
        
        for person_id in people_to_remove:
            del self.people[person_id]
        
        return current_people
    
    def draw_paper_results(self, frame, current_people):
        """ë…¼ë¬¸ ë°©ì‹ ê²°ê³¼ ì‹œê°í™” (ê¹”ë”í•˜ê²Œ ê°œì„ )"""
        result_frame = frame.copy()
        
        # í˜„ì¬ í”„ë ˆì„ì—ì„œ YOLOë¡œ ê°ì§€ëœ ì‚¬ëŒë“¤
        yolo_detections = []
        try:
            from simple_detector import PersonDetector
            detector = PersonDetector()
            yolo_detections = detector.detect_people(frame)
        except:
            pass
        
        # YOLO ê°ì§€ ê²°ê³¼ë¥¼ ì‹œê°í™” (ì—°í•œ ì´ˆë¡ìƒ‰, ì–‡ì€ ì„ )
        for detection in yolo_detections:
            bbox = detection['bbox']
            confidence = detection['confidence']
            x1, y1, x2, y2 = map(int, bbox)
            
            # YOLO ê°ì§€ ê²°ê³¼ (ì—°í•œ ì´ˆë¡ìƒ‰, ì–‡ì€ ì„ )
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 200, 0), 1)
            
            # YOLO ì‹ ë¢°ë„ í‘œì‹œ (ì‘ê²Œ)
            cv2.putText(result_frame, f"YOLO: {confidence:.2f}", (x1, y1-5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 200, 0), 1)
        
        # ì¶”ì  ì¤‘ì¸ ì‚¬ëŒë“¤ì„ ì‹œê°í™” (ê¹”ë”í•˜ê²Œ)
        for person_id in current_people:
            if person_id in self.people:
                person_data = self.people[person_id]
                bbox = person_data['bbox']
                color = person_data['color']
                confidence = person_data.get('confidence', 0.5)
                
                # ì „ì²´ ë°”ìš´ë”©ë°•ìŠ¤ (ì§„í•œ ìƒ‰ìƒ, êµµì€ ì„ )
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 3)
                
                # ìƒí•˜ì²´ êµ¬ë¶„ì„  (í°ìƒ‰, ì–‡ì€ ì„ )
                split_y = y1 + int((y2 - y1) * self.upper_ratio)
                cv2.line(result_frame, (x1, split_y), (x2, split_y), (255, 255, 255), 1)
                
                # IDì™€ ì‹ ë¢°ë„ í‘œì‹œ (ê¹”ë”í•˜ê²Œ)
                id_text = f"ID:{person_id}"
                conf_text = f"{confidence:.2f}"
                
                # ID í…ìŠ¤íŠ¸ ë°°ê²½
                (id_w, id_h), _ = cv2.getTextSize(id_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(result_frame, (x1, y1-id_h-5), (x1+id_w+10, y1), color, -1)
                cv2.rectangle(result_frame, (x1, y1-id_h-5), (x1+id_w+10, y1), (255, 255, 255), 2)
                
                # ID í…ìŠ¤íŠ¸
                cv2.putText(result_frame, id_text, (x1+5, y1-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                # ì‹ ë¢°ë„ í…ìŠ¤íŠ¸ (ìš°ìƒë‹¨)
                (conf_w, conf_h), _ = cv2.getTextSize(conf_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
                cv2.rectangle(result_frame, (x2-conf_w-10, y1), (x2, y1+conf_h+5), (0, 0, 0), -1)
                cv2.rectangle(result_frame, (x2-conf_w-10, y1), (x2, y1+conf_h+5), color, 1)
                cv2.putText(result_frame, conf_text, (x2-conf_w-5, y1+conf_h), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
                
                # ì¤‘ì‹¬ì  í‘œì‹œ (ì‘ê²Œ)
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                cv2.circle(result_frame, (center_x, center_y), 3, color, -1)
        
        # ì‹œìŠ¤í…œ í†µê³„ (ê°„ë‹¨í•˜ê²Œ)
        stats_y = 650
        stats_texts = [
            f"ğŸ¯ Paper-Based Person Re-identification",
            f"Active: {len(current_people)} | YOLO: {len(yolo_detections)} | Frame: {self.frame_count}",
            f"Auto Screenshots: {self.performance_stats['auto_screenshots']} | Weights: Î±={self.alpha}, Î²={self.beta}, Î³={self.gamma}"
        ]
        
        # í†µê³„ ë°°ê²½ (ê°„ë‹¨í•˜ê²Œ)
        cv2.rectangle(result_frame, (10, stats_y - 10), (800, 720 - 10), (0, 0, 0), -1)
        cv2.rectangle(result_frame, (10, stats_y - 10), (800, 720 - 10), (255, 255, 255), 2)
        
        for i, text in enumerate(stats_texts):
            color = (0, 255, 255) if i == 0 else (255, 255, 255)
            cv2.putText(result_frame, text, (20, stats_y + 10 + i * 18), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        
        # ë²”ë¡€ (ê°„ë‹¨í•˜ê²Œ)
        legend_y = 30
        legend_texts = [
            "ğŸ“Š LEGEND:",
            "ğŸŸ¢ YOLO Detection",
            "ğŸ”´ Tracked Person"
        ]
        
        for i, text in enumerate(legend_texts):
            cv2.putText(result_frame, text, (10, legend_y + i * 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return result_frame
    
    def smooth_bbox(self, person_id, new_bbox):
        """ë°”ìš´ë”©ë°•ìŠ¤ ìŠ¤ë¬´ë”© ë° ì˜ˆì¸¡"""
        if person_id not in self.bbox_history:
            self.bbox_history[person_id] = []
            self.velocity_history[person_id] = [0, 0, 0, 0]  # dx1, dy1, dx2, dy2
        
        # í˜„ì¬ ë°”ìš´ë”©ë°•ìŠ¤
        x1, y1, x2, y2 = map(int, new_bbox)
        
        # ì´ì „ ë°”ìš´ë”©ë°•ìŠ¤ê°€ ìˆìœ¼ë©´ ìŠ¤ë¬´ë”© ì ìš©
        if self.bbox_history[person_id]:
            prev_bbox = self.bbox_history[person_id][-1]
            prev_x1, prev_y1, prev_x2, prev_y2 = prev_bbox
            
            # ì†ë„ ê³„ì‚°
            velocity = self.velocity_history[person_id]
            new_velocity = [
                x1 - prev_x1,
                y1 - prev_y1, 
                x2 - prev_x2,
                y2 - prev_y2
            ]
            
            # ì†ë„ ìŠ¤ë¬´ë”©
            for i in range(4):
                velocity[i] = velocity[i] * 0.8 + new_velocity[i] * 0.2
            
            # ì˜ˆì¸¡ëœ ìœ„ì¹˜
            predicted_x1 = prev_x1 + int(velocity[0])
            predicted_y1 = prev_y1 + int(velocity[1])
            predicted_x2 = prev_x2 + int(velocity[2])
            predicted_y2 = prev_y2 + int(velocity[3])
            
            # í˜„ì¬ ìœ„ì¹˜ì™€ ì˜ˆì¸¡ ìœ„ì¹˜ì˜ ê°€ì¤‘ í‰ê· 
            smoothed_x1 = int(predicted_x1 * self.bbox_smoothing_factor + x1 * (1 - self.bbox_smoothing_factor))
            smoothed_y1 = int(predicted_y1 * self.bbox_smoothing_factor + y1 * (1 - self.bbox_smoothing_factor))
            smoothed_x2 = int(predicted_x2 * self.bbox_smoothing_factor + x2 * (1 - self.bbox_smoothing_factor))
            smoothed_y2 = int(predicted_y2 * self.bbox_smoothing_factor + y2 * (1 - self.bbox_smoothing_factor))
            
            smoothed_bbox = [smoothed_x1, smoothed_y1, smoothed_x2, smoothed_y2]
        else:
            smoothed_bbox = [x1, y1, x2, y2]
        
        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ìµœëŒ€ 3ê°œ ìœ ì§€)
        self.bbox_history[person_id].append(smoothed_bbox)
        if len(self.bbox_history[person_id]) > 3:  # 5 â†’ 3 (ë©”ëª¨ë¦¬ ì ˆì•½)
            self.bbox_history[person_id].pop(0)
        
        return smoothed_bbox


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë…¼ë¬¸ ë°©ì‹ êµ¬í˜„"""
    print("ğŸš€ PAPER-BASED PERSON RE-IDENTIFICATION SYSTEM")
    print("ğŸ“– Based on: Color Histogram and Spatial Configuration of Dominant Color Regions")
    print("ğŸ‘¥ Authors: Kwangchol Jang, Sokmin Han, Insong Kim")
    print("ğŸ« Institution: KIM IL SUNG University")
    
    # ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ìƒì„±
    import os
    screenshot_dir = os.path.join(os.path.dirname(__file__), "paper_debug_screenshots")
    os.makedirs(screenshot_dir, exist_ok=True)
    print(f"ğŸ“ Screenshot directory: {screenshot_dir}")
    
    detector = PersonDetector()
    tracker = PaperBasedPersonReidentification()
    tracker.set_screenshot_directory(screenshot_dir) # íŠ¸ë˜ì»¤ì— ìŠ¤í¬ë¦°ìƒ· ë””ë ‰í† ë¦¬ ì„¤ì •
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ Cannot open webcam!")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    print("ğŸ“– Controls:")
    print("  - q: Quit")
    print("  - s: Save screenshot")
    print("  - r: Reset tracking")
    print("  - p: Performance stats")
    
    window_name = "Paper-Based Person Re-identification"
    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ Cannot read frame!")
                break
            
            tracker.frame_count += 1
            tracker.performance_stats['total_frames'] += 1
            
            # YOLO ì‚¬ëŒ ê°ì§€
            yolo_detections = detector.detect_people(frame)
            
            # ë…¼ë¬¸ ë°©ì‹ ì²˜ë¦¬
            current_people = tracker.process_person_detection(frame, yolo_detections)
            
            # ê²°ê³¼ ì‹œê°í™”
            result_frame = tracker.draw_paper_results(frame, current_people)
            
            cv2.imshow(window_name, result_frame)
            
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                print("ğŸ›‘ System shutdown")
                break
            elif key == ord('s'):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"paper_reid_{timestamp}.jpg"
                filepath = os.path.join(screenshot_dir, filename)
                
                # ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
                debug_info = [
                    f"Paper-Based Re-identification",
                    f"Frame: {tracker.frame_count}",
                    f"Active People: {len(current_people)}",
                    f"YOLO Detections: {len(yolo_detections)}",
                    f"HSV Quantization: 8x3x3",
                    f"Weights: Î±={tracker.alpha}, Î²={tracker.beta}, Î³={tracker.gamma}",
                    f"Timestamp: {timestamp}"
                ]
                
                debug_frame = result_frame.copy()
                debug_y = 30
                for info in debug_info:
                    cv2.putText(debug_frame, info, (10, debug_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    debug_y += 25
                
                success = cv2.imwrite(filepath, debug_frame)
                if success:
                    print(f"ğŸ“¸ Paper method screenshot saved: {filepath}")
                    print(f"   - Frame: {tracker.frame_count}")
                    print(f"   - Active people: {len(current_people)}")
                    print(f"   - DCD matches: {tracker.performance_stats['dcd_matches']}")
                else:
                    print(f"âŒ Failed to save screenshot: {filepath}")
            elif key == ord('r'):
                tracker.people.clear()
                tracker.next_id = 0
                print("ğŸ”„ Tracking reset")
            elif key == ord('p'):
                stats = tracker.performance_stats
                print(f"\nğŸ“Š Paper Method Performance Stats:")
                print(f"  - Total frames: {stats['total_frames']}")
                print(f"  - Processed frames: {stats['processed_frames']}")
                print(f"  - Skipped frames: {stats['skipped_frames']}")
                print(f"  - Active people: {len(tracker.people)}")
                print(f"  - DCD matches: {stats['dcd_matches']}")
                print(f"  - Spatial matches: {stats['spatial_matches']}")
                print(f"  - Integrated matches: {stats['integrated_matches']}")
                print(f"  - Upper body matches: {stats['upper_body_matches']}")
                print(f"  - Lower body matches: {stats['lower_body_matches']}")
                print(f"  - Auto screenshots: {stats['auto_screenshots']}")
                
    except KeyboardInterrupt:
        print("ğŸ›‘ User interrupt")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\nğŸ¯ Final Results:")
        print(f"  - Total frames: {tracker.performance_stats['total_frames']}")
        print(f"  - Processed frames: {tracker.performance_stats['processed_frames']}")
        print(f"  - Active people: {len(tracker.people)}")
        print(f"  - Auto screenshots: {tracker.performance_stats['auto_screenshots']}")
        print(f"  - Paper method implemented successfully")
        print(f"  - Screenshots saved in: {screenshot_dir}")
        
        print(f"âœ… Paper-Based Person Re-identification System terminated")

if __name__ == "__main__":
    main()

import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
import os
from datetime import datetime

# Qt ÌîåÎû´Ìèº ÌîåÎü¨Í∑∏Ïù∏ Ïò§Î•ò Î∞©ÏßÄ
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['QT_AUTO_SCREEN_SCALE_FACTOR'] = '0'
os.environ['QT_SCALE_FACTOR'] = '1'

# matplotlib Î∞±ÏóîÎìúÎ•º AggÎ°ú ÏÑ§Ï†ï
plt.switch_backend('Agg')

def estimate_distance(bbox_height, ref_height=300, ref_distance=1.0):
    distance = ref_height / (bbox_height + 1e-6) * ref_distance
    return round(distance, 2)

def estimate_distance_from_mask(mask, ref_height=300, ref_distance=1.0):
    """ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÎßàÏä§ÌÅ¨Î•º ÏÇ¨Ïö©Ìïú Îçî Ï†ïÌôïÌïú Í±∞Î¶¨ Í≥ÑÏÇ∞"""
    # ÎßàÏä§ÌÅ¨ÏóêÏÑú Ïã§Ï†ú ÏÇ¨Îûå ÏòÅÏó≠Ïùò ÎÜíÏù¥ Í≥ÑÏÇ∞
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # Í∞ÄÏû• ÌÅ∞ Ïª®Ìà¨Ïñ¥ (ÏÇ¨Îûå) ÏÑ†ÌÉù
        cnt = max(contours, key=cv2.contourArea)
        
        # Ïª®Ìà¨Ïñ¥Ïùò Î∞îÏö¥Îî© Î∞ïÏä§
        x, y, w, h = cv2.boundingRect(cnt)
        
        # Ïã§Ï†ú ÏÇ¨Îûå ÎÜíÏù¥ (ÌîΩÏÖÄ Îã®ÏúÑ)
        person_height = h
        
        # Í±∞Î¶¨ Í≥ÑÏÇ∞ (Ïó≠ÎπÑÎ°Ä Í¥ÄÍ≥Ñ)
        distance = ref_height / (person_height + 1e-6) * ref_distance
        return round(distance, 2)
    
    # Ïª®Ìà¨Ïñ¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÎäî Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í Î∞òÌôò
    return 2.0

def estimate_distance_advanced(mask, ref_height=300, ref_distance=1.0):
    """Í≥†Í∏â Í±∞Î¶¨ Í≥ÑÏÇ∞ - ÎßàÏä§ÌÅ¨Ïùò Ïã§Ï†ú ÌîΩÏÖÄ ÏàòÏôÄ ÌòïÌÉú Í≥†Î†§"""
    # ÎßàÏä§ÌÅ¨ÏóêÏÑú Ïã§Ï†ú ÏÇ¨Îûå ÏòÅÏó≠ Î∂ÑÏÑù
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        
        # Ïª®Ìà¨Ïñ¥Ïùò Î∞îÏö¥Îî© Î∞ïÏä§
        x, y, w, h = cv2.boundingRect(cnt)
        
        # Ïã§Ï†ú ÏÇ¨Îûå ÏòÅÏó≠Ïùò ÌîΩÏÖÄ Ïàò
        person_pixels = cv2.contourArea(cnt)
        
        # Î∞îÏö¥Îî© Î∞ïÏä§ ÏòÅÏó≠
        bbox_area = w * h
        
        # ÏÇ¨ÎûåÏù¥ Î∞îÏö¥Îî© Î∞ïÏä§Î•º ÏñºÎßàÎÇò Ï±ÑÏö∞ÎäîÏßÄ (Î∞ÄÎèÑ)
        density = person_pixels / (bbox_area + 1e-6)
        
        # Î∞ÄÎèÑÎ•º Í≥†Î†§Ìïú Ï°∞Ï†ïÎêú ÎÜíÏù¥
        adjusted_height = h * density
        
        # Í±∞Î¶¨ Í≥ÑÏÇ∞
        distance = ref_height / (adjusted_height + 1e-6) * ref_distance
        
        return round(distance, 2), {
            'person_height': h,
            'person_pixels': person_pixels,
            'bbox_area': bbox_area,
            'density': density,
            'adjusted_height': adjusted_height
        }
    
    return 2.0, {}

class HSVAnalyzer:
    def __init__(self):
        self.model = YOLO('yolov8s-seg.pt')
        self.people_data = {}  # ID: {histograms: [], timestamps: [], images: []}
        self.next_id = 0
        
        # ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï
        self.process_every_n_frames = 3  # 3ÌîÑÎ†àÏûÑÎßàÎã§ Ï≤òÎ¶¨ (ÏÑ±Îä• Ìñ•ÏÉÅ)
        self.frame_skip_counter = 0
        
        # Îß§Ïπ≠ Í¥ÄÎ†® ÏÑ§Ï†ï
        self.match_threshold = 0.35  # Îß§Ïπ≠ ÏûÑÍ≥ÑÍ∞í
        self.reentry_threshold = 0.30  # Ïû¨ÏßÑÏûÖ ÏûÑÍ≥ÑÍ∞í
        self.min_detection_confidence = 0.6  # ÏµúÏÜå Í∞êÏßÄ Ïã†Î¢∞ÎèÑ
        self.min_person_area = 5000  # ÏµúÏÜå ÏÇ¨Îûå ÏòÅÏó≠
        self.max_frames_without_seen = 300  # 10Ï¥à ÌõÑÏóêÎèÑ Í∏∞Ïñµ
        
        # ÌûàÏä§ÌÜ†Í∑∏Îû® Í∏∞Ïñµ ÏÑ§Ï†ï
        self.max_histograms_per_person = 10  # ÏÇ¨ÎûåÎãπ ÏµúÎåÄ ÌûàÏä§ÌÜ†Í∑∏Îû® Ï†ÄÏû• Ïàò
        self.histogram_memory_duration = 30  # 30Ï¥àÍ∞Ñ ÌûàÏä§ÌÜ†Í∑∏Îû® Í∏∞Ïñµ
        
        # Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
        self.analysis_dir = "./hsv_analysis"
        if not os.path.exists(self.analysis_dir):
            os.makedirs(self.analysis_dir)
            print(f"üìÅ Î∂ÑÏÑù ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±: {self.analysis_dir}")
    
    def extract_histogram(self, img, mask, bins=16):
        """HSVÏùò Î™®Îì† Ï±ÑÎÑê(H, S, V)ÏùÑ Í≥†Î†§Ìïú ÌûàÏä§ÌÜ†Í∑∏Îû® Ï∂îÏ∂ú"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # Í∞Å Ï±ÑÎÑêÎ≥Ñ ÌûàÏä§ÌÜ†Í∑∏Îû® Í≥ÑÏÇ∞
        h_hist = cv2.calcHist([hsv], [0], mask, [bins], [0, 180])
        s_hist = cv2.calcHist([hsv], [1], mask, [bins], [0, 256])
        v_hist = cv2.calcHist([hsv], [2], mask, [bins], [0, 256])
        
        # Ï†ïÍ∑úÌôî
        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()
        
        # Î™®Îì† Ï±ÑÎÑêÏùÑ ÌïòÎÇòÏùò Î≤°ÌÑ∞Î°ú Í≤∞Ìï©
        combined_hist = np.concatenate([h_hist, s_hist, v_hist])
        
        return combined_hist, h_hist, s_hist, v_hist
    
    def calculate_similarity_metrics(self, hist1, hist2):
        """Îã§ÏñëÌïú Ïú†ÏÇ¨ÎèÑ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""
        # Bhattacharyya Í±∞Î¶¨
        bhatt_dist = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_BHATTACHARYYA)
        
        # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ
        dot_product = np.dot(hist1, hist2)
        norm1 = np.linalg.norm(hist1)
        norm2 = np.linalg.norm(hist2)
        cosine_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
        
        # ÏÉÅÍ¥ÄÍ≥ÑÏàò
        corr = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)
        
        # Chi-Square Í±∞Î¶¨
        chi_square = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CHISQR)
        
        # Intersection
        intersection = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_INTERSECT)
        
        return {
            'bhattacharyya': bhatt_dist,
            'cosine_similarity': cosine_sim,
            'correlation': corr,
            'chi_square': chi_square,
            'intersection': intersection
        }
    
    def find_best_match(self, current_hist, current_bbox, used_ids):
        """Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÏÇ¨Îûå Ï∞æÍ∏∞ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)"""
        best_match_id = None
        best_score = 0.0
        best_metrics = {}
        
        x1, y1, x2, y2 = current_bbox
        current_area = (x2 - x1) * (y2 - y1)
        current_center = ((x1 + x2) // 2, (y1 + y2) // 2)
        
        # ÌòÑÏû¨ ÌîÑÎ†àÏûÑÏóêÏÑú Í∞êÏßÄÎêú Î™®Îì† ÏÇ¨ÎûåÍ≥º ÎπÑÍµê
        for pid, pdata in self.people_data.items():
            # Ïù¥ÎØ∏ ÏÇ¨Ïö©Îêú IDÎäî Ï†úÏô∏
            if pid in used_ids:
                continue
                
            if len(pdata['histograms']) == 0:
                continue
            
            # Î™®Îì† ÌûàÏä§ÌÜ†Í∑∏Îû®Í≥º ÎπÑÍµê (ÏµúÍ∑º 10Í∞ú)
            hist_scores = []
            for i, stored_hist in enumerate(pdata['histograms'][-self.max_histograms_per_person:]):
                metrics = self.calculate_similarity_metrics(current_hist, stored_hist)
                hist_score = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                hist_scores.append(hist_score)
            
            # ÏµúÍ≥† ÌûàÏä§ÌÜ†Í∑∏Îû® Ï†êÏàò ÏÇ¨Ïö©
            best_hist_score = max(hist_scores) if hist_scores else 0.0
            
            # Í≥µÍ∞ÑÏ†Å Ïú†ÏÇ¨ÎèÑ (Í∞ÄÏ§ëÏπò ÎåÄÌè≠ Í∞êÏÜå: 30% ‚Üí 10%)
            latest_bbox = pdata['bboxes'][-1]
            stored_area = (latest_bbox[2] - latest_bbox[0]) * (latest_bbox[3] - latest_bbox[1])
            stored_center = ((latest_bbox[0] + latest_bbox[2]) // 2, (latest_bbox[1] + latest_bbox[3]) // 2)
            
            # Ï§ëÏã¨Ï†ê Í±∞Î¶¨ Í≥ÑÏÇ∞
            center_distance = np.sqrt((current_center[0] - stored_center[0])**2 + 
                                     (current_center[1] - stored_center[1])**2)
            max_distance = np.sqrt(640**2 + 480**2)
            spatial_score = 1.0 - (center_distance / max_distance)
            
            # Ï¢ÖÌï© Ï†êÏàò (ÌûàÏä§ÌÜ†Í∑∏Îû® 90%, Í≥µÍ∞ÑÏ†Å ÏúÑÏπò 10%) - ÏúÑÏπò Í∞ÄÏ§ëÏπò ÎåÄÌè≠ Í∞êÏÜå
            total_score = 0.9 * best_hist_score + 0.1 * spatial_score
            
            # Îçî ÎÜíÏùÄ Ï†êÏàòÎ•º Í∞ÄÏßÑ Îß§Ïπ≠ ÏÑ†ÌÉù
            if total_score > best_score:
                best_score = total_score
                best_match_id = pid
                best_metrics = {
                    'hist_score': best_hist_score,
                    'spatial_score': spatial_score,
                    'hist_scores': hist_scores
                }
        
        return best_match_id, best_score, best_metrics
    
    def visualize_histogram_comparison(self, hist1, hist2, person_id1, person_id2, frame_count, save_path):
        """ÌûàÏä§ÌÜ†Í∑∏Îû® ÎπÑÍµê ÏãúÍ∞ÅÌôî"""
        bins = 16
        h_hist1 = hist1[:bins]
        s_hist1 = hist1[bins:2*bins]
        v_hist1 = hist1[2*bins:]
        h_hist2 = hist2[:bins]
        s_hist2 = hist2[bins:2*bins]
        v_hist2 = hist2[2*bins:]
        
        # Ïú†ÏÇ¨ÎèÑ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        metrics = self.calculate_similarity_metrics(hist1, hist2)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'HSV Histogram Comparison: {person_id1} vs {person_id2} (Frame {frame_count})', fontsize=16)
        
        # H Ï±ÑÎÑê ÎπÑÍµê
        axes[0, 0].bar(np.arange(bins) - 0.2, h_hist1, 0.4, label=person_id1, alpha=0.7, color='blue')
        axes[0, 0].bar(np.arange(bins) + 0.2, h_hist2, 0.4, label=person_id2, alpha=0.7, color='red')
        axes[0, 0].set_title('Hue Channel')
        axes[0, 0].set_xlabel('Hue Bins')
        axes[0, 0].set_ylabel('Normalized Frequency')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # S Ï±ÑÎÑê ÎπÑÍµê
        axes[0, 1].bar(np.arange(bins) - 0.2, s_hist1, 0.4, label=person_id1, alpha=0.7, color='blue')
        axes[0, 1].bar(np.arange(bins) + 0.2, s_hist2, 0.4, label=person_id2, alpha=0.7, color='red')
        axes[0, 1].set_title('Saturation Channel')
        axes[0, 1].set_xlabel('Saturation Bins')
        axes[0, 1].set_ylabel('Normalized Frequency')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # V Ï±ÑÎÑê ÎπÑÍµê
        axes[0, 2].bar(np.arange(bins) - 0.2, v_hist1, 0.4, label=person_id1, alpha=0.7, color='blue')
        axes[0, 2].bar(np.arange(bins) + 0.2, v_hist2, 0.4, label=person_id2, alpha=0.7, color='red')
        axes[0, 2].set_title('Value Channel')
        axes[0, 2].set_xlabel('Value Bins')
        axes[0, 2].set_ylabel('Normalized Frequency')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # Ï∞®Ïù¥ Î∂ÑÏÑù
        axes[1, 0].bar(np.arange(bins), np.abs(h_hist1 - h_hist2), color='orange', alpha=0.7)
        axes[1, 0].set_title(f'Hue Difference (Sum: {np.sum(np.abs(h_hist1 - h_hist2)):.4f})')
        axes[1, 0].set_xlabel('Hue Bins')
        axes[1, 0].set_ylabel('Absolute Difference')
        axes[1, 0].grid(True, alpha=0.3)
        
        axes[1, 1].bar(np.arange(bins), np.abs(s_hist1 - s_hist2), color='orange', alpha=0.7)
        axes[1, 1].set_title(f'Saturation Difference (Sum: {np.sum(np.abs(s_hist1 - s_hist2)):.4f})')
        axes[1, 1].set_xlabel('Saturation Bins')
        axes[1, 1].set_ylabel('Absolute Difference')
        axes[1, 1].grid(True, alpha=0.3)
        
        axes[1, 2].bar(np.arange(bins), np.abs(v_hist1 - v_hist2), color='orange', alpha=0.7)
        axes[1, 2].set_title(f'Value Difference (Sum: {np.sum(np.abs(v_hist1 - v_hist2)):.4f})')
        axes[1, 2].set_xlabel('Value Bins')
        axes[1, 2].set_ylabel('Absolute Difference')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        return metrics
    
    def create_similarity_matrix(self, save_path):
        """Î™®Îì† ÏÇ¨Îûå Í∞ÑÏùò Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§ ÏÉùÏÑ±"""
        if len(self.people_data) < 2:
            print("‚ö†Ô∏è Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§Î•º ÏÉùÏÑ±ÌïòÎ†§Î©¥ ÏµúÏÜå 2Î™ÖÏùò ÏÇ¨ÎûåÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.")
            return
        
        people_ids = list(self.people_data.keys())
        n_people = len(people_ids)
        
        # Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§ Ï¥àÍ∏∞Ìôî
        similarity_matrix = np.zeros((n_people, n_people))
        bhatt_matrix = np.zeros((n_people, n_people))
        cosine_matrix = np.zeros((n_people, n_people))
        
        print(f"\nüìä {n_people}Î™ÖÏùò ÏÇ¨Îûå Í∞Ñ Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§ ÏÉùÏÑ± Ï§ë...")
        
        for i, pid1 in enumerate(people_ids):
            for j, pid2 in enumerate(people_ids):
                if i == j:
                    similarity_matrix[i, j] = 1.0
                    bhatt_matrix[i, j] = 0.0
                    cosine_matrix[i, j] = 1.0
                else:
                    # Í∞Å ÏÇ¨ÎûåÏùò ÏµúÏã† ÌûàÏä§ÌÜ†Í∑∏Îû® ÏÇ¨Ïö©
                    hist1 = self.people_data[pid1]['histograms'][-1]
                    hist2 = self.people_data[pid2]['histograms'][-1]
                    
                    metrics = self.calculate_similarity_metrics(hist1, hist2)
                    
                    # Ï¢ÖÌï© Ïú†ÏÇ¨ÎèÑ Ï†êÏàò
                    hist_score = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                    similarity_matrix[i, j] = hist_score
                    bhatt_matrix[i, j] = metrics['bhattacharyya']
                    cosine_matrix[i, j] = metrics['cosine_similarity']
        
        # ÏãúÍ∞ÅÌôî
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        
        # Ï¢ÖÌï© Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§
        im1 = axes[0].imshow(similarity_matrix, cmap='RdYlBu_r', vmin=0, vmax=1)
        axes[0].set_title('Overall Similarity Matrix')
        axes[0].set_xticks(range(n_people))
        axes[0].set_yticks(range(n_people))
        axes[0].set_xticklabels(people_ids, rotation=45)
        axes[0].set_yticklabels(people_ids)
        plt.colorbar(im1, ax=axes[0])
        
        # Í∞í ÌëúÏãú
        for i in range(n_people):
            for j in range(n_people):
                axes[0].text(j, i, f'{similarity_matrix[i, j]:.3f}', 
                           ha='center', va='center', fontsize=8)
        
        # Bhattacharyya Í±∞Î¶¨ Îß§Ìä∏Î¶≠Ïä§
        im2 = axes[1].imshow(bhatt_matrix, cmap='Reds', vmin=0, vmax=1)
        axes[1].set_title('Bhattacharyya Distance Matrix')
        axes[1].set_xticks(range(n_people))
        axes[1].set_yticks(range(n_people))
        axes[1].set_xticklabels(people_ids, rotation=45)
        axes[1].set_yticklabels(people_ids)
        plt.colorbar(im2, ax=axes[1])
        
        # Í∞í ÌëúÏãú
        for i in range(n_people):
            for j in range(n_people):
                axes[1].text(j, i, f'{bhatt_matrix[i, j]:.3f}', 
                           ha='center', va='center', fontsize=8)
        
        # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§
        im3 = axes[2].imshow(cosine_matrix, cmap='Blues', vmin=0, vmax=1)
        axes[2].set_title('Cosine Similarity Matrix')
        axes[2].set_xticks(range(n_people))
        axes[2].set_yticks(range(n_people))
        axes[2].set_xticklabels(people_ids, rotation=45)
        axes[2].set_yticklabels(people_ids)
        plt.colorbar(im3, ax=axes[2])
        
        # Í∞í ÌëúÏãú
        for i in range(n_people):
            for j in range(n_people):
                axes[2].text(j, i, f'{cosine_matrix[i, j]:.3f}', 
                           ha='center', va='center', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§ Ï†ÄÏû•: {save_path}")
        
        # ÏàòÏπòÏ†Å Î∂ÑÏÑù Í≤∞Í≥º Ï∂úÎ†•
        print(f"\nüìà ÏàòÏπòÏ†Å Î∂ÑÏÑù Í≤∞Í≥º:")
        print(f"   - ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ: {np.mean(similarity_matrix):.3f}")
        print(f"   - ÏµúÎåÄ Ïú†ÏÇ¨ÎèÑ: {np.max(similarity_matrix):.3f}")
        print(f"   - ÏµúÏÜå Ïú†ÏÇ¨ÎèÑ: {np.min(similarity_matrix):.3f}")
        print(f"   - ÌëúÏ§ÄÌé∏Ï∞®: {np.std(similarity_matrix):.3f}")
        
        # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÏåçÍ≥º Í∞ÄÏû• Îã§Î•∏ Ïåç Ï∞æÍ∏∞
        max_sim_idx = np.unravel_index(np.argmax(similarity_matrix), similarity_matrix.shape)
        min_sim_idx = np.unravel_index(np.argmin(similarity_matrix), similarity_matrix.shape)
        
        if max_sim_idx[0] != max_sim_idx[1]:
            print(f"   - Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïåç: {people_ids[max_sim_idx[0]]} vs {people_ids[max_sim_idx[1]]} (Ïú†ÏÇ¨ÎèÑ: {similarity_matrix[max_sim_idx]:.3f})")
        if min_sim_idx[0] != min_sim_idx[1]:
            print(f"   - Í∞ÄÏû• Îã§Î•∏ Ïåç: {people_ids[min_sim_idx[0]]} vs {people_ids[min_sim_idx[1]]} (Ïú†ÏÇ¨ÎèÑ: {similarity_matrix[min_sim_idx]:.3f})")
    
    def run_analysis(self, duration_seconds=30):
        """HSV ÌûàÏä§ÌÜ†Í∑∏Îû® Î∂ÑÏÑù Ïã§Ìñâ"""
        cap = cv2.VideoCapture(0)  # 2ÏóêÏÑú 0ÏúºÎ°ú Î≥ÄÍ≤Ω
        
        if not cap.isOpened():
            print(f"‚ùå Ïπ¥Î©îÎùºÏó∞Í≤∞ Ïã§Ìå®")
            return
        
       
        frame_count = 0
        start_time = datetime.now()
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            if elapsed_time > duration_seconds:
                break
            
            # ÌîÑÎ†àÏûÑ Ï≤òÎ¶¨ Í∞ÑÍ≤© Ï°∞Ï†à
            if self.frame_skip_counter < self.process_every_n_frames - 1:
                self.frame_skip_counter += 1
                continue
            
            self.frame_skip_counter = 0 # Ïπ¥Ïö¥ÌÑ∞ Ï¥àÍ∏∞Ìôî
            
            # ÏÇ¨Îûå Í∞êÏßÄ
            results = self.model(frame, classes=[0])  # class 0 = person
            annotated = frame.copy()
            
            # ÌòÑÏû¨ ÌîÑÎ†àÏûÑÏóêÏÑú Í∞êÏßÄÎêú Î™®Îì† ÏÇ¨ÎûåÏùò Ï†ïÎ≥¥Î•º Î®ºÏ†Ä ÏàòÏßë
            current_detections = []
            
            for result in results:
                for i in range(len(result.boxes)):
                    seg = result.masks.data[i]
                    box = result.boxes[i]
                    confidence = box.conf[0].item()
                    
                    if confidence < self.min_detection_confidence:
                        continue
                    
                    mask = seg.cpu().numpy().astype(np.uint8) * 255
                    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)
                    
                    # ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞
                    kernel = np.ones((5,5), np.uint8)
                    mask_cleaned = cv2.morphologyEx(mask_resized, cv2.MORPH_CLOSE, kernel)
                    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)
                    
                    # HSV ÌûàÏä§ÌÜ†Í∑∏Îû® Ï∂îÏ∂ú
                    combined_hist, h_hist, s_hist, v_hist = self.extract_histogram(frame, mask_cleaned)
                    
                    # Î∞îÏö¥Îî© Î∞ïÏä§ Ï∂îÏ∂ú
                    bbox = box.xyxy[0].cpu().numpy()
                    x1, y1, x2, y2 = bbox
                    area = (x2 - x1) * (y2 - y1)
                    
                    if area < self.min_person_area:
                        continue
                    
                    # Í±∞Î¶¨ Ï∂îÏ†ï (ÏÇ¨Ïö©ÏûêÍ∞Ä Ï∂îÍ∞ÄÌïú Ìï®Ïàò ÏÇ¨Ïö©)
                    person_height = y2 - y1
                    distance = estimate_distance(person_height, ref_height=300, ref_distance=1.0)  # m Îã®ÏúÑ
                    
                    # ÏÑ∏Í∑∏Î©òÌÖåÏù¥ÏÖò ÎßàÏä§ÌÅ¨Î•º ÏÇ¨Ïö©Ìïú Îçî Ï†ïÌôïÌïú Í±∞Î¶¨ Í≥ÑÏÇ∞
                    est_dist, dist_info = estimate_distance_advanced(mask_cleaned, ref_height=300, ref_distance=1.0)
                    
                    current_detections.append({
                        'hist': combined_hist,
                        'bbox': bbox,
                        'mask': mask_cleaned,
                        'confidence': confidence,
                        'area': area,
                        'distance': est_dist,  # Îçî Ï†ïÌôïÌïú Í±∞Î¶¨ ÏÇ¨Ïö©
                        'dist_info': dist_info  # Í±∞Î¶¨ Í≥ÑÏÇ∞ Ï†ïÎ≥¥ÎèÑ Ï†ÄÏû•
                    })
            
            # Í∞êÏßÄÎêú ÏÇ¨ÎûåÎì§ÏùÑ ÏòÅÏó≠ ÌÅ¨Í∏∞ ÏàúÏúºÎ°ú Ï†ïÎ†¨ (ÌÅ∞ ÏÇ¨ÎûåÎ∂ÄÌÑ∞ Ï≤òÎ¶¨)
            current_detections.sort(key=lambda x: x['area'], reverse=True)
            
            # Ïù¥ÎØ∏ Îß§Ïπ≠Îêú IDÎì§ÏùÑ Ï∂îÏ†Å
            used_ids = set()
            
            # Í∞Å Í∞êÏßÄÎêú ÏÇ¨ÎûåÏóê ÎåÄÌï¥ Îß§Ïπ≠ ÏàòÌñâ
            for detection in current_detections:
                combined_hist = detection['hist']
                bbox = detection['bbox']
                
                # Îß§Ïπ≠ ÏãúÎèÑ
                matched_id, match_score, metrics = self.find_best_match(combined_hist, bbox, used_ids)
                
                print(f"üéØ Îß§Ïπ≠ Í≤∞Í≥º: {matched_id}, Ï†êÏàò: {match_score:.3f}, Í±∞Î¶¨: {detection['distance']:.2f}m, ÏûÑÍ≥ÑÍ∞í: {self.match_threshold:.3f}")
                
                # Í±∞Î¶¨ Í≥ÑÏÇ∞ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï∂úÎ†•
                if detection['dist_info']:
                    print(f"   üìè Í±∞Î¶¨ Í≥ÑÏÇ∞ ÏÉÅÏÑ∏: ÎÜíÏù¥={detection['dist_info']['person_height']}px, Î∞ÄÎèÑ={detection['dist_info']['density']:.3f}")
                
                # Îß§Ïπ≠ ÏÑ±Í≥µ Ïó¨Î∂ÄÏóê Îî∞Î•∏ Ï≤òÎ¶¨
                if matched_id is not None and match_score > self.match_threshold:
                    # Í∏∞Ï°¥ ÏÇ¨Îûå ÏóÖÎç∞Ïù¥Ìä∏
                    self.people_data[matched_id]['histograms'].append(combined_hist)
                    self.people_data[matched_id]['bboxes'].append(bbox)
                    self.people_data[matched_id]['timestamps'].append(elapsed_time)
                    used_ids.add(matched_id)  # ID ÏÇ¨Ïö©Îê® ÌëúÏãú
                    
                    # ÌûàÏä§ÌÜ†Í∑∏Îû® Î©îÎ™®Î¶¨ Í¥ÄÎ¶¨ (ÏµúÎåÄ Í∞úÏàò Ï†úÌïú)
                    if len(self.people_data[matched_id]['histograms']) > self.max_histograms_per_person:
                        # Í∞ÄÏû• Ïò§ÎûòÎêú ÌûàÏä§ÌÜ†Í∑∏Îû® Ï†úÍ±∞
                        self.people_data[matched_id]['histograms'].pop(0)
                        self.people_data[matched_id]['bboxes'].pop(0)
                        self.people_data[matched_id]['timestamps'].pop(0)
                    
                    color = (0, 255, 0)  # Ï¥àÎ°ùÏÉâ (Í∏∞Ï°¥ ÏÇ¨Îûå)
                    print(f"üîÑ Í∏∞Ï°¥ ÏÇ¨Îûå Ïû¨ÏãùÎ≥Ñ: {matched_id} (Ï†êÏàò: {match_score:.3f})")
                    print(f"   - ÌûàÏä§ÌÜ†Í∑∏Îû® Ï†êÏàò: {metrics['hist_score']:.3f}")
                    print(f"   - Í≥µÍ∞ÑÏ†Å Ï†êÏàò: {metrics['spatial_score']:.3f}")
                    print(f"   - Ï†ÄÏû•Îêú ÌûàÏä§ÌÜ†Í∑∏Îû® Ïàò: {len(self.people_data[matched_id]['histograms'])}")
                    
                else:
                    # ÏÉàÎ°úÏö¥ ÏÇ¨Îûå (Îß§Ïπ≠ Ïã§Ìå® ÎòêÎäî ÏûÑÍ≥ÑÍ∞í ÎØ∏Îã¨)
                    new_id = f"Person_{self.next_id}"
                    self.people_data[new_id] = {
                        'histograms': [combined_hist],
                        'bboxes': [bbox],
                        'timestamps': [elapsed_time],
                        'images': []
                    }
                    self.next_id += 1
                    used_ids.add(new_id)  # ID ÏÇ¨Ïö©Îê® ÌëúÏãú
                    
                    color = (0, 0, 255)  # Îπ®Í∞ÑÏÉâ (ÏÉàÎ°úÏö¥ ÏÇ¨Îûå)
                    print(f"üÜï ÏÉàÎ°úÏö¥ ÏÇ¨Îûå Í∞êÏßÄ: {new_id} (ÏµúÍ≥† Ï†êÏàò: {match_score:.3f}, ÏûÑÍ≥ÑÍ∞í ÎØ∏Îã¨)")
                    print(f"   - Îß§Ïπ≠ Ïã§Ìå®Î°ú Ïù∏Ìïú ÏÉàÎ°úÏö¥ ÏÇ¨Îûå Îì±Î°ù")
                
                # ÏãúÍ∞ÅÌôî
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                
                # ID ÌëúÏãú
                id_text = matched_id if matched_id else new_id
                cv2.putText(annotated, id_text, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                
                # Confidence ÌëúÏãú
                conf_text = f"Conf: {detection['confidence']:.2f}"
                cv2.putText(annotated, conf_text, (x1, y2+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # Í±∞Î¶¨ ÌëúÏãú (m Îã®ÏúÑ) - Îçî Ï†ïÌôïÌïú Í±∞Î¶¨ Í≥ÑÏÇ∞ ÏÇ¨Ïö©
                distance_text = f"Dist: {detection['distance']:.2f}m"
                cv2.putText(annotated, distance_text, (x1, y2+40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # Î∞ÄÎèÑ Ï†ïÎ≥¥ ÌëúÏãú (ÎîîÎ≤ÑÍπÖÏö©)
                if detection['dist_info']:
                    density_text = f"Density: {detection['dist_info']['density']:.2f}"
                    cv2.putText(annotated, density_text, (x1, y2+60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                
                # Ïú†ÏÇ¨ÎèÑ Ï†êÏàò ÌëúÏãú
                score_text = f"{match_score:.3f}"
                cv2.putText(annotated, score_text, (x1, y2+80), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # ÏãúÏä§ÌÖú Ï†ïÎ≥¥ ÌëúÏãú
            info_text = f"People: {len(self.people_data)} | Frame: {frame_count} | Time: {elapsed_time:.1f}s"
            cv2.putText(annotated, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # ÏÑ±Îä• ÏµúÏ†ÅÌôî: 5ÌîÑÎ†àÏûÑÎßàÎã§Îßå ÌôîÎ©¥ ÏóÖÎç∞Ïù¥Ìä∏
            if frame_count % 5 == 0:
                cv2.imshow("HSV Histogram Analysis", annotated)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # Î∂ÑÏÑù Í≤∞Í≥º ÏÉùÏÑ±
        print(f"\nüìä Î∂ÑÏÑù ÏôÑÎ£å!")
        print(f"   - Ï¥ù ÌîÑÎ†àÏûÑ: {frame_count}")
        print(f"   - Í∞êÏßÄÎêú ÏÇ¨Îûå Ïàò: {len(self.people_data)}")
        
        # Ïú†ÏÇ¨ÎèÑ Îß§Ìä∏Î¶≠Ïä§ ÏÉùÏÑ±
        if len(self.people_data) > 1:
            matrix_path = os.path.join(self.analysis_dir, "similarity_matrix.png")
            self.create_similarity_matrix(matrix_path)
        
        # Í∞Å ÏÇ¨ÎûåÏùò ÌûàÏä§ÌÜ†Í∑∏Îû® Î≥ÄÌôî Î∂ÑÏÑù
        for pid, pdata in self.people_data.items():
            if len(pdata['histograms']) > 1:
                print(f"\nüë§ {pid} HSV ÌûàÏä§ÌÜ†Í∑∏Îû® Î≥ÄÌôî Î∂ÑÏÑù:")
                histograms = np.array(pdata['histograms'])
                
                # ÏãúÍ∞ÑÏóê Îî∞Î•∏ ÌûàÏä§ÌÜ†Í∑∏Îû® Î≥ÄÌôî
                hist_variance = np.var(histograms, axis=0)
                hist_mean = np.mean(histograms, axis=0)
                hist_max = np.max(histograms, axis=0)
                hist_min = np.min(histograms, axis=0)
                
                # HSV Ï±ÑÎÑêÎ≥Ñ Î∂ÑÏÑù
                bins = 16
                h_variance = hist_variance[:bins]
                s_variance = hist_variance[bins:2*bins]
                v_variance = hist_variance[2*bins:]
                
                h_mean = hist_mean[:bins]
                s_mean = hist_mean[bins:2*bins]
                v_mean = hist_mean[2*bins:]
                
                h_max = hist_max[:bins]
                s_max = hist_max[bins:2*bins]
                v_max = hist_max[2*bins:]
                
                print(f"   üìä Ï†ÑÏ≤¥ ÌûàÏä§ÌÜ†Í∑∏Îû® ÌÜµÍ≥Ñ:")
                print(f"      - Î∂ÑÏÇ∞ (ÌèâÍ∑†): {np.mean(hist_variance):.6f}")
                print(f"      - Î∂ÑÏÇ∞ (ÏµúÎåÄ): {np.max(hist_variance):.6f}")
                print(f"      - Î∂ÑÏÇ∞ (ÏµúÏÜå): {np.min(hist_variance):.6f}")
                print(f"      - Î∂ÑÏÇ∞ (ÌëúÏ§ÄÌé∏Ï∞®): {np.std(hist_variance):.6f}")
                
                print(f"   üé® HSV Ï±ÑÎÑêÎ≥Ñ Î∂ÑÏÇ∞ Î∂ÑÏÑù:")
                print(f"      - Hue Î∂ÑÏÇ∞ (ÌèâÍ∑†): {np.mean(h_variance):.6f}")
                print(f"      - Saturation Î∂ÑÏÇ∞ (ÌèâÍ∑†): {np.mean(s_variance):.6f}")
                print(f"      - Value Î∂ÑÏÇ∞ (ÌèâÍ∑†): {np.mean(v_variance):.6f}")
                
                print(f"   üìà HSV Ï±ÑÎÑêÎ≥Ñ ÌèâÍ∑†Í∞í:")
                print(f"      - Hue ÌèâÍ∑†: {np.mean(h_mean):.4f}")
                print(f"      - Saturation ÌèâÍ∑†: {np.mean(s_mean):.4f}")
                print(f"      - Value ÌèâÍ∑†: {np.mean(v_mean):.4f}")
                
                print(f"   üî• HSV Ï±ÑÎÑêÎ≥Ñ ÏµúÎåÄÍ∞í:")
                print(f"      - Hue ÏµúÎåÄ: {np.max(h_max):.4f}")
                print(f"      - Saturation ÏµúÎåÄ: {np.max(s_max):.4f}")
                print(f"      - Value ÏµúÎåÄ: {np.max(v_max):.4f}")
                
                # ÏïàÏ†ïÏÑ± ÌèâÍ∞Ä
                stability_score = 1.0 - np.mean(hist_variance)
                print(f"   üéØ ÏïàÏ†ïÏÑ± ÌèâÍ∞Ä:")
                print(f"      - Ï†ÑÏ≤¥ ÏïàÏ†ïÏÑ± Ï†êÏàò: {stability_score:.4f}")
                print(f"      - Hue ÏïàÏ†ïÏÑ±: {1.0 - np.mean(h_variance):.4f}")
                print(f"      - Saturation ÏïàÏ†ïÏÑ±: {1.0 - np.mean(s_variance):.4f}")
                print(f"      - Value ÏïàÏ†ïÏÑ±: {1.0 - np.mean(v_variance):.4f}")
                
                # Ï≤´ Î≤àÏß∏ÏôÄ ÎßàÏßÄÎßâ ÌûàÏä§ÌÜ†Í∑∏Îû® ÎπÑÍµê
                first_hist = histograms[0]
                last_hist = histograms[-1]
                metrics = self.calculate_similarity_metrics(first_hist, last_hist)
                print(f"   üîÑ Ï≤´-ÎßàÏßÄÎßâ ÌîÑÎ†àÏûÑ ÎπÑÍµê:")
                print(f"      - Bhattacharyya Í±∞Î¶¨: {metrics['bhattacharyya']:.4f}")
                print(f"      - ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ: {metrics['cosine_similarity']:.4f}")
                print(f"      - ÏÉÅÍ¥ÄÍ≥ÑÏàò: {metrics['correlation']:.4f}")
                print(f"      - Ï¢ÖÌï© Ïú†ÏÇ¨ÎèÑ: {max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity']):.4f}")
                
                # Ìä∏ÎûòÌÇπ ÏßÄÏÜç Í∞ÄÎä•ÏÑ± ÌèâÍ∞Ä
                if stability_score > 0.8:
                    tracking_assessment = "üü¢ Îß§Ïö∞ ÏïàÏ†ïÏ†Å - Ïû•Í∏∞ Ìä∏ÎûòÌÇπ Í∞ÄÎä•"
                elif stability_score > 0.6:
                    tracking_assessment = "üü° Î≥¥ÌÜµ ÏïàÏ†ïÏ†Å - Ï§ëÍ∏∞ Ìä∏ÎûòÌÇπ Í∞ÄÎä•"
                elif stability_score > 0.4:
                    tracking_assessment = "üü† Î∂àÏïàÏ†ï - Îã®Í∏∞ Ìä∏ÎûòÌÇπÎßå Í∞ÄÎä•"
                else:
                    tracking_assessment = "üî¥ Îß§Ïö∞ Î∂àÏïàÏ†ï - Ìä∏ÎûòÌÇπ Ïñ¥Î†§ÏõÄ"
                
                print(f"   üéØ Ìä∏ÎûòÌÇπ ÏßÄÏÜç Í∞ÄÎä•ÏÑ±: {tracking_assessment}")
                
                # Ïó∞ÏÜçÏÑ± Î∂ÑÏÑù
                consecutive_similarities = []
                for i in range(1, len(histograms)):
                    prev_hist = histograms[i-1]
                    curr_hist = histograms[i]
                    metrics = self.calculate_similarity_metrics(prev_hist, curr_hist)
                    similarity = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                    consecutive_similarities.append(similarity)
                
                if consecutive_similarities:
                    print(f"   üìä Ïó∞ÏÜç ÌîÑÎ†àÏûÑ Ïú†ÏÇ¨ÎèÑ:")
                    print(f"      - ÌèâÍ∑†: {np.mean(consecutive_similarities):.4f}")
                    print(f"      - ÏµúÏÜå: {np.min(consecutive_similarities):.4f}")
                    print(f"      - ÏµúÎåÄ: {np.max(consecutive_similarities):.4f}")
                    print(f"      - ÌëúÏ§ÄÌé∏Ï∞®: {np.std(consecutive_similarities):.4f}")

if __name__ == "__main__":
    analyzer = HSVAnalyzer()
    
    analyzer.run_analysis(duration_seconds=60)
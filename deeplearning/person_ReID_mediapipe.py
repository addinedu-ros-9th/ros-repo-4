import cv2
import numpy as np
from ultralytics import YOLO
import mediapipe as mp
import torch
import torch.nn as nn
from collections import deque
import os
from datetime import datetime

# Qt í”Œë«í¼ í”ŒëŸ¬ê·¸ì¸ ì˜¤ë¥˜ ë°©ì§€
os.environ['QT_QPA_PLATFORM'] = 'xcb'
os.environ['QT_DEBUG_PLUGINS'] = '0'
os.environ['QT_AUTO_SCREEN_SCALE_FACTOR'] = '0'
os.environ['QT_SCALE_FACTOR'] = '1'

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ í´ë˜ìŠ¤
class GestureCNN(nn.Module):
    """ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN)"""
    def __init__(self, num_classes=3, num_frames=60, num_joints=21):
        super(GestureCNN, self).__init__()
        
        self.num_classes = num_classes
        self.num_frames = num_frames
        self.num_joints = num_joints
        
        # ì…ë ¥ íŠ¹ì§• ê³„ì‚°
        self.basic_features = 99  # ëœë“œë§ˆí¬(84) + ê°ë„(15)
        self.gesture_features = 260  # ë™ì (252) + ì†ëª¨ì–‘(8)
        self.total_features = self.basic_features * num_frames + self.gesture_features
        
        # 1. ê¸°ë³¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
        self.sequence_encoder = nn.Sequential(
            nn.Linear(self.basic_features * num_frames, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # 2. ì œìŠ¤ì²˜ íŠ¹ì„± ì²˜ë¦¬
        self.gesture_encoder = nn.Sequential(
            nn.Linear(self.gesture_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 3. ê²°í•©ëœ íŠ¹ì§• ì²˜ë¦¬
        self.combined_encoder = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # ê¸°ë³¸ ì‹œí€€ìŠ¤ íŠ¹ì§• ì¶”ì¶œ
        sequence_features = x[:, :self.basic_features * self.num_frames]
        sequence_encoded = self.sequence_encoder(sequence_features)
        
        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ
        gesture_features = x[:, self.basic_features * self.num_frames:]
        gesture_encoded = self.gesture_encoder(gesture_features)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([sequence_encoded, gesture_encoded], dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.combined_encoder(combined_features)
        
        return output

def estimate_distance(bbox_height, ref_height=300, ref_distance=1.0):
    distance = ref_height / (bbox_height + 1e-6) * ref_distance
    return round(distance, 2)

def estimate_distance_from_mask(mask, ref_height=300, ref_distance=1.0):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œ ë” ì •í™•í•œ ê±°ë¦¬ ê³„ì‚°"""
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        person_height = h
        distance = ref_height / (person_height + 1e-6) * ref_distance
        return round(distance, 2)
    
    return 2.0

def estimate_distance_advanced(mask, ref_height=300, ref_distance=1.0):
    """ê³ ê¸‰ ê±°ë¦¬ ê³„ì‚° - ë§ˆìŠ¤í¬ì˜ ì‹¤ì œ í”½ì…€ ìˆ˜ì™€ í˜•íƒœ ê³ ë ¤"""
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        person_pixels = cv2.contourArea(cnt)
        bbox_area = w * h
        density = person_pixels / (bbox_area + 1e-6)
        adjusted_height = h * density
        distance = ref_height / (adjusted_height + 1e-6) * ref_distance
        
        return round(distance, 2), {
            'person_height': h,
            'person_pixels': person_pixels,
            'bbox_area': bbox_area,
            'density': density,
            'adjusted_height': adjusted_height
        }
    
    return 2.0, {}

def extract_gesture_features(data):
    """ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ (í•™ìŠµ ì‹œì™€ ë™ì¼)"""
    landmarks = data[:, :84]
    angles = data[:, 84:99]
    
    # 1. ë™ì  íŠ¹ì„±
    motion_features = []
    for i in range(1, len(data)):
        landmark_diff = landmarks[i] - landmarks[i-1]
        motion_features.append(landmark_diff)
    
    if len(motion_features) > 0:
        motion_features = np.array(motion_features)
        motion_mean = np.mean(motion_features, axis=0)
        motion_std = np.std(motion_features, axis=0)
        motion_max = np.max(np.abs(motion_features), axis=0)
    else:
        motion_mean = np.zeros(84)
        motion_std = np.zeros(84)
        motion_max = np.zeros(84)
    
    # 2. ì† ëª¨ì–‘ íŠ¹ì„±
    finger_tips = [8, 12, 16, 20]
    palm_center = 0
    
    hand_shape_features = []
    for frame in landmarks:
        frame_reshaped = frame.reshape(-1, 4)
        finger_distances = []
        for tip_idx in finger_tips:
            tip_pos = frame_reshaped[tip_idx][:3]
            palm_pos = frame_reshaped[palm_center][:3]
            distance = np.linalg.norm(tip_pos - palm_pos)
            finger_distances.append(distance)
        hand_shape_features.append(finger_distances)
    
    hand_shape_features = np.array(hand_shape_features)
    hand_shape_mean = np.mean(hand_shape_features, axis=0)
    hand_shape_std = np.std(hand_shape_features, axis=0)
    
    # 3. ì œìŠ¤ì²˜ë³„ íŠ¹ì„± ë²¡í„°
    gesture_specific = np.concatenate([
        motion_mean, motion_std, motion_max,
        hand_shape_mean, hand_shape_std
    ])
    
    return gesture_specific

class IntegratedPersonGestureSystem:
    def __init__(self):
        # YOLO ëª¨ë¸ ì´ˆê¸°í™”
        self.model = YOLO('./deeplearning/yolov8s-seg.pt')
        
        # MediaPipe Hands ì´ˆê¸°í™”
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=4,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # ì œìŠ¤ì²˜ ëª¨ë¸ ì´ˆê¸°í™”
        self.gesture_model = GestureCNN(num_classes=3, num_frames=60, num_joints=21)
        model_path = 'gesture_recognition/best_gesture_cnn_model.pth'
        if os.path.exists(model_path):
            self.gesture_model.load_state_dict(torch.load(model_path))
            print(f"âœ… ì œìŠ¤ì²˜ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        else:
            print(f"âŒ ì œìŠ¤ì²˜ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print("ì œìŠ¤ì²˜ ì¸ì‹ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            self.enable_gesture_recognition = False
        self.gesture_model = self.gesture_model.to(device)
        self.gesture_model.eval()
        
        # ì‚¬ëŒ ì¬ì‹ë³„ ë°ì´í„°
        self.people_data = {}
        self.next_id = 0
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.process_every_n_frames = 3
        self.frame_skip_counter = 0
        
        # ë§¤ì¹­ ê´€ë ¨ ì„¤ì • (ë” ì—„ê²©í•œ ì„¤ì • - ëª¨ì/ì˜ìƒ êµ¬ë¶„ ê°•í™”)
        self.match_threshold = 0.50  # 0.45 â†’ 0.50ë¡œ ì¦ê°€ (ë” ì—„ê²©í•œ ë§¤ì¹­)
        self.reentry_threshold = 0.45  # 0.40 â†’ 0.45ë¡œ ì¦ê°€
        self.min_detection_confidence = 0.6
        self.min_person_area = 5000
        self.max_frames_without_seen = 300
        
        # íˆìŠ¤í† ê·¸ë¨ ê¸°ì–µ ì„¤ì • (ë” ë§ì€ íˆìŠ¤í† ê·¸ë¨ ì €ì¥)
        self.max_histograms_per_person = 20  # 15 â†’ 20ìœ¼ë¡œ ì¦ê°€ (ë” ë§ì€ ìƒ˜í”Œ)
        self.histogram_memory_duration = 30
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„¤ì • (ë” ì—„ê²©í•œ ì¡°ê±´)
        self.enable_gesture_recognition = True
        self.gesture_frame_buffer = deque(maxlen=60)
        self.gesture_prediction_buffer = deque(maxlen=5)
        self.actions = ['COME', 'AWAY', 'STOP']
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ì„ê³„ê°’ (ë” ì—„ê²©í•˜ê²Œ)
        self.gesture_confidence_threshold = 0.75  # 0.75 ì´ìƒì¼ ë•Œë§Œ ì œìŠ¤ì²˜ë¡œ ì¸ì‹
        self.gesture_buffer_threshold = 4  # 4ë²ˆ ì´ìƒ ê°™ì€ ì œìŠ¤ì²˜ê°€ ë‚˜ì™€ì•¼ ì¸ì‹
        self.min_gesture_frames = 40  # ìµœì†Œ 40í”„ë ˆì„ì´ ëª¨ì—¬ì•¼ ì˜ˆì¸¡ ì‹œì‘
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.analysis_dir = "./deeplearning/integrated_analysis"
        if not os.path.exists(self.analysis_dir):
            os.makedirs(self.analysis_dir)
            print(f"ğŸ“ í†µí•© ë¶„ì„ ë””ë ‰í† ë¦¬ ìƒì„±: {self.analysis_dir}")
    
    def extract_histogram(self, img, mask, bins=16):
        """HSVì˜ ëª¨ë“  ì±„ë„(H, S, V)ì„ ê³ ë ¤í•œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        
        # ì „ì²´ ë§ˆìŠ¤í¬ì—ì„œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
        h_hist = cv2.calcHist([hsv], [0], mask, [bins], [0, 180])
        s_hist = cv2.calcHist([hsv], [1], mask, [bins], [0, 256])
        v_hist = cv2.calcHist([hsv], [2], mask, [bins], [0, 256])
        
        # ìƒì²´ ë¶€ë¶„ (ë¨¸ë¦¬/ëª¨ì/ìƒì˜)ì— ì§‘ì¤‘í•œ íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            cnt = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(cnt)
            
            # ìƒì²´ ë¶€ë¶„ ë§ˆìŠ¤í¬ ìƒì„± (ì „ì²´ ë†’ì´ì˜ ìƒìœ„ 60%)
            upper_mask = np.zeros_like(mask)
            upper_y = y + int(h * 0.6)  # ìƒìœ„ 60% ë¶€ë¶„
            upper_mask[y:upper_y, x:x+w] = mask[y:upper_y, x:x+w]
            
            # ìƒì²´ ë¶€ë¶„ íˆìŠ¤í† ê·¸ë¨
            h_hist_upper = cv2.calcHist([hsv], [0], upper_mask, [bins], [0, 180])
            s_hist_upper = cv2.calcHist([hsv], [1], upper_mask, [bins], [0, 256])
            v_hist_upper = cv2.calcHist([hsv], [2], upper_mask, [bins], [0, 256])
        else:
            h_hist_upper = h_hist.copy()
            s_hist_upper = s_hist.copy()
            v_hist_upper = v_hist.copy()
        
        # ì •ê·œí™”
        h_hist = cv2.normalize(h_hist, h_hist).flatten()
        s_hist = cv2.normalize(s_hist, s_hist).flatten()
        v_hist = cv2.normalize(v_hist, v_hist).flatten()
        
        h_hist_upper = cv2.normalize(h_hist_upper, h_hist_upper).flatten()
        s_hist_upper = cv2.normalize(s_hist_upper, s_hist_upper).flatten()
        v_hist_upper = cv2.normalize(v_hist_upper, v_hist_upper).flatten()
        
        # ì „ì²´ + ìƒì²´ íˆìŠ¤í† ê·¸ë¨ ê²°í•© (ìƒì²´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        combined_hist = np.concatenate([
            h_hist * 0.3, s_hist * 0.3, v_hist * 0.3,  # ì „ì²´ (30%)
            h_hist_upper * 0.7, s_hist_upper * 0.7, v_hist_upper * 0.7  # ìƒì²´ (70%)
        ])
        
        return combined_hist, h_hist, s_hist, v_hist
    
    def calculate_similarity_metrics(self, hist1, hist2):
        """ë‹¤ì–‘í•œ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        bhatt_dist = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_BHATTACHARYYA)
        
        dot_product = np.dot(hist1, hist2)
        norm1 = np.linalg.norm(hist1)
        norm2 = np.linalg.norm(hist2)
        cosine_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
        
        corr = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CORREL)
        chi_square = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_CHISQR)
        intersection = cv2.compareHist(hist1.astype(np.float32), hist2.astype(np.float32), cv2.HISTCMP_INTERSECT)
        
        return {
            'bhattacharyya': bhatt_dist,
            'cosine_similarity': cosine_sim,
            'correlation': corr,
            'chi_square': chi_square,
            'intersection': intersection
        }
    
    def find_best_match(self, current_hist, current_bbox, used_ids, elapsed_time=0.0):
        """ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ëŒ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)"""
        best_match_id = None
        best_score = 0.0
        best_metrics = {}
        
        x1, y1, x2, y2 = current_bbox
        current_center = ((x1 + x2) // 2, (y1 + y2) // 2)
        current_area = (x2 - x1) * (y2 - y1)
        
        for pid, pdata in self.people_data.items():
            if pid in used_ids:
                continue
                
            if len(pdata['histograms']) == 0:
                continue
            
            # 1. íˆìŠ¤í† ê·¸ë¨ ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¥ ì¤‘ìš”)
            hist_scores = []
            for i, stored_hist in enumerate(pdata['histograms'][-self.max_histograms_per_person:]):
                metrics = self.calculate_similarity_metrics(current_hist, stored_hist)
                # Bhattacharyya ê±°ë¦¬ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ëª¨ë‘ ê³ ë ¤
                hist_score = max(1.0 - metrics['bhattacharyya'], metrics['cosine_similarity'])
                hist_scores.append(hist_score)
            
            # ìµœê³  íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜ (ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ê·¸ë¨ ì‚¬ìš©)
            best_hist_score = max(hist_scores) if hist_scores else 0.0
            
            # 2. ê³µê°„ì  ìœ ì‚¬ë„ ê³„ì‚°
            latest_bbox = pdata['bboxes'][-1]
            stored_center = ((latest_bbox[0] + latest_bbox[2]) // 2, (latest_bbox[1] + latest_bbox[3]) // 2)
            stored_area = (latest_bbox[2] - latest_bbox[0]) * (latest_bbox[3] - latest_bbox[1])
            
            # ì¤‘ì‹¬ì  ê±°ë¦¬
            center_distance = np.sqrt((current_center[0] - stored_center[0])**2 + 
                                     (current_center[1] - stored_center[1])**2)
            max_distance = np.sqrt(640**2 + 480**2)
            spatial_score = 1.0 - (center_distance / max_distance)
            
            # 3. í¬ê¸° ìœ ì‚¬ë„ (ì‚¬ëŒì´ ê°‘ìê¸° í¬ê²Œ ë³€í•˜ì§€ ì•ŠìŒ)
            area_ratio = min(current_area, stored_area) / max(current_area, stored_area)
            size_score = area_ratio
            
            # 4. ì‹œê°„ì  ì—°ì†ì„± (ìµœê·¼ì— ë³¸ ì‚¬ëŒì—ê²Œ ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            if pdata['timestamps']:
                time_since_last_seen = elapsed_time - pdata['timestamps'][-1]
                time_score = max(0.0, 1.0 - (time_since_last_seen / 10.0))  # 10ì´ˆ ë‚´ì— ë³¸ ì‚¬ëŒì—ê²Œ ë†’ì€ ì ìˆ˜
            else:
                time_score = 0.0
            
            # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì¡°ì •)
            total_score = (
                0.7 * best_hist_score +      # íˆìŠ¤í† ê·¸ë¨ (ê°€ì¥ ì¤‘ìš”)
                0.15 * spatial_score +       # ê³µê°„ì  ìœ„ì¹˜
                0.1 * size_score +           # í¬ê¸° ìœ ì‚¬ë„
                0.05 * time_score            # ì‹œê°„ì  ì—°ì†ì„±
            )
            
            # 6. ìµœì†Œ ì„ê³„ê°’ ê²€ì‚¬ (íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸)
            if best_hist_score < 0.35:  # 0.3 â†’ 0.35ë¡œ ì¦ê°€ (ë” ì—„ê²©í•œ íˆìŠ¤í† ê·¸ë¨ ë§¤ì¹­)
                continue
            
            # 7. ë” ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë§¤ì¹­ ì„ íƒ
            if total_score > best_score:
                best_score = total_score
                best_match_id = pid
                best_metrics = {
                    'hist_score': best_hist_score,
                    'spatial_score': spatial_score,
                    'size_score': size_score,
                    'time_score': time_score,
                    'hist_scores': hist_scores
                }
        
        return best_match_id, best_score, best_metrics
    
    def process_gesture(self, frame):
        """ì œìŠ¤ì²˜ ì¸ì‹ ì²˜ë¦¬"""
        if not self.enable_gesture_recognition:
            return "DISABLED", 0.0
        
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(img_rgb)
        
        prediction = "WAITING..."
        confidence = 0.0
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                self.mp_drawing.draw_landmarks(frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS)
                
                # ëœë“œë§ˆí¬ ë°ì´í„° ì¶”ì¶œ
                joint = np.zeros((21, 4))
                for j, lm in enumerate(hand_landmarks.landmark):
                    joint[j] = [lm.x, lm.y, lm.z, lm.visibility]
                
                # ê°ë„ ê³„ì‚°
                v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
                v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
                v = v2 - v1
                v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]
                
                angle = np.arccos(np.einsum('nt,nt->n',
                    v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], 
                    v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))
                angle = np.degrees(angle)
                
                # íŠ¹ì§• ë²¡í„° ìƒì„±
                d = np.concatenate([joint.flatten(), angle])
                self.gesture_frame_buffer.append(d)
                
                # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
                if len(self.gesture_frame_buffer) >= self.min_gesture_frames:
                    try:
                        frame_data = np.array(list(self.gesture_frame_buffer))
                        
                        frames = len(frame_data)
                        if frames < 60:
                            padding = np.zeros((60 - frames, 99), dtype=np.float32)
                            x = np.vstack([frame_data, padding])
                        elif frames > 60:
                            start = (frames - 60) // 2
                            x = frame_data[start:start + 60]
                        else:
                            x = frame_data
                        
                        gesture_features = extract_gesture_features(x)
                        x_with_gesture = np.concatenate([x.flatten(), gesture_features])
                        model_input = torch.FloatTensor(x_with_gesture).unsqueeze(0).to(device)
                        
                        with torch.no_grad():
                            outputs = self.gesture_model(model_input)
                            probabilities = torch.softmax(outputs, dim=1)
                            predicted = torch.argmax(outputs, dim=1).item()
                            confidence = probabilities[0][predicted].item()
                        
                        prediction = self.actions[predicted]
                        self.gesture_prediction_buffer.append(predicted)
                        
                    except Exception as e:
                        print(f"ì œìŠ¤ì²˜ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                        prediction = "ERROR"
        
        # ë¶€ë“œëŸ¬ìš´ ì˜ˆì¸¡ ê²°ê³¼
        if len(self.gesture_prediction_buffer) >= self.gesture_buffer_threshold:
            from collections import Counter
            most_common = Counter(self.gesture_prediction_buffer).most_common(1)[0]
            final_prediction = self.actions[most_common[0]]
            final_confidence = most_common[1] / len(self.gesture_prediction_buffer)
            
            # ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ì„ ë„˜ì§€ ì•Šìœ¼ë©´ WAITINGìœ¼ë¡œ ì²˜ë¦¬
            if final_confidence < 0.8:  # 80% ì´ìƒ ì¼ê´€ì„±ì´ ìˆì–´ì•¼ ì œìŠ¤ì²˜ë¡œ ì¸ì‹
                final_prediction = "WAITING..."
                final_confidence = 0.0
        else:
            final_prediction = prediction
            final_confidence = confidence
        
        # ìµœì¢… ì‹ ë¢°ë„ ê²€ì‚¬ (ëª¨ë¸ ì‹ ë¢°ë„ë„ ê³ ë ¤)
        if final_confidence < self.gesture_confidence_threshold:
            final_prediction = "WAITING..."
            final_confidence = 0.0
        
        return final_prediction, final_confidence
    
    def run_integrated_system(self, camera_device="/dev/video2"):
        """í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰ (ë¬´í•œ ì‹¤í–‰)"""
        print("ğŸš€ í†µí•© ì‚¬ëŒ ì¬ì‹ë³„ + ì œìŠ¤ì²˜ ì¸ì‹ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ“¹ ì¹´ë©”ë¼: {camera_device}")
        print("â±ï¸ ë¬´í•œ ì‹¤í–‰ ëª¨ë“œ (qí‚¤ë¡œ ì¢…ë£Œ)")
        
        cap = cv2.VideoCapture(camera_device)
        
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {camera_device}")
            return
        
        frame_count = 0
        start_time = datetime.now()
        
        # ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼ ì¶”ì 
        current_gesture = "WAITING..."
        current_gesture_confidence = 0.0
        gesture_history = []
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            elapsed_time = (datetime.now() - start_time).total_seconds()
            
            # í”„ë ˆì„ ì²˜ë¦¬ ê°„ê²© ì¡°ì ˆ
            if self.frame_skip_counter < self.process_every_n_frames - 1:
                self.frame_skip_counter += 1
                continue
            
            self.frame_skip_counter = 0
            
            # ì‚¬ëŒ ê°ì§€
            results = self.model(frame, classes=[0])
            annotated = frame.copy()
            
            # ì œìŠ¤ì²˜ ì¸ì‹ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸)
            gesture_prediction, gesture_confidence = self.process_gesture(annotated)
            
            # ì œìŠ¤ì²˜ ê²°ê³¼ ì—…ë°ì´íŠ¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
            if (gesture_prediction != "WAITING..." and 
                gesture_prediction != "ERROR" and 
                gesture_confidence >= self.gesture_confidence_threshold):
                
                current_gesture = gesture_prediction
                current_gesture_confidence = gesture_confidence
                
                # ì œìŠ¤ì²˜ íˆìŠ¤í† ë¦¬ ì €ì¥
                gesture_history.append({
                    'gesture': gesture_prediction,
                    'confidence': gesture_confidence,
                    'timestamp': elapsed_time,
                    'frame': frame_count
                })
                
                # ì½˜ì†”ì— ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼ ì¶œë ¥
                print(f"âœ‹ ì œìŠ¤ì²˜ ì¸ì‹: {gesture_prediction} (ì‹ ë¢°ë„: {gesture_confidence:.2f})")
            elif gesture_prediction == "WAITING...":
                # WAITING ìƒíƒœì¼ ë•ŒëŠ” ì´ì „ ì œìŠ¤ì²˜ë¥¼ ìœ ì§€í•˜ë˜ ì‹ ë¢°ë„ëŠ” ë‚®ì¶¤
                current_gesture_confidence = max(0.0, current_gesture_confidence - 0.1)
            
            # í˜„ì¬ í”„ë ˆì„ì—ì„œ ê°ì§€ëœ ëª¨ë“  ì‚¬ëŒì˜ ì •ë³´ë¥¼ ë¨¼ì € ìˆ˜ì§‘
            current_detections = []
            
            for result in results:
                for i in range(len(result.boxes)):
                    seg = result.masks.data[i]
                    box = result.boxes[i]
                    confidence = box.conf[0].item()
                    
                    if confidence < self.min_detection_confidence:
                        continue
                    
                    mask = seg.cpu().numpy().astype(np.uint8) * 255
                    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)
                    
                    # ë…¸ì´ì¦ˆ ì œê±°
                    kernel = np.ones((5,5), np.uint8)
                    mask_cleaned = cv2.morphologyEx(mask_resized, cv2.MORPH_CLOSE, kernel)
                    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_OPEN, kernel)
                    
                    # HSV íˆìŠ¤í† ê·¸ë¨ ì¶”ì¶œ
                    combined_hist, h_hist, s_hist, v_hist = self.extract_histogram(frame, mask_cleaned)
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
                    bbox = box.xyxy[0].cpu().numpy()
                    x1, y1, x2, y2 = bbox
                    area = (x2 - x1) * (y2 - y1)
                    
                    if area < self.min_person_area:
                        continue
                    
                    # ê±°ë¦¬ ì¶”ì •
                    est_dist, dist_info = estimate_distance_advanced(mask_cleaned, ref_height=300, ref_distance=1.0)
                    
                    current_detections.append({
                        'hist': combined_hist,
                        'bbox': bbox,
                        'mask': mask_cleaned,
                        'confidence': confidence,
                        'area': area,
                        'distance': est_dist,
                        'dist_info': dist_info
                    })
            
            # ê°ì§€ëœ ì‚¬ëŒë“¤ì„ ì˜ì—­ í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
            current_detections.sort(key=lambda x: x['area'], reverse=True)
            
            # ì´ë¯¸ ë§¤ì¹­ëœ IDë“¤ì„ ì¶”ì 
            used_ids = set()
            
            # ê° ê°ì§€ëœ ì‚¬ëŒì— ëŒ€í•´ ë§¤ì¹­ ìˆ˜í–‰
            for detection in current_detections:
                combined_hist = detection['hist']
                bbox = detection['bbox']
                
                # ë§¤ì¹­ ì‹œë„
                matched_id, match_score, metrics = self.find_best_match(combined_hist, bbox, used_ids, elapsed_time)
                
                # ë§¤ì¹­ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬
                if matched_id is not None and match_score > self.match_threshold:
                    # ê¸°ì¡´ ì‚¬ëŒ ì—…ë°ì´íŠ¸
                    self.people_data[matched_id]['histograms'].append(combined_hist)
                    self.people_data[matched_id]['bboxes'].append(bbox)
                    self.people_data[matched_id]['timestamps'].append(elapsed_time)
                    used_ids.add(matched_id)
                    
                    # íˆìŠ¤í† ê·¸ë¨ ë©”ëª¨ë¦¬ ê´€ë¦¬
                    if len(self.people_data[matched_id]['histograms']) > self.max_histograms_per_person:
                        self.people_data[matched_id]['histograms'].pop(0)
                        self.people_data[matched_id]['bboxes'].pop(0)
                        self.people_data[matched_id]['timestamps'].pop(0)
                    
                    color = (0, 255, 0)  # ì´ˆë¡ìƒ‰ (ê¸°ì¡´ ì‚¬ëŒ)
                    print(f"ğŸ”„ ê¸°ì¡´ ì‚¬ëŒ ì¬ì‹ë³„: {matched_id} (ì ìˆ˜: {match_score:.3f})")
                    print(f"   - íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜: {metrics['hist_score']:.3f}")
                    print(f"   - ê³µê°„ì  ì ìˆ˜: {metrics['spatial_score']:.3f}")
                    print(f"   - í¬ê¸° ì ìˆ˜: {metrics['size_score']:.3f}")
                    print(f"   - ì‹œê°„ ì ìˆ˜: {metrics['time_score']:.3f}")
                    print(f"   - ì €ì¥ëœ íˆìŠ¤í† ê·¸ë¨ ìˆ˜: {len(self.people_data[matched_id]['histograms'])}")
                    
                else:
                    # ìƒˆë¡œìš´ ì‚¬ëŒ (ë§¤ì¹­ ì‹¤íŒ¨ ë˜ëŠ” ì„ê³„ê°’ ë¯¸ë‹¬)
                    new_id = f"Person_{self.next_id}"
                    self.people_data[new_id] = {
                        'histograms': [combined_hist],
                        'bboxes': [bbox],
                        'timestamps': [elapsed_time],
                        'images': []
                    }
                    self.next_id += 1
                    used_ids.add(new_id)
                    
                    color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰ (ìƒˆë¡œìš´ ì‚¬ëŒ)
                    print(f"ğŸ†• ìƒˆë¡œìš´ ì‚¬ëŒ ê°ì§€: {new_id}")
                    if matched_id is not None:
                        print(f"   - ìµœê³  ë§¤ì¹­ ì ìˆ˜: {match_score:.3f} (ì„ê³„ê°’: {self.match_threshold:.3f})")
                        print(f"   - ë§¤ì¹­ëœ ID: {matched_id}")
                        print(f"   - íˆìŠ¤í† ê·¸ë¨ ì ìˆ˜: {metrics['hist_score']:.3f}")
                    else:
                        print(f"   - ë§¤ì¹­ ê°€ëŠ¥í•œ ì‚¬ëŒ ì—†ìŒ")
                        print(f"   - ë§¤ì¹­ ì‹¤íŒ¨ë¡œ ì¸í•œ ìƒˆë¡œìš´ ì‚¬ëŒ ë“±ë¡")
                
                # ì‹œê°í™”
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
                
                # ID í‘œì‹œ
                id_text = matched_id if matched_id else new_id
                cv2.putText(annotated, id_text, (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                
                # Confidence í‘œì‹œ
                conf_text = f"Conf: {detection['confidence']:.2f}"
                cv2.putText(annotated, conf_text, (x1, y2+20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ê±°ë¦¬ í‘œì‹œ
                distance_text = f"Dist: {detection['distance']:.2f}m"
                cv2.putText(annotated, distance_text, (x1, y2+40), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                
                # ë°€ë„ ì •ë³´ í‘œì‹œ
                if detection['dist_info']:
                    density_text = f"Density: {detection['dist_info']['density']:.2f}"
                    cv2.putText(annotated, density_text, (x1, y2+60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
                score_text = f"{match_score:.3f}"
                cv2.putText(annotated, score_text, (x1, y2+80), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            
            # ì œìŠ¤ì²˜ ì •ë³´ í‘œì‹œ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ëœ ê²°ê³¼)
            gesture_text = f"Gesture: {current_gesture}"
            cv2.putText(annotated, gesture_text, (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            
            gesture_conf_text = f"Gesture Conf: {current_gesture_confidence:.2f}"
            cv2.putText(annotated, gesture_conf_text, (10, 150), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            
            # ì œìŠ¤ì²˜ ë²„í¼ ìƒíƒœ í‘œì‹œ
            buffer_text = f"Buffer: {len(self.gesture_prediction_buffer)}/{self.gesture_buffer_threshold}"
            cv2.putText(annotated, buffer_text, (10, 180), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
            info_text = f"People: {len(self.people_data)} | Frame: {frame_count} | Time: {elapsed_time:.1f}s"
            cv2.putText(annotated, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # ì œìŠ¤ì²˜ ê°€ì´ë“œ (ë” ëª…í™•í•˜ê²Œ)
            cv2.putText(annotated, 'COME: Move hand quickly', (10, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(annotated, 'STOP: Make a fist', (10, 420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(annotated, 'AWAY: Open palm clearly', (10, 440), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            cv2.putText(annotated, f'Threshold: {self.gesture_confidence_threshold}', (10, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
            
            # ì„±ëŠ¥ ìµœì í™”: 5í”„ë ˆì„ë§ˆë‹¤ë§Œ í™”ë©´ ì—…ë°ì´íŠ¸
            if frame_count % 5 == 0:
                cv2.imshow("Integrated Person + Gesture Recognition", annotated)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
        
        cap.release()
        cv2.destroyAllWindows()
        
        # ë¶„ì„ ê²°ê³¼ ìƒì„± ë° ì €ì¥
        print(f"\nğŸ“Š í†µí•© ë¶„ì„ ì™„ë£Œ!")
        print(f"   - ì´ í”„ë ˆì„: {frame_count}")
        print(f"   - ê°ì§€ëœ ì‚¬ëŒ ìˆ˜: {len(self.people_data)}")
        print(f"   - ìµœì¢… ì œìŠ¤ì²˜ ì¸ì‹: {current_gesture} (ì‹ ë¢°ë„: {current_gesture_confidence:.2f})")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.save_analysis_results(frame_count, gesture_history, start_time)
    
    def save_analysis_results(self, frame_count, gesture_history, start_time):
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ë¶„ì„ ê²°ê³¼ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = os.path.join(self.analysis_dir, f"analysis_results_{timestamp}.txt")
            
            with open(results_file, 'w', encoding='utf-8') as f:
                f.write("=== í†µí•© ì‚¬ëŒ ì¬ì‹ë³„ + ì œìŠ¤ì²˜ ì¸ì‹ ë¶„ì„ ê²°ê³¼ ===\n\n")
                f.write(f"ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ì´ í”„ë ˆì„: {frame_count}\n")
                f.write(f"ê°ì§€ëœ ì‚¬ëŒ ìˆ˜: {len(self.people_data)}\n")
                f.write(f"ì œìŠ¤ì²˜ ì¸ì‹ íšŸìˆ˜: {len(gesture_history)}\n\n")
                
                # ì‚¬ëŒë³„ ì •ë³´
                f.write("=== ì‚¬ëŒë³„ ì •ë³´ ===\n")
                for pid, pdata in self.people_data.items():
                    f.write(f"\n{pid}:\n")
                    f.write(f"  - ê°ì§€ íšŸìˆ˜: {len(pdata['histograms'])}\n")
                    f.write(f"  - ì²« ê°ì§€: {pdata['timestamps'][0]:.1f}ì´ˆ\n")
                    f.write(f"  - ë§ˆì§€ë§‰ ê°ì§€: {pdata['timestamps'][-1]:.1f}ì´ˆ\n")
                
                # ì œìŠ¤ì²˜ íˆìŠ¤í† ë¦¬
                if gesture_history:
                    f.write("\n=== ì œìŠ¤ì²˜ ì¸ì‹ íˆìŠ¤í† ë¦¬ ===\n")
                    for i, gesture in enumerate(gesture_history):
                        f.write(f"{i+1}. {gesture['gesture']} (ì‹ ë¢°ë„: {gesture['confidence']:.2f}) - {gesture['timestamp']:.1f}ì´ˆ\n")
                
                # ì œìŠ¤ì²˜ í†µê³„
                if gesture_history:
                    gestures = [g['gesture'] for g in gesture_history]
                    from collections import Counter
                    gesture_counts = Counter(gestures)
                    f.write("\n=== ì œìŠ¤ì²˜ í†µê³„ ===\n")
                    for gesture, count in gesture_counts.items():
                        f.write(f"  - {gesture}: {count}íšŒ\n")
            
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {results_file}")
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    system = IntegratedPersonGestureSystem()
    system.run_integrated_system(camera_device="/dev/video2") 
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class HandGestureDataset(Dataset):
    """ì† ì œìŠ¤ì²˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ì œìŠ¤ì²˜ íŠ¹ì„± ë°˜ì˜)"""
    def __init__(self, data_paths, labels):
        self.data_paths = data_paths
        self.labels = labels
        self.samples = []
        
        # ê° íŒŒì¼ì—ì„œ ì‹œí€€ìŠ¤ë“¤ì„ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„ë¦¬
        for file_path, label in zip(data_paths, labels):
            data = np.load(file_path)
            # data shape: (num_sequences, 60, 100)
            for i in range(data.shape[0]):
                self.samples.append((data[i], label))  # (sequence, label)
        
    def __len__(self):
        return len(self.samples)
    
    def extract_gesture_features(self, data):
        """ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ"""
        # ê¸°ë³¸ íŠ¹ì§•: ëœë“œë§ˆí¬ + ê°ë„
        landmarks = data[:, :84]  # 21*4 = 84
        angles = data[:, 84:99]   # 15
        
        # 1. ë™ì  íŠ¹ì„± (ì›€ì§ì„ ë¶„ì„)
        motion_features = []
        for i in range(1, len(data)):
            # ì´ì „ í”„ë ˆì„ê³¼ì˜ ëœë“œë§ˆí¬ ë³€í™”ëŸ‰
            landmark_diff = landmarks[i] - landmarks[i-1]
            motion_features.append(landmark_diff)
        
        if len(motion_features) > 0:
            motion_features = np.array(motion_features)
            # ì›€ì§ì„ í†µê³„: í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’
            motion_mean = np.mean(motion_features, axis=0)
            motion_std = np.std(motion_features, axis=0)
            motion_max = np.max(np.abs(motion_features), axis=0)
        else:
            motion_mean = np.zeros(84)
            motion_std = np.zeros(84)
            motion_max = np.zeros(84)
        
        # 2. ì† ëª¨ì–‘ íŠ¹ì„± (ì£¼ë¨¹ vs ì†ë°”ë‹¥)
        # ì†ê°€ë½ ëì ë“¤ (8, 12, 16, 20)ê³¼ ì†ë°”ë‹¥ ì¤‘ì‹¬ì (0)ì˜ ê±°ë¦¬
        finger_tips = [8, 12, 16, 20]  # ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ìƒˆë¼ ì†ê°€ë½ ë
        palm_center = 0  # ì†ë°”ë‹¥ ì¤‘ì‹¬
        
        hand_shape_features = []
        for frame in landmarks:
            frame_reshaped = frame.reshape(-1, 4)  # (21, 4)
            
            # ì†ê°€ë½ ëì ë“¤ì˜ ì†ë°”ë‹¥ ì¤‘ì‹¬ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
            finger_distances = []
            for tip_idx in finger_tips:
                tip_pos = frame_reshaped[tip_idx][:3]  # x, y, z
                palm_pos = frame_reshaped[palm_center][:3]
                distance = np.linalg.norm(tip_pos - palm_pos)
                finger_distances.append(distance)
            
            # ì†ê°€ë½ë“¤ì´ í´ì ¸ìˆìœ¼ë©´ ê±°ë¦¬ê°€ ë©€ê³ , ì£¼ë¨¹ì´ë©´ ê±°ë¦¬ê°€ ê°€ê¹Œì›€
            hand_shape_features.append(finger_distances)
        
        hand_shape_features = np.array(hand_shape_features)
        hand_shape_mean = np.mean(hand_shape_features, axis=0)
        hand_shape_std = np.std(hand_shape_features, axis=0)
        
        # 3. ì œìŠ¤ì²˜ë³„ íŠ¹ì„± ë²¡í„°
        # come: ë™ì  + ì†ë°”ë‹¥
        # stop: ì •ì  + ì£¼ë¨¹  
        # away: ì •ì  + ì†ë°”ë‹¥
        gesture_specific = np.concatenate([
            motion_mean, motion_std, motion_max,  # ë™ì  íŠ¹ì„± (84*3 = 252)
            hand_shape_mean, hand_shape_std       # ì† ëª¨ì–‘ íŠ¹ì„± (4*2 = 8)
        ])
        
        return gesture_specific
    
    def __getitem__(self, idx):
        # ê°œë³„ ì‹œí€€ìŠ¤ì™€ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
        sequence, label = self.samples[idx]
        
        # ë°ì´í„° í˜•íƒœ: (60, 100) - 60í”„ë ˆì„, 100íŠ¹ì§• (99 + 1ë¼ë²¨)
        frames = sequence.shape[0]  # 60
        features = sequence.shape[1] - 1  # 99 (ë¼ë²¨ ì œì™¸)
        
        # ì…ë ¥ ë°ì´í„°ì™€ ë¼ë²¨ ë¶„ë¦¬
        x = sequence[:, :-1].astype(np.float32)  # íŠ¹ì§• ë°ì´í„° (60, 99)
        y = int(sequence[0, -1])  # ë¼ë²¨ (ì²« ë²ˆì§¸ í”„ë ˆì„ì˜ ë¼ë²¨ ì‚¬ìš©)
        
        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ
        gesture_features = self.extract_gesture_features(x)
        
        # ê¸°ë³¸ íŠ¹ì§• + ì œìŠ¤ì²˜ íŠ¹ì„± ê²°í•©
        # ê¸°ë³¸: 60í”„ë ˆì„ Ã— 99íŠ¹ì§•
        # ì œìŠ¤ì²˜ íŠ¹ì„±: 260 (252 + 8)
        # ìµœì¢…: 60í”„ë ˆì„ Ã— 99íŠ¹ì§• + 260 ì œìŠ¤ì²˜ íŠ¹ì„±
        x_with_gesture = np.concatenate([
            x.flatten(),  # 60*99 = 5940
            gesture_features  # 260
        ])
        
        return torch.FloatTensor(x_with_gesture), torch.LongTensor([y])

class GestureCNN(nn.Module):
    """ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN)"""
    def __init__(self, num_classes=3, num_frames=60, num_joints=21):
        super(GestureCNN, self).__init__()
        
        self.num_classes = num_classes
        self.num_frames = num_frames  # 60í”„ë ˆì„ (2ì´ˆ)
        self.num_joints = num_joints
        
        # ì…ë ¥ íŠ¹ì§• ê³„ì‚°
        self.basic_features = 99  # ëœë“œë§ˆí¬(84) + ê°ë„(15)
        self.gesture_features = 260  # ë™ì (252) + ì†ëª¨ì–‘(8)
        self.total_features = self.basic_features * num_frames + self.gesture_features  # 5940 + 260 = 6200
        
        # 1. ê¸°ë³¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (60í”„ë ˆì„ Ã— 99íŠ¹ì§•)
        self.sequence_encoder = nn.Sequential(
            nn.Linear(self.basic_features * num_frames, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # 2. ì œìŠ¤ì²˜ íŠ¹ì„± ì²˜ë¦¬
        self.gesture_encoder = nn.Sequential(
            nn.Linear(self.gesture_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 3. ê²°í•©ëœ íŠ¹ì§• ì²˜ë¦¬
        self.combined_encoder = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        # x shape: (batch_size, total_features)
        batch_size = x.size(0)
        
        # ê¸°ë³¸ ì‹œí€€ìŠ¤ íŠ¹ì§• ì¶”ì¶œ (ì²˜ìŒ 5940ê°œ)
        sequence_features = x[:, :self.basic_features * self.num_frames]
        sequence_encoded = self.sequence_encoder(sequence_features)
        
        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ (ë‚˜ë¨¸ì§€ 260ê°œ)
        gesture_features = x[:, self.basic_features * self.num_frames:]
        gesture_encoded = self.gesture_encoder(gesture_features)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([sequence_encoded, gesture_encoded], dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.combined_encoder(combined_features)
        
        return output

def load_dataset(data_dir):
    """ë°ì´í„°ì…‹ ë¡œë“œ"""
    print("ğŸ“ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
    seq_files = glob.glob(os.path.join(data_dir, 'seq_*.npy'))
    
    if not seq_files:
        print("âš ï¸ ì‹œí€€ìŠ¤ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ì‹œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ì›ì‹œ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        raw_files = glob.glob(os.path.join(data_dir, 'raw_*.npy'))
        if not raw_files:
            raise FileNotFoundError("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        data_files = raw_files
    else:
        data_files = seq_files
    
    print(f"ğŸ“Š ë°œê²¬ëœ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
    
    # íŒŒì¼ë³„ë¡œ ë¼ë²¨ ì¶”ì¶œ
    data_paths = []
    labels = []
    
    for file_path in data_files:
        filename = os.path.basename(file_path)
        
        # ë¼ë²¨ ì¶”ì¶œ (come=0, away=1, stop=2)
        if 'come' in filename:
            label = 0
        elif 'away' in filename:
            label = 1
        elif 'stop' in filename:
            label = 2
        else:
            continue
            
        data_paths.append(file_path)
        labels.append(label)
    
    print(f"ğŸ“ˆ í´ë˜ìŠ¤ë³„ ë°ì´í„° ìˆ˜:")
    for i, action in enumerate(['come', 'away', 'stop']):
        count = labels.count(i)
        print(f"  {action}: {count}ê°œ")
    
    return data_paths, labels

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):
    """ëª¨ë¸ í•™ìŠµ"""
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    best_val_acc = 0.0
    
    for epoch in range(num_epochs):
        # í•™ìŠµ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.squeeze().to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x = batch_x.to(device)
                batch_y = batch_y.squeeze().to(device)
                
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()
        
        # í†µê³„ ê³„ì‚°
        train_loss = train_loss / len(train_loader)
        val_loss = val_loss / len(val_loader)
        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f'Epoch [{epoch+1}/{num_epochs}] - '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_gesture_cnn_model.pth')
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (ê²€ì¦ ì •í™•ë„: {val_acc:.2f}%)")
    
    return train_losses, val_losses, train_accuracies, val_accuracies

def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):
    """í•™ìŠµ íˆìŠ¤í† ë¦¬ í”Œë¡¯"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ì†ì‹¤ ê·¸ë˜í”„
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Validation Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # ì •í™•ë„ ê·¸ë˜í”„
    ax2.plot(train_accuracies, label='Train Accuracy')
    ax2.plot(val_accuracies, label='Validation Accuracy')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def evaluate_model(model, test_loader):
    """ëª¨ë¸ í‰ê°€"""
    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.squeeze().to(device)
            
            outputs = model(batch_x)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(batch_y.cpu().numpy())
    
    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_score(all_labels, all_predictions)
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(all_labels, all_predictions, 
                              target_names=['come', 'away', 'stop']))
    
    return accuracy

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ í•™ìŠµ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN)")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    data_paths, labels = load_dataset('dataset')
    
    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°:")
    for i, action in enumerate(['come', 'away', 'stop']):
        count = labels.count(i)
        print(f"  {action}: {count}ê°œ")
    
    # ë°ì´í„°ì…‹ ìƒì„± (ì‹œí€€ìŠ¤ë“¤ì´ ê°œë³„ ìƒ˜í”Œë¡œ ë¶„ë¦¬ë¨)
    train_dataset = HandGestureDataset(data_paths, labels)
    
    # ì „ì²´ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    total_samples = len(train_dataset)
    print(f"ğŸ“Š ì´ ì‹œí€€ìŠ¤ ìƒ˜í”Œ: {total_samples}ê°œ")
    
    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    class_counts = [0, 0, 0]
    for _, label in train_dataset.samples:
        class_counts[label] += 1
    
    print(f"ğŸ“ˆ í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ìˆ˜:")
    for i, action in enumerate(['come', 'away', 'stop']):
        print(f"  {action}: {class_counts[i]}ê°œ")
    
    # ë°ì´í„° ë¶„í•  (stratify ì‚¬ìš© ê°€ëŠ¥)
    all_samples = list(range(total_samples))
    all_labels = [label for _, label in train_dataset.samples]
    
    X_train, X_temp, y_train, y_temp = train_test_split(
        all_samples, all_labels, test_size=0.3, random_state=42, stratify=all_labels
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    print(f"ğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"  í•™ìŠµ: {len(X_train)}ê°œ")
    print(f"  ê²€ì¦: {len(X_val)}ê°œ")
    print(f"  í…ŒìŠ¤íŠ¸: {len(X_test)}ê°œ")
    
    # ì„œë¸Œì…‹ ë°ì´í„°ì…‹ ìƒì„±
    class SubsetDataset(Dataset):
        def __init__(self, full_dataset, indices):
            self.full_dataset = full_dataset
            self.indices = indices
            
        def __len__(self):
            return len(self.indices)
            
        def __getitem__(self, idx):
            return self.full_dataset[self.indices[idx]]
    
    train_subset = SubsetDataset(train_dataset, X_train)
    val_subset = SubsetDataset(train_dataset, X_val)
    test_subset = SubsetDataset(train_dataset, X_test)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    batch_size = 32  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)
    
    # ëª¨ë¸ ìƒì„± (ì œìŠ¤ì²˜ íŠ¹ì„± ë°˜ì˜)
    model = GestureCNN(num_classes=3, num_frames=60, num_joints=21)
    model = model.to(device)
    
    print(f"ğŸ“ˆ ëª¨ë¸ êµ¬ì¡° (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN):")
    print(f"  - ê¸°ë³¸ íŠ¹ì§•: 60í”„ë ˆì„ Ã— 99íŠ¹ì§• (5940)")
    print(f"  - ì œìŠ¤ì²˜ íŠ¹ì„±: 260 (ë™ì  252 + ì†ëª¨ì–‘ 8)")
    print(f"  - ì´ ì…ë ¥: 6200 íŠ¹ì§•")
    print(f"  - ì¶œë ¥: 3ê°œ í´ë˜ìŠ¤ (come, away, stop)")
    print(f"  - ì œìŠ¤ì²˜ë³„ íŠ¹ì„±:")
    print(f"    â€¢ come: ë™ì  + ì†ë°”ë‹¥ (ì›€ì§ì„ ë§ìŒ)")
    print(f"    â€¢ stop: ì •ì  + ì£¼ë¨¹ (ì›€ì§ì„ ì ìŒ, ì£¼ë¨¹ ëª¨ì–‘)")
    print(f"    â€¢ away: ì •ì  + ì†ë°”ë‹¥ (ì›€ì§ì„ ì ìŒ, ì†ë°”ë‹¥ í´ì§)")
    print(model)
    
    # ëª¨ë¸ í•™ìŠµ
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(
        model, train_loader, val_loader, num_epochs=50, learning_rate=0.001
    )
    
    # í•™ìŠµ íˆìŠ¤í† ë¦¬ í”Œë¡¯
    plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies)
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    if os.path.exists('best_gesture_cnn_model.pth'):
        model.load_state_dict(torch.load('best_gesture_cnn_model.pth'))
        
        # ëª¨ë¸ í‰ê°€
        test_accuracy = evaluate_model(model, test_loader)
        
        print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"  - best_gesture_cnn_model.pth (ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸)")
        print(f"  - training_history.png (í•™ìŠµ íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„)")
        print(f"ğŸ’¡ ì œìŠ¤ì²˜ íŠ¹ì„± ë¶„ì„:")
        print(f"  - ë™ì  íŠ¹ì„±: ì›€ì§ì„ íŒ¨í„´ ë¶„ì„")
        print(f"  - ì† ëª¨ì–‘: ì£¼ë¨¹ vs ì†ë°”ë‹¥ êµ¬ë¶„")
        print(f"  - ì‹¤ì‹œê°„ ì¸ì‹: 2ì´ˆ ë‹¨ìœ„ë¡œ ì •í™•í•œ ì œìŠ¤ì²˜ íŒë‹¨")
    else:
        print("âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 
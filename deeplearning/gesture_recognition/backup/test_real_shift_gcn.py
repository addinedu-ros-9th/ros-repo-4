import torch
import torch.nn as nn
import cv2
import mediapipe as mp
import numpy as np
from collections import deque
import time

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ì§„ì§œ Shift-GCN ëª¨ë¸ í´ë˜ìŠ¤ (í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼)
class ShiftGCNLayer(nn.Module):
    """Shift-GCN ë ˆì´ì–´"""
    def __init__(self, in_channels, out_channels, adjacency_matrix, num_adj=8):
        super(ShiftGCNLayer, self).__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_adj = num_adj
        
        # ì¸ì ‘ í–‰ë ¬ ë¶„í• 
        A_list = self._split_adjacency_matrix(adjacency_matrix)
        # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        self.register_buffer('A_list', torch.stack(A_list))
        
        # ê° ë¶„í• ì— ëŒ€í•œ ì»¨ë³¼ë£¨ì…˜
        self.conv_list = nn.ModuleList([
            nn.Conv2d(in_channels, out_channels // num_adj, 
                     kernel_size=1, bias=False)
            for _ in range(num_adj)
        ])
        
        # ì”ì°¨ ì—°ê²°
        if in_channels != out_channels:
            self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.residual = nn.Identity()
        
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        
    def _split_adjacency_matrix(self, A):
        """ì¸ì ‘ í–‰ë ¬ì„ ì—¬ëŸ¬ ê°œë¡œ ë¶„í• """
        A_list = []
        
        # ê¸°ë³¸ ì¸ì ‘ í–‰ë ¬
        A_list.append(A)
        
        # ê±°ë¦¬ì— ë”°ë¥¸ ì¸ì ‘ í–‰ë ¬ë“¤ (1-hop, 2-hop, ...)
        A_power = A.clone()
        for _ in range(self.num_adj - 1):
            A_power = torch.mm(A_power, A)
            A_list.append(A_power)
        
        return A_list
    
    def forward(self, x):
        # x shape: (batch_size, in_channels, num_frames, num_joints)
        batch_size, in_channels, num_frames, num_joints = x.size()
        
        # Shift-GCN ì—°ì‚°
        out_list = []
        for i in range(self.num_adj):
            A = self.A_list[i]  # (num_joints, num_joints)
            conv = self.conv_list[i]
            
            # ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ - einsum ì‚¬ìš©
            # x: (batch_size, in_channels, num_frames, num_joints)
            # A: (num_joints, num_joints)
            # einsum('bcfj,jk->bcfk', x, A): (batch_size, in_channels, num_frames, num_joints)
            graph_conv = torch.einsum('bcfj,jk->bcfk', x, A)
            conv_out = conv(graph_conv)
            out_list.append(conv_out)
        
        # ê²°ê³¼ ê²°í•©
        out = torch.cat(out_list, dim=1)  # (batch_size, out_channels, num_frames, num_joints)
        
        # ì”ì°¨ ì—°ê²°
        residual = self.residual(x)
        out = out + residual
        
        # BatchNormê³¼ ReLU
        out = self.bn(out)
        out = self.relu(out)
        
        return out

class RealShiftGCN(nn.Module):
    """ì§„ì§œ Shift-GCN ëª¨ë¸ êµ¬í˜„"""
    def __init__(self, num_classes=3, num_frames=60, num_joints=21, num_features=3):
        super(RealShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_frames = num_frames
        self.num_joints = num_joints
        self.num_features = num_features  # x, y, z
        
        # ì† ê´€ì ˆ ê·¸ë˜í”„ êµ¬ì¡° ì •ì˜ (MediaPipe Hands ê¸°ì¤€)
        # 21ê°œ ê´€ì ˆì˜ ì—°ê²° ê´€ê³„
        self.hand_connections = [
            # ì—„ì§€
            (0, 1), (1, 2), (2, 3), (3, 4),
            # ê²€ì§€
            (0, 5), (5, 6), (6, 7), (7, 8),
            # ì¤‘ì§€
            (0, 9), (9, 10), (10, 11), (11, 12),
            # ì•½ì§€
            (0, 13), (13, 14), (14, 15), (15, 16),
            # ìƒˆë¼
            (0, 17), (17, 18), (18, 19), (19, 20),
            # ì†ë°”ë‹¥ ì—°ê²°
            (5, 9), (9, 13), (13, 17)
        ]
        
        # ì¸ì ‘ í–‰ë ¬ ìƒì„±
        self.register_buffer('A', self._build_adjacency_matrix())
        
        # Shift-GCN ë ˆì´ì–´ë“¤
        self.gcn_layers = nn.ModuleList([
            ShiftGCNLayer(3, 64, self.A),    # 3 -> 64
            ShiftGCNLayer(64, 128, self.A),  # 64 -> 128
            ShiftGCNLayer(128, 256, self.A), # 128 -> 256
        ])
        
        # Temporal CNN
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Conv1d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm1d(512)
        )
        
        # Global Average Pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
        
    def _build_adjacency_matrix(self):
        """ì¸ì ‘ í–‰ë ¬ ìƒì„±"""
        A = torch.zeros(self.num_joints, self.num_joints)
        for i, j in self.hand_connections:
            A[i, j] = 1
            A[j, i] = 1  # ë¬´ë°©í–¥ ê·¸ë˜í”„
        # ìê¸° ìì‹ ê³¼ì˜ ì—°ê²°
        A += torch.eye(self.num_joints)
        return A
    
    def forward(self, x):
        # x shape: (batch_size, num_frames, num_joints, num_features)
        batch_size, num_frames, num_joints, num_features = x.size()
        
        # (batch_size, num_frames, num_joints, num_features) -> (batch_size, num_features, num_frames, num_joints)
        x = x.permute(0, 3, 1, 2)  # (batch_size, 3, num_frames, num_joints)
        
        # GCN ë ˆì´ì–´ë“¤ í†µê³¼
        gcn_out = x
        for gcn_layer in self.gcn_layers:
            gcn_out = gcn_layer(gcn_out)
        
        # Temporal modeling
        # (batch_size, 256, num_frames, num_joints) -> (batch_size, 256, num_frames)
        gcn_out = gcn_out.mean(dim=3)  # ê´€ì ˆ ì°¨ì› í‰ê· 
        
        # Temporal CNN
        temporal_out = self.temporal_conv(gcn_out)
        
        # Global pooling
        pooled = self.global_pool(temporal_out)  # (batch_size, 512, 1)
        pooled = pooled.squeeze(-1)  # (batch_size, 512)
        
        # Classification
        output = self.classifier(pooled)
        
        return output

def calculate_angles(landmarks):
    """ê´€ì ˆ ê°ë„ ê³„ì‚°"""
    angles = []
    
    # ì†ê°€ë½ ê´€ì ˆ ê°ë„ë“¤ (15ê°œ)
    finger_joints = [
        # ì—„ì§€
        (0, 1, 2), (1, 2, 3), (2, 3, 4),
        # ê²€ì§€
        (0, 5, 6), (5, 6, 7), (6, 7, 8),
        # ì¤‘ì§€
        (0, 9, 10), (9, 10, 11), (10, 11, 12),
        # ì•½ì§€
        (0, 13, 14), (13, 14, 15), (14, 15, 16),
        # ìƒˆë¼
        (0, 17, 18), (17, 18, 19), (18, 19, 20)
    ]
    
    for joint in finger_joints:
        p1, p2, p3 = joint
        
        # 3D ë²¡í„° ê³„ì‚°
        v1 = landmarks[p1] - landmarks[p2]
        v2 = landmarks[p3] - landmarks[p2]
        
        # ê°ë„ ê³„ì‚°
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        cos_angle = np.clip(cos_angle, -1.0, 1.0)
        angle = np.degrees(np.arccos(cos_angle))
        
        angles.append(angle)
    
    return np.array(angles)

def process_frame_for_prediction(frame_data):
    """í”„ë ˆì„ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
    frames = len(frame_data)
    
    if frames < 60:
        # ë¶€ì¡±í•œ í”„ë ˆì„ì€ 0ìœ¼ë¡œ íŒ¨ë”©
        padding = np.zeros((60 - frames, 21, 3), dtype=np.float32)
        x = np.vstack([frame_data, padding])
    elif frames > 60:
        # ì´ˆê³¼í•˜ëŠ” í”„ë ˆì„ì€ ì¤‘ì•™ì—ì„œ 60ê°œ ì¶”ì¶œ
        start = (frames - 60) // 2
        x = frame_data[start:start + 60]
    else:
        x = frame_data
    
    # (60, 21, 3) -> (1, 60, 21, 3) - ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    x = torch.FloatTensor(x).unsqueeze(0)
    
    return x

def main():
    """ì§„ì§œ Shift-GCN ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ì§„ì§œ Shift-GCN ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    print("ğŸ’¡ ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ + Shift ì—°ì‚°ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸")
    
    # ëª¨ë¸ ë¡œë“œ
    model = RealShiftGCN(num_classes=3, num_frames=60, num_joints=21, num_features=3)
    model.load_state_dict(torch.load('best_real_shift_gcn_model.pth'))
    model = model.to(device)
    model.eval()
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # MediaPipe ì„¤ì •
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    hands = mp_hands.Hands(
        max_num_hands=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    
    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture('/dev/video0')
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # í”„ë ˆì„ ë²„í¼ (60í”„ë ˆì„)
    frame_buffer = deque(maxlen=60)
    prediction_buffer = deque(maxlen=5)  # ì˜ˆì¸¡ ê²°ê³¼ ìŠ¤ë¬´ë”©
    
    # ì œìŠ¤ì²˜ ë¼ë²¨
    gesture_labels = ['come', 'away', 'stop']
    
    print("ğŸ¥ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ ì‹œì‘ (q: ì¢…ë£Œ)")
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # ì¢Œìš° ë°˜ì „
        frame = cv2.flip(frame, 1)
        
        # BGR -> RGB ë³€í™˜
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(rgb_frame)
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
                # ëœë“œë§ˆí¬ ì¢Œí‘œ ì¶”ì¶œ
                landmarks = []
                for landmark in hand_landmarks.landmark:
                    landmarks.append([landmark.x, landmark.y, landmark.z])
                
                landmarks = np.array(landmarks)
                
                # í”„ë ˆì„ ë²„í¼ì— ì¶”ê°€
                frame_buffer.append(landmarks)
        
        # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
        if len(frame_buffer) >= 30:  # ìµœì†Œ 30í”„ë ˆì„
            try:
                # í”„ë ˆì„ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
                frame_data = list(frame_buffer)
                x = process_frame_for_prediction(frame_data)
                x = x.to(device)
                
                # ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():
                    outputs = model(x)
                    probabilities = torch.softmax(outputs, dim=1)
                    predicted_class = torch.argmax(outputs, dim=1).item()
                    confidence = probabilities[0][predicted_class].item()
                
                # ì˜ˆì¸¡ ê²°ê³¼ ë²„í¼ì— ì¶”ê°€
                prediction_buffer.append(predicted_class)
                
                # ìµœë¹ˆê°’ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
                if len(prediction_buffer) >= 3:
                    final_prediction = max(set(prediction_buffer), key=prediction_buffer.count)
                    gesture = gesture_labels[final_prediction]
                    
                    # ê²°ê³¼ í‘œì‹œ
                    cv2.putText(frame, f"Gesture: {gesture}", (10, 30), 
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    cv2.putText(frame, f"Confidence: {confidence:.2f}", (10, 70), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(frame, f"Buffer: {len(frame_buffer)}/60", (10, 100), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
                    
            except Exception as e:
                print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
        
        # í”„ë ˆì„ í‘œì‹œ
        cv2.imshow('Real Shift-GCN Gesture Recognition', frame)
        
        # q í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ì •ë¦¬
    cap.release()
    cv2.destroyAllWindows()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 
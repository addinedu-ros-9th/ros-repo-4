"""
ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë™ì˜ìƒ íŒŒì¼ë¡œ ì €ì¥
- `RealtimeGesturePredictor`ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
- ì²˜ë¦¬ëœ ê° í”„ë ˆì„ì„ `cv2.VideoWriter`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒìœ¼ë¡œ ì €ì¥
- ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸
"""

import cv2
import numpy as np
import torch
import torch.nn.functional as F
from ultralytics import YOLO
import os
import glob
from collections import deque
from train_sliding_shift_gcn import SlidingShiftGCN

class RealtimeGesturePredictor:
    """ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ë¶„ë¥˜ê¸° (predict_realtime.pyì—ì„œ ê°€ì ¸ì˜´)"""
    
    def __init__(self, model_path, yolo_path='yolov8n-pose.pt'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        self.yolo_model = YOLO(yolo_path)
        if torch.cuda.is_available():
            self.yolo_model.to(self.device)
        
        self.joint_indices = [0, 5, 6, 7, 8, 9, 10, 11, 12]
        self.num_joints = len(self.joint_indices)
        
        self.load_shift_gcn_model(model_path)
        
        self.window_size = 30
        self.keypoint_buffer = deque(maxlen=self.window_size)
        
        self.action_labels = {0: 'COME', 1: 'NORMAL'}
        self.colors = {0: (0, 255, 0), 1: (0, 0, 255)}
        self.last_prediction = None
        
        print("âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_shift_gcn_model(self, model_path):
        adj_matrix_path = os.path.join(os.path.dirname(model_path), '../shift_gcn_data_sliding/adjacency_matrix.npy')
        adjacency_matrix = np.load(adj_matrix_path) if os.path.exists(adj_matrix_path) else np.eye(self.num_joints)
        
        self.model = SlidingShiftGCN(num_classes=2, num_joints=self.num_joints, dropout=0.3)
        self.model.set_adjacency_matrix(adjacency_matrix)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def normalize_keypoints(self, keypoints):
        normalized = keypoints.copy()
        valid_joints = keypoints[:, 2] > 0
        if not np.any(valid_joints): return None
        valid_points = keypoints[valid_joints, :2]
        center = np.mean(valid_points, axis=0)
        scale = np.std(valid_points)
        if scale == 0: scale = 1.0
        normalized[:, :2] = (keypoints[:, :2] - center) / scale
        return normalized

    def predict_gesture(self):
        if len(self.keypoint_buffer) < self.window_size:
            return None, 0.0
        
        window_data = np.array(list(self.keypoint_buffer))
        shift_gcn_input = window_data.transpose(2, 0, 1)[np.newaxis, :, :, :, np.newaxis]
        
        with torch.no_grad():
            input_tensor = torch.FloatTensor(shift_gcn_input).to(self.device)
            outputs = self.model(input_tensor)
            probabilities = F.softmax(outputs, dim=1).squeeze()
            prediction = torch.argmax(probabilities).item()
            confidence = probabilities[prediction].item()
        return prediction, confidence

    def draw_visualization(self, frame, keypoints, prediction, confidence):
        if keypoints is not None:
            connections = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6), (1, 7), (2, 8), (7, 8)]
            for i, j in connections:
                idx1, idx2 = self.joint_indices[i], self.joint_indices[j]
                if keypoints[idx1, 2] > 0.3 and keypoints[idx2, 2] > 0.3:
                    p1 = (int(keypoints[idx1, 0]), int(keypoints[idx1, 1]))
                    p2 = (int(keypoints[idx2, 0]), int(keypoints[idx2, 1]))
                    cv2.line(frame, p1, p2, (255, 255, 255), 2)
            for idx in self.joint_indices:
                if keypoints[idx, 2] > 0.3:
                    center = (int(keypoints[idx, 0]), int(keypoints[idx, 1]))
                    cv2.circle(frame, center, 5, (0, 255, 255), -1)

        if prediction is None:
            text = f"Collecting frames... [{len(self.keypoint_buffer)}/{self.window_size}]"
            cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            label = self.action_labels[prediction]
            color = self.colors[prediction]
            text = f"{label}: {confidence:.2f}"
            cv2.rectangle(frame, (10, 10), (350, 60), color, -1)
            cv2.putText(frame, text, (20, 45), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
            if self.last_prediction != prediction:
                print(f"--> ì˜ˆì¸¡ ë³€ê²½: {label} (ì‹ ë¢°ë„: {confidence:.2f})")
                self.last_prediction = prediction
        return frame

    def process_frame(self, frame):
        results = self.yolo_model(frame, verbose=False)
        all_keypoints = None
        if results[0].keypoints is not None and len(results[0].keypoints.data) > 0:
            all_keypoints = results[0].keypoints.data[0].cpu().numpy()
            selected_keypoints = all_keypoints[self.joint_indices]
            if np.all(selected_keypoints[:, 2] > 0.3):
                normalized = self.normalize_keypoints(selected_keypoints)
                if normalized is not None:
                    self.keypoint_buffer.append(normalized)
        
        prediction, confidence = self.predict_gesture()
        frame = self.draw_visualization(frame, all_keypoints, prediction, confidence)
        return frame

def save_prediction_to_video(predictor, video_path, output_path):
    """ì˜ˆì¸¡ ê³¼ì •ì„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        return

    # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ë° VideoWriter ì„¤ì •
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    print(f"\nğŸ¬ ì˜ìƒ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)}")
    print(f"   -> ì €ì¥ ê²½ë¡œ: {output_path}")

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        processed_frame = predictor.process_frame(frame)
        out.write(processed_frame)
        frame_count += 1

    cap.release()
    out.release()
    predictor.keypoint_buffer.clear() # ë‹¤ìŒ ì˜ìƒì„ ìœ„í•´ ë²„í¼ ì´ˆê¸°í™”
    print(f"âœ… ì˜ìƒ ì €ì¥ ì™„ë£Œ ({frame_count} í”„ë ˆì„)")

if __name__ == "__main__":
    model_path = "./models/sliding_shift_gcn_model.pth"
    output_dir = "./visualization_results/all_videos"  # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ í•˜ìœ„ í´ë” ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    predictor = RealtimeGesturePredictor(model_path)
    
    # ì²˜ë¦¬í•  ëª¨ë“  ë¹„ë””ì˜¤ ì°¾ê¸°
    all_videos = sorted(glob.glob("./pose_dataset/**/*.avi", recursive=True))

    if not all_videos:
        print("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… ì´ {len(all_videos)}ê°œì˜ ë¹„ë””ì˜¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        for video_path in all_videos:
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì • (í•˜ìœ„ í´ë” êµ¬ì¡° ìœ ì§€)
            relative_path = os.path.relpath(video_path, "./pose_dataset")
            output_filename = f"predicted_{os.path.splitext(relative_path.replace(os.sep, '_'))[0]}.mp4"
            output_path = os.path.join(output_dir, output_filename)
            
            save_prediction_to_video(predictor, video_path, output_path)

        print("\nğŸ‰ ëª¨ë“  ì˜ìƒ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì €ì¥ í´ë”: {output_dir}") 
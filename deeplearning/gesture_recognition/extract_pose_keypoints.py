"""
YOLO Poseë¥¼ ì‚¬ìš©í•œ ê´€ì ˆì  ì¶”ì¶œ ë° Shift-GCN ë°ì´í„° ìƒì„±

Shift-GCN ë°ì´í„° í˜•ì‹:
- Input shape: (C, T, V, M)
  - C: ì±„ë„ (3 - x, y, confidence)
  - T: ì‹œê°„ í”„ë ˆì„ ìˆ˜
  - V: ê´€ì ˆì  ìˆ˜ (ì–´ê¹¨ë¶€í„° ì†ê¹Œì§€ ì„ íƒ ê°€ëŠ¥)
  - M: ì‚¬ëŒ ìˆ˜ (ê¸°ë³¸ 1)

COCO 17 ê´€ì ˆì  ì¸ë±ìŠ¤ (YOLO Pose ê¸°ì¤€):
0: nose, 1: left_eye, 2: right_eye, 3: left_ear, 4: right_ear,
5: left_shoulder, 6: right_shoulder, 7: left_elbow, 8: right_elbow,
9: left_wrist, 10: right_wrist, 11: left_hip, 12: right_hip,
13: left_knee, 14: right_knee, 15: left_ankle, 16: right_ankle

ì–´ê¹¨ë¶€í„° ì†ê¹Œì§€: [5, 6, 7, 8, 9, 10] (6ê°œ ê´€ì ˆì )
ìƒì²´ ì „ì²´: [0, 5, 6, 7, 8, 9, 10, 11, 12] (9ê°œ ê´€ì ˆì )
"""

import cv2
import numpy as np
import os
import glob
from ultralytics import YOLO
import time
from pathlib import Path

class YOLOPoseExtractor:
    """YOLO Poseë¥¼ ì‚¬ìš©í•œ ê´€ì ˆì  ì¶”ì¶œê¸°"""
    
    def __init__(self, model_path='yolov8n-pose.pt', target_joints='upper_body'):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: YOLO Pose ëª¨ë¸ ê²½ë¡œ
            target_joints: ì¶”ì¶œí•  ê´€ì ˆì  ë²”ìœ„
                - 'arms_only': ì–´ê¹¨ë¶€í„° ì†ê¹Œì§€ (6ê°œ)
                - 'upper_body': ìƒì²´ ì „ì²´ (9ê°œ)
                - 'full_body': ì „ì‹  (17ê°œ)
        """
        self.model = YOLO(model_path)
        self.target_joints = target_joints
        
        # ê´€ì ˆì  ì¸ë±ìŠ¤ ì •ì˜
        self.joint_indices = {
            'arms_only': [5, 6, 7, 8, 9, 10],  # ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©
            'upper_body': [0, 5, 6, 7, 8, 9, 10, 11, 12],  # ì½”, ì–´ê¹¨, íŒ”ê¿ˆì¹˜, ì†ëª©, ì—‰ë©ì´
            'full_body': list(range(17))  # ì „ì‹ 
        }
        
        # ê´€ì ˆì  ì—°ê²° ì •ì˜ (Shift-GCN adjacency matrixìš©)
        self.edges = {
            'arms_only': [
                (0, 1),  # left_shoulder - right_shoulder
                (0, 2),  # left_shoulder - left_elbow
                (1, 3),  # right_shoulder - right_elbow
                (2, 4),  # left_elbow - left_wrist
                (3, 5),  # right_elbow - right_wrist
            ],
            'upper_body': [
                (0, 1), (0, 2),  # nose - shoulders
                (1, 2),  # shoulders
                (1, 3), (2, 4),  # shoulders - elbows
                (3, 5), (4, 6),  # elbows - wrists
                (1, 7), (2, 8),  # shoulders - hips
                (7, 8),  # hips
            ],
            'full_body': [
                (0, 1), (0, 2), (1, 3), (2, 4),  # head
                (5, 6), (5, 7), (6, 8), (7, 9), (8, 10),  # arms
                (5, 11), (6, 12), (11, 12),  # torso
                (11, 13), (12, 14), (13, 15), (14, 16)  # legs
            ]
        }
        
        self.selected_indices = self.joint_indices[target_joints]
        self.num_joints = len(self.selected_indices)
        
        print(f"âœ… YOLO Pose ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ëª¨ë¸: {model_path}")
        print(f"   - ê´€ì ˆì  ë²”ìœ„: {target_joints} ({self.num_joints}ê°œ)")
        print(f"   - ì„ íƒëœ ê´€ì ˆì : {self.selected_indices}")
    
    def extract_keypoints_from_video(self, video_path):
        """
        ë¹„ë””ì˜¤ì—ì„œ ê´€ì ˆì  ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            keypoints: (T, V, C) í˜•íƒœì˜ ê´€ì ˆì  ë°ì´í„°
            valid_frames: ìœ íš¨í•œ í”„ë ˆì„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
            return None, []
        
        keypoints_sequence = []
        valid_frames = []
        frame_idx = 0
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # YOLO Pose ì¶”ë¡ 
            results = self.model(frame, verbose=False)
            
            if results[0].keypoints is not None and len(results[0].keypoints.data) > 0:
                # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ ê´€ì ˆì ë§Œ ì‚¬ìš© (ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê²ƒ)
                all_keypoints = results[0].keypoints.data[0].cpu().numpy()  # (17, 3)
                
                # ì„ íƒëœ ê´€ì ˆì ë§Œ ì¶”ì¶œ
                selected_keypoints = all_keypoints[self.selected_indices]  # (V, 3)
                
                # ì‹ ë¢°ë„ ì²´í¬ (ëª¨ë“  ê´€ì ˆì ì˜ ì‹ ë¢°ë„ê°€ 0.3 ì´ìƒ)
                if np.all(selected_keypoints[:, 2] > 0.3):
                    keypoints_sequence.append(selected_keypoints)
                    valid_frames.append(frame_idx)
                else:
                    # ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° ì´ì „ í”„ë ˆì„ ë³µì‚¬ (ìˆë‹¤ë©´)
                    if len(keypoints_sequence) > 0:
                        keypoints_sequence.append(keypoints_sequence[-1].copy())
                        valid_frames.append(frame_idx)
            else:
                # ê´€ì ˆì ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì´ì „ í”„ë ˆì„ ë³µì‚¬ (ìˆë‹¤ë©´)
                if len(keypoints_sequence) > 0:
                    keypoints_sequence.append(keypoints_sequence[-1].copy())
                    valid_frames.append(frame_idx)
            
            frame_idx += 1
        
        cap.release()
        
        if len(keypoints_sequence) == 0:
            print(f"âŒ ê´€ì ˆì  ì¶”ì¶œ ì‹¤íŒ¨: {video_path}")
            return None, []
        
        keypoints_array = np.array(keypoints_sequence)  # (T, V, 3)
        print(f"âœ… ê´€ì ˆì  ì¶”ì¶œ ì™„ë£Œ: {video_path}")
        print(f"   - ì´ í”„ë ˆì„: {frame_idx}")
        print(f"   - ìœ íš¨ í”„ë ˆì„: {len(valid_frames)}")
        print(f"   - ê´€ì ˆì  shape: {keypoints_array.shape}")
        
        return keypoints_array, valid_frames
    
    def normalize_keypoints(self, keypoints):
        """
        ê´€ì ˆì  ì •ê·œí™”
        
        Args:
            keypoints: (T, V, 3) í˜•íƒœì˜ ê´€ì ˆì  ë°ì´í„°
            
        Returns:
            normalized_keypoints: ì •ê·œí™”ëœ ê´€ì ˆì  ë°ì´í„°
        """
        if keypoints is None:
            return None
        
        normalized_keypoints = keypoints.copy()
        
        for t in range(keypoints.shape[0]):
            frame_keypoints = keypoints[t]  # (V, 3)
            
            # ìœ íš¨í•œ ê´€ì ˆì ë§Œ ì‚¬ìš© (confidence > 0)
            valid_joints = frame_keypoints[:, 2] > 0
            
            if np.any(valid_joints):
                valid_points = frame_keypoints[valid_joints]
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‰ê· )
                center_x = np.mean(valid_points[:, 0])
                center_y = np.mean(valid_points[:, 1])
                
                # ìŠ¤ì¼€ì¼ ê³„ì‚° (ìœ íš¨í•œ ê´€ì ˆì ë“¤ì˜ í‘œì¤€í¸ì°¨)
                scale = np.std(valid_points[:, :2])
                if scale == 0:
                    scale = 1.0
                
                # ì •ê·œí™” ì ìš©
                normalized_keypoints[t, :, 0] = (frame_keypoints[:, 0] - center_x) / scale
                normalized_keypoints[t, :, 1] = (frame_keypoints[:, 1] - center_y) / scale
                # confidenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                
        return normalized_keypoints
    
    def create_adjacency_matrix(self):
        """
        Shift-GCNìš© adjacency matrix ìƒì„±
        
        Returns:
            adjacency_matrix: (V, V) í˜•íƒœì˜ ì¸ì ‘ í–‰ë ¬
        """
        A = np.zeros((self.num_joints, self.num_joints))
        
        # self-connection
        for i in range(self.num_joints):
            A[i, i] = 1
        
        # joint connections
        edges = self.edges[self.target_joints]
        for i, j in edges:
            A[i, j] = 1
            A[j, i] = 1
        
        return A
    
    def convert_to_shift_gcn_format(self, keypoints, target_frames=64):
        """
        Shift-GCN ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            keypoints: (T, V, 3) í˜•íƒœì˜ ê´€ì ˆì  ë°ì´í„°
            target_frames: ëª©í‘œ í”„ë ˆì„ ìˆ˜ (ì‹œê°„ ì •ê·œí™”)
            
        Returns:
            shift_gcn_data: (C, T, V, M) í˜•íƒœì˜ ë°ì´í„°
        """
        if keypoints is None:
            return None
        
        T, V, C = keypoints.shape
        M = 1  # ì‚¬ëŒ ìˆ˜
        
        # ì‹œê°„ ì •ê·œí™” (target_framesë¡œ ë¦¬ìƒ˜í”Œë§)
        if T != target_frames:
            # ì„ í˜• ë³´ê°„ì„ ì‚¬ìš©í•œ ì‹œê°„ ì •ê·œí™”
            old_indices = np.linspace(0, T-1, T)
            new_indices = np.linspace(0, T-1, target_frames)
            
            resampled_keypoints = np.zeros((target_frames, V, C))
            for v in range(V):
                for c in range(C):
                    resampled_keypoints[:, v, c] = np.interp(new_indices, old_indices, keypoints[:, v, c])
            
            keypoints = resampled_keypoints
        
        # (T, V, C) -> (C, T, V, M) ë³€í™˜
        shift_gcn_data = np.zeros((C, target_frames, V, M))
        shift_gcn_data[:, :, :, 0] = keypoints.transpose(2, 0, 1)  # (C, T, V)
        
        return shift_gcn_data

def process_gesture_videos(video_dir, output_dir, target_joints='upper_body', target_frames=64):
    """
    ì œìŠ¤ì²˜ ë¹„ë””ì˜¤ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ Shift-GCN ë°ì´í„° ìƒì„±
    
    Args:
        video_dir: ë¹„ë””ì˜¤ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        target_joints: ì¶”ì¶œí•  ê´€ì ˆì  ë²”ìœ„
        target_frames: ëª©í‘œ í”„ë ˆì„ ìˆ˜
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # YOLO Pose ì¶”ì¶œê¸° ì´ˆê¸°í™”
    extractor = YOLOPoseExtractor(target_joints=target_joints)
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    video_extensions = ['*.mp4', '*.avi', '*.mov']
    video_files = []
    for ext in video_extensions:
        video_files.extend(glob.glob(os.path.join(video_dir, '**', ext), recursive=True))
    
    print(f"ğŸ“¹ ì°¾ì€ ë¹„ë””ì˜¤ íŒŒì¼: {len(video_files)}ê°œ")
    
    if len(video_files) == 0:
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_dir}")
        return
    
    # ì•¡ì…˜ë³„ ë°ì´í„° ìˆ˜ì§‘
    action_data = {}
    
    for video_path in video_files:
        print(f"\nğŸ” ì²˜ë¦¬ ì¤‘: {os.path.basename(video_path)}")
        
        # ì•¡ì…˜ëª… ì¶”ì¶œ (íŒŒì¼ëª… ë˜ëŠ” í´ë”ëª…ì—ì„œ)
        path_parts = Path(video_path).parts
        action_name = None
        
        # í´ë”ëª…ì—ì„œ ì•¡ì…˜ ì°¾ê¸° (come, normal ë“±)
        for part in reversed(path_parts):
            if part.lower() in ['come', 'normal']:
                action_name = part.lower()
                break
        
        # íŒŒì¼ëª…ì—ì„œ ì•¡ì…˜ ì°¾ê¸°
        if action_name is None:
            filename = os.path.basename(video_path).lower()
            if 'come' in filename:
                action_name = 'come'
            elif 'normal' in filename:
                action_name = 'normal'
        
        if action_name is None:
            print(f"âš ï¸ ì•¡ì…˜ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            continue
        
        # ê´€ì ˆì  ì¶”ì¶œ
        keypoints, valid_frames = extractor.extract_keypoints_from_video(video_path)
        
        if keypoints is not None:
            # ì •ê·œí™”
            normalized_keypoints = extractor.normalize_keypoints(keypoints)
            
            # Shift-GCN í˜•íƒœë¡œ ë³€í™˜
            shift_gcn_data = extractor.convert_to_shift_gcn_format(normalized_keypoints, target_frames)
            
            if action_name not in action_data:
                action_data[action_name] = []
            
            action_data[action_name].append({
                'data': shift_gcn_data,
                'video_path': video_path,
                'valid_frames': len(valid_frames)
            })
    
    # ë°ì´í„° ì €ì¥
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
    
    for action_name, samples in action_data.items():
        print(f"ğŸ“Š {action_name}: {len(samples)}ê°œ ìƒ˜í”Œ")
        
        # ì•¡ì…˜ë³„ ë°ì´í„° í•©ì¹˜ê¸°
        all_data = []
        labels = []
        
        for i, sample in enumerate(samples):
            all_data.append(sample['data'])
            labels.append(action_name)
        
        if len(all_data) > 0:
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            data_array = np.array(all_data)  # (N, C, T, V, M)
            
            # ì €ì¥
            data_file = os.path.join(output_dir, f'{action_name}_pose_data.npy')
            labels_file = os.path.join(output_dir, f'{action_name}_labels.npy')
            
            np.save(data_file, data_array)
            np.save(labels_file, labels)
            
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {data_file}")
            print(f"   - Shape: {data_array.shape}")
    
    # ì¸ì ‘ í–‰ë ¬ ì €ì¥
    adjacency_matrix = extractor.create_adjacency_matrix()
    adj_file = os.path.join(output_dir, 'adjacency_matrix.npy')
    np.save(adj_file, adjacency_matrix)
    print(f"âœ… ì¸ì ‘ í–‰ë ¬ ì €ì¥: {adj_file}")
    print(f"   - Shape: {adjacency_matrix.shape}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'target_joints': target_joints,
        'num_joints': extractor.num_joints,
        'joint_indices': extractor.selected_indices,
        'target_frames': target_frames,
        'actions': list(action_data.keys()),
        'total_samples': sum(len(samples) for samples in action_data.values())
    }
    
    metadata_file = os.path.join(output_dir, 'metadata.npy')
    np.save(metadata_file, metadata)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")

if __name__ == "__main__":
    # ì´ë¯¸ ì°ì€ ì˜ìƒì— ë§ëŠ” ì„¤ì •
    video_dir = "./deeplearning/gesture_recognition/pose_dataset"
    output_dir = "./deeplearning/gesture_recognition/shift_gcn_data"
    
    # ì´ë¯¸ ì°ì€ ì˜ìƒ ì„¤ì • (ìˆ˜ì • ë¶ˆê°€)
    RECORDING_TIME = 3.0    # ì´ë¯¸ ì°ì€ ì˜ìƒ: 3ì´ˆ
    ESTIMATED_FPS = 30.0    # ì˜ˆìƒ FPS
    ESTIMATED_FRAMES = int(RECORDING_TIME * ESTIMATED_FPS)  # ì•½ 90 í”„ë ˆì„
    
    # Shift-GCN ì˜µì…˜
    TARGET_FRAMES_OPTIONS = {
        'use_original': ESTIMATED_FRAMES,  # 90 í”„ë ˆì„ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
        'standard_64': 64,                 # 64 í”„ë ˆì„ (ì¼ë°˜ì )
        'standard_128': 128                # 128 í”„ë ˆì„ (ê³ í’ˆì§ˆ)
    }
    
    # ì„ íƒ: ì›ë³¸ í”„ë ˆì„ ìˆ˜ ì‚¬ìš© vs í‘œì¤€ í”„ë ˆì„ ìˆ˜ ì‚¬ìš©
    SELECTED_OPTION = 'use_original'  # 'use_original', 'standard_64', 'standard_128' ì¤‘ ì„ íƒ
    TARGET_FRAMES = TARGET_FRAMES_OPTIONS[SELECTED_OPTION]
    
    print("ğŸš€ YOLO Pose ê¸°ë°˜ Shift-GCN ë°ì´í„° ìƒì„± ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“¹ ì´ë¯¸ ì°ì€ ì˜ìƒ ì •ë³´:")
    print(f"   - ë…¹í™” ì‹œê°„: {RECORDING_TIME}ì´ˆ")
    print(f"   - ì˜ˆìƒ FPS: {ESTIMATED_FPS}")
    print(f"   - ì˜ˆìƒ í”„ë ˆì„: {ESTIMATED_FRAMES}")
    print(f"ğŸ¯ Shift-GCN ì„¤ì •:")
    print(f"   - ì„ íƒëœ ì˜µì…˜: {SELECTED_OPTION}")
    print(f"   - ëª©í‘œ í”„ë ˆì„: {TARGET_FRAMES}")
    
    if TARGET_FRAMES != ESTIMATED_FRAMES:
        change_percent = ((TARGET_FRAMES - ESTIMATED_FRAMES) / ESTIMATED_FRAMES) * 100
        print(f"   - í”„ë ˆì„ ë³€í™”: {change_percent:+.1f}%")
        if abs(change_percent) > 20:
            print(f"   âš ï¸ í° ë³€í™” ê°ì§€! ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„± ìˆìŒ")
        else:
            print(f"   âœ… ì ì ˆí•œ ë³€í™” ë²”ìœ„")
    else:
        print(f"   âœ… ì›ë³¸ í”„ë ˆì„ ìˆ˜ ìœ ì§€ (ë³€í™” ì—†ìŒ)")
    
    # ìƒì²´ ê´€ì ˆì ìœ¼ë¡œ ë°ì´í„° ìƒì„±
    process_gesture_videos(
        video_dir=video_dir,
        output_dir=output_dir,
        target_joints='upper_body',  # 'arms_only', 'upper_body', 'full_body' ì¤‘ ì„ íƒ
        target_frames=TARGET_FRAMES  # ì´ë¯¸ ì°ì€ ì˜ìƒì— ë§ê²Œ ì¡°ì •
    )
    
    print("\nğŸ‰ Shift-GCN ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ’¡ Tip: ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ target_framesë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”") 
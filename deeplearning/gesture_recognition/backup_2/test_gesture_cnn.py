import cv2
import mediapipe as mp
import numpy as np
import torch
import torch.nn as nn
from collections import deque

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ê¸°ì¡´ ëª¨ë¸ í´ë˜ìŠ¤ (í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼)
class GestureCNN(nn.Module):
    """ì œìŠ¤ì²˜ ì¸ì‹ CNN ëª¨ë¸ (íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ + CNN)"""
    def __init__(self, num_classes=3, num_frames=60, num_joints=21):
        super(GestureCNN, self).__init__()
        
        self.num_classes = num_classes
        self.num_frames = num_frames
        self.num_joints = num_joints
        
        # ì…ë ¥ íŠ¹ì§• ê³„ì‚°
        self.basic_features = 99  # ëœë“œë§ˆí¬(84) + ê°ë„(15)
        self.gesture_features = 260  # ë™ì (252) + ì†ëª¨ì–‘(8)
        self.total_features = self.basic_features * num_frames + self.gesture_features
        
        # 1. ê¸°ë³¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
        self.sequence_encoder = nn.Sequential(
            nn.Linear(self.basic_features * num_frames, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # 2. ì œìŠ¤ì²˜ íŠ¹ì„± ì²˜ë¦¬
        self.gesture_encoder = nn.Sequential(
            nn.Linear(self.gesture_features, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # 3. ê²°í•©ëœ íŠ¹ì§• ì²˜ë¦¬
        self.combined_encoder = nn.Sequential(
            nn.Linear(512 + 64, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        
        # ê¸°ë³¸ ì‹œí€€ìŠ¤ íŠ¹ì§• ì¶”ì¶œ
        sequence_features = x[:, :self.basic_features * self.num_frames]
        sequence_encoded = self.sequence_encoder(sequence_features)
        
        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ
        gesture_features = x[:, self.basic_features * self.num_frames:]
        gesture_encoded = self.gesture_encoder(gesture_features)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([sequence_encoded, gesture_encoded], dim=1)
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.combined_encoder(combined_features)
        
        return output

def extract_gesture_features(data):
    """ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ (í•™ìŠµ ì‹œì™€ ë™ì¼)"""
    landmarks = data[:, :84]
    angles = data[:, 84:99]
    
    # 1. ë™ì  íŠ¹ì„±
    motion_features = []
    for i in range(1, len(data)):
        landmark_diff = landmarks[i] - landmarks[i-1]
        motion_features.append(landmark_diff)
    
    if len(motion_features) > 0:
        motion_features = np.array(motion_features)
        motion_mean = np.mean(motion_features, axis=0)
        motion_std = np.std(motion_features, axis=0)
        motion_max = np.max(np.abs(motion_features), axis=0)
    else:
        motion_mean = np.zeros(84)
        motion_std = np.zeros(84)
        motion_max = np.zeros(84)
    
    # 2. ì† ëª¨ì–‘ íŠ¹ì„±
    finger_tips = [8, 12, 16, 20]
    palm_center = 0
    
    hand_shape_features = []
    for frame in landmarks:
        frame_reshaped = frame.reshape(-1, 4)
        finger_distances = []
        for tip_idx in finger_tips:
            tip_pos = frame_reshaped[tip_idx][:3]
            palm_pos = frame_reshaped[palm_center][:3]
            distance = np.linalg.norm(tip_pos - palm_pos)
            finger_distances.append(distance)
        hand_shape_features.append(finger_distances)
    
    hand_shape_features = np.array(hand_shape_features)
    hand_shape_mean = np.mean(hand_shape_features, axis=0)
    hand_shape_std = np.std(hand_shape_features, axis=0)
    
    # 3. ì œìŠ¤ì²˜ë³„ íŠ¹ì„± ë²¡í„°
    gesture_specific = np.concatenate([
        motion_mean, motion_std, motion_max,
        hand_shape_mean, hand_shape_std
    ])
    
    return gesture_specific

def main():
    """ê°„ë‹¨í•œ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê°„ë‹¨í•œ ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    print("ğŸ’¡ ëª¨ë¸ì´ í•™ìŠµí•œ íŒ¨í„´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
    
    # ëª¨ë¸ ë¡œë“œ
    model = GestureCNN(num_classes=3, num_frames=60, num_joints=21)
    model.load_state_dict(torch.load('best_gesture_cnn_model.pth'))
    model = model.to(device)
    model.eval()
    
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # MediaPipe ì„¤ì •
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    hands = mp_hands.Hands(
        max_num_hands=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    
    # ì¹´ë©”ë¼ ì„¤ì •
    cap = cv2.VideoCapture('/dev/video0')
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # í”„ë ˆì„ ë²„í¼
    frame_buffer = deque(maxlen=60)
    
    # ì œìŠ¤ì²˜ ë¼ë²¨
    actions = ['COME', 'AWAY', 'STOP']
    
    # ì˜ˆì¸¡ ê²°ê³¼ ë²„í¼
    prediction_buffer = deque(maxlen=5)
    
    print("ğŸ“¹ ì¹´ë©”ë¼ ì‹œì‘...")
    print("ğŸ’¡ ì œìŠ¤ì²˜ë¥¼ í•´ë³´ì„¸ìš”:")
    print("   - COME: ì†ì„ ê¹Œë”±ê¹Œë”± ì›€ì§ì´ê¸°")
    print("   - STOP: ì£¼ë¨¹ ì¥ê¸°")
    print("   - AWAY: ì†ë°”ë‹¥ í´ê¸°")
    print("   - q: ì¢…ë£Œ, r: ë¦¬ì…‹")
    
    while cap.isOpened():
        ret, img = cap.read()
        if not ret:
            break
            
        img = cv2.flip(img, 1)
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)
        img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
        
        prediction = "WAITING..."
        confidence = 0.0
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
                # ëœë“œë§ˆí¬ ë°ì´í„° ì¶”ì¶œ
                joint = np.zeros((21, 4))
                for j, lm in enumerate(hand_landmarks.landmark):
                    joint[j] = [lm.x, lm.y, lm.z, lm.visibility]
                
                # ê°ë„ ê³„ì‚°
                v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :3]
                v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :3]
                v = v2 - v1
                v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]
                
                angle = np.arccos(np.einsum('nt,nt->n',
                    v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:], 
                    v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))
                angle = np.degrees(angle)
                
                # íŠ¹ì§• ë²¡í„° ìƒì„±
                d = np.concatenate([joint.flatten(), angle])
                frame_buffer.append(d)
                
                # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
                if len(frame_buffer) >= 30:
                    try:
                        # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
                        frame_data = np.array(list(frame_buffer))
                        
                        # í”„ë ˆì„ ìˆ˜ë¥¼ 60ìœ¼ë¡œ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
                        frames = len(frame_data)
                        if frames < 60:
                            padding = np.zeros((60 - frames, 99), dtype=np.float32)
                            x = np.vstack([frame_data, padding])
                        elif frames > 60:
                            start = (frames - 60) // 2
                            x = frame_data[start:start + 60]
                        else:
                            x = frame_data
                        
                        # ì œìŠ¤ì²˜ íŠ¹ì„± ì¶”ì¶œ
                        gesture_features = extract_gesture_features(x)
                        
                        # ê¸°ë³¸ íŠ¹ì§• + ì œìŠ¤ì²˜ íŠ¹ì„± ê²°í•©
                        x_with_gesture = np.concatenate([
                            x.flatten(),
                            gesture_features
                        ])
                        
                        model_input = torch.FloatTensor(x_with_gesture).unsqueeze(0).to(device)
                        
                        # ì˜ˆì¸¡
                        with torch.no_grad():
                            outputs = model(model_input)
                            probabilities = torch.softmax(outputs, dim=1)
                            predicted = torch.argmax(outputs, dim=1).item()
                            confidence = probabilities[0][predicted].item()
                        
                        prediction = actions[predicted]
                        prediction_buffer.append(predicted)
                        
                    except Exception as e:
                        print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                        prediction = "ERROR"
        
        # ë¶€ë“œëŸ¬ìš´ ì˜ˆì¸¡ ê²°ê³¼
        if len(prediction_buffer) >= 3:
            from collections import Counter
            most_common = Counter(prediction_buffer).most_common(1)[0]
            final_prediction = actions[most_common[0]]
            final_confidence = most_common[1] / len(prediction_buffer)
        else:
            final_prediction = prediction
            final_confidence = confidence
        
        # í™”ë©´ì— ì •ë³´ í‘œì‹œ
        cv2.putText(img, f'Prediction: {final_prediction}', 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(img, f'Confidence: {final_confidence:.2f}', 
                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
        cv2.putText(img, f'Buffer: {len(frame_buffer)}/60', 
                   (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ì œìŠ¤ì²˜ ê°€ì´ë“œ
        cv2.putText(img, 'COME: Move hand', (10, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(img, 'STOP: Fist', (10, 420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(img, 'AWAY: Open palm', (10, 440), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        cv2.imshow('Simple Gesture Recognition', img)
        
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('r'):
            frame_buffer.clear()
            prediction_buffer.clear()
            print("ğŸ”„ ë²„í¼ ë¦¬ì…‹")
    
    cap.release()
    cv2.destroyAllWindows()
    print("âœ… í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")

if __name__ == "__main__":
    main() 
"""
Sliding Window ë°ì´í„°ì— ìµœì í™”ëœ Shift-GCN
- 30í”„ë ˆì„ ì§§ì€ ì‹œí€€ìŠ¤ì— ìµœì í™”
- ë§ì€ ìƒ˜í”Œ ìˆ˜ (3000+)ì— ë§ëŠ” ì„¤ì •
- ì•ˆì •ì ì¸ í•™ìŠµ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import glob
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

class SlidingShiftGCN(nn.Module):
    """30í”„ë ˆì„ ì‹œí€€ìŠ¤ì— ìµœì í™”ëœ Shift-GCN"""
    
    def __init__(self, num_classes, num_joints, in_channels=3, dropout=0.3):
        super(SlidingShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_joints = num_joints
        
        # Adjacency matrix
        self.register_buffer('A', torch.eye(num_joints))
        
        # ì…ë ¥ ì •ê·œí™”
        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)
        
        # ì§§ì€ ì‹œí€€ìŠ¤ì— ë§ëŠ” ì–•ì€ ë„¤íŠ¸ì›Œí¬
        self.conv1 = nn.Conv2d(in_channels, 64, 1)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.conv2 = nn.Conv2d(64, 128, 1) 
        self.bn2 = nn.BatchNorm2d(128)
        
        # Temporal convolution (ì§§ì€ ì‹œí€€ìŠ¤ì— ë§ê²Œ)
        self.tcn1 = nn.Conv2d(128, 128, (3, 1), padding=(1, 0))
        self.tcn_bn1 = nn.BatchNorm2d(128)
        
        self.tcn2 = nn.Conv2d(128, 256, (3, 1), padding=(1, 0))
        self.tcn_bn2 = nn.BatchNorm2d(256)
        
        # Classifier
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(256, num_classes)
        
    def set_adjacency_matrix(self, A):
        self.A = torch.FloatTensor(A)
        if next(self.parameters()).is_cuda:
            self.A = self.A.cuda()
    
    def forward(self, x):
        N, C, T, V, M = x.size()
        
        # Focus on first person
        x = x[:, :, :, :, 0]  # (N, C, T, V)
        
        # Data normalization
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, V, T)
        x = x.view(N, C * V, T)
        x = self.data_bn(x)
        x = x.view(N, C, V, T)
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, T, V)
        
        # Graph convolution layers
        # Layer 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Layer 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Apply graph adjacency
        x = torch.einsum('nctv,vw->nctw', x, self.A)
        
        # Temporal convolutions
        x = self.tcn1(x)
        x = self.tcn_bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        x = self.tcn2(x)
        x = self.tcn_bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Global pooling
        x = F.avg_pool2d(x, x.size()[2:])  # (N, 256, 1, 1)
        x = x.view(N, -1)  # (N, 256)
        
        # Classification
        x = self.fc(x)
        
        return x

class SlidingGestureDataset(Dataset):
    """Sliding Window ì œìŠ¤ì²˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_dir, split='train', test_size=0.2, random_state=42):
        self.data_dir = data_dir
        self.split = split
        
        # Load data and labels
        self.data, self.labels, self.action_to_idx = self.load_data()
        
        # Split train/test
        if len(self.data) > 0:
            train_data, test_data, train_labels, test_labels = train_test_split(
                self.data, self.labels, test_size=test_size, random_state=random_state, 
                stratify=self.labels
            )
            
            if split == 'train':
                self.data = train_data
                self.labels = train_labels
            else:
                self.data = test_data
                self.labels = test_labels
        
        print(f"âœ… {split} ë°ì´í„°ì…‹ ë¡œë“œ: {len(self.data)}ê°œ ìƒ˜í”Œ")
    
    def load_data(self):
        all_data = []
        all_labels = []
        action_to_idx = {}
        
        data_files = glob.glob(os.path.join(self.data_dir, '*_pose_data.npy'))
        
        if len(data_files) == 0:
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return [], [], {}
        
        for data_file in data_files:
            action_name = os.path.basename(data_file).replace('_pose_data.npy', '')
            
            if action_name not in action_to_idx:
                action_to_idx[action_name] = len(action_to_idx)
            
            action_data = np.load(data_file)
            action_labels = [action_to_idx[action_name]] * len(action_data)
            
            all_data.extend(action_data)
            all_labels.extend(action_labels)
            
            print(f"ğŸ“Š {action_name}: {len(action_data)}ê°œ ìƒ˜í”Œ, shape: {action_data.shape}")
        
        return np.array(all_data), np.array(all_labels), action_to_idx
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data = torch.FloatTensor(self.data[idx])
        label = torch.LongTensor([self.labels[idx]])[0]
        return data, label

def train_sliding_shift_gcn(data_dir, model_save_path, epochs=100, batch_size=32, lr=0.001):
    """Sliding Window Shift-GCN í•™ìŠµ"""
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # Load metadata
    metadata_file = os.path.join(data_dir, 'metadata.npy')
    if os.path.exists(metadata_file):
        metadata = np.load(metadata_file, allow_pickle=True).item()
        num_joints = metadata['num_joints']
        window_size = metadata['window_size']
        total_samples = metadata['total_samples']
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {window_size} í”„ë ˆì„, {total_samples}ê°œ ìƒ˜í”Œ")
    else:
        print("âš ï¸ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
        num_joints = 9
        window_size = 30
    
    # Load adjacency matrix
    adj_file = os.path.join(data_dir, 'adjacency_matrix.npy')
    if os.path.exists(adj_file):
        adjacency_matrix = np.load(adj_file)
        print(f"ğŸ“Š ì¸ì ‘ í–‰ë ¬: {adjacency_matrix.shape}")
    else:
        print("âš ï¸ ì¸ì ‘ í–‰ë ¬ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¨ìœ„ í–‰ë ¬ ì‚¬ìš©")
        adjacency_matrix = np.eye(num_joints)
    
    # Create datasets
    train_dataset = SlidingGestureDataset(data_dir, split='train', test_size=0.2)
    test_dataset = SlidingGestureDataset(data_dir, split='test', test_size=0.2)
    
    if len(train_dataset) == 0:
        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„:")
    print(f"   - í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset)}")
    print(f"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {window_size} í”„ë ˆì„")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Get number of classes
    num_classes = len(train_dataset.action_to_idx)
    print(f"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    print(f"ğŸ“Š ì•¡ì…˜ ë§¤í•‘: {train_dataset.action_to_idx}")
    
    # Create model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SlidingShiftGCN(
        num_classes=num_classes, 
        num_joints=num_joints, 
        dropout=0.3  # ë§ì€ ë°ì´í„°ì— ë§ê²Œ ë‚®ì€ ë“œë¡­ì•„ì›ƒ
    )
    model.set_adjacency_matrix(adjacency_matrix)
    model = model.to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸš€ ëª¨ë¸ ìƒì„±: {device}")
    print(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    
    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)
    
    # Training history
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    best_test_acc = 0.0
    patience_counter = 0
    early_stopping_patience = 20
    
    print(f"\nğŸš€ Sliding Window Shift-GCN í•™ìŠµ ì‹œì‘")
    print(f"   - ì—í¬í¬: {epochs}")
    print(f"   - í•™ìŠµë¥ : {lr}")
    print(f"   - ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience} ì—í¬í¬")
    print("=" * 60)
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, labels) in enumerate(train_loader):
            data, labels = data.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            # ì§„í–‰ë¥  í‘œì‹œ (ëŒ€ëŸ‰ ë°ì´í„°ìš©)
            if (batch_idx + 1) % 50 == 0:
                current_acc = 100.0 * train_correct / train_total
                print(f'  Batch [{batch_idx+1}/{len(train_loader)}] '
                      f'Loss: {loss.item():.4f}, Acc: {current_acc:.2f}%')
        
        train_acc = 100.0 * train_correct / train_total
        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc)
        
        # Testing
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(device), labels.to(device)
                outputs = model(data)
                _, predicted = torch.max(outputs.data, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_acc = 100.0 * test_correct / test_total
        test_accuracies.append(test_acc)
        
        # Update scheduler
        scheduler.step()
        
        # Save best model and early stopping
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), model_save_path)
            patience_counter = 0
        else:
            patience_counter += 1
        
        # Print progress
        print(f'Epoch [{epoch+1}/{epochs}]')
        print(f'  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Test Acc: {test_acc:.2f}%, Best: {best_test_acc:.2f}%')
        print(f'  Patience: {patience_counter}/{early_stopping_patience}')
        print(f'  LR: {scheduler.get_last_lr()[0]:.6f}')
        
        # Early stopping
        if patience_counter >= early_stopping_patience:
            print(f"\nâ¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"   ìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_test_acc:.2f}%")
    print(f"   ì‹¤ì œ ì—í¬í¬: {epoch + 1}/{epochs}")
    
    # Final evaluation
    model.load_state_dict(torch.load(model_save_path))
    model.eval()
    
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Classification report
    idx_to_action = {v: k for k, v in train_dataset.action_to_idx.items()}
    action_names = [idx_to_action[i] for i in range(num_classes)]
    
    print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€:")
    print(classification_report(all_labels, all_predictions, target_names=action_names))
    
    # Save plots
    plots_dir = os.path.dirname(model_save_path)
    
    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=action_names, yticklabels=action_names)
    plt.title('Sliding Window Shift-GCN Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'sliding_confusion_matrix.png'))
    plt.close()
    
    # Training history
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, label='Train', alpha=0.7)
    plt.plot(test_accuracies, label='Test', alpha=0.7)
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'sliding_training_history.png'))
    plt.close()
    
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼ ì €ì¥: {plots_dir}")

if __name__ == "__main__":
    # Sliding window ë°ì´í„° ì„¤ì •
    data_dir = "./shift_gcn_data_sliding"
    model_save_path = "./models/sliding_shift_gcn_model.pth"
    
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    print("ğŸš€ Sliding Window Shift-GCN ì œìŠ¤ì²˜ ì¸ì‹ í•™ìŠµ")
    print("=" * 60)
    
    # ëŒ€ëŸ‰ ë°ì´í„°ì— ë§ëŠ” ì„¤ì •
    train_sliding_shift_gcn(
        data_dir=data_dir,
        model_save_path=model_save_path,
        epochs=100,      # ëŒ€ëŸ‰ ë°ì´í„°ë¡œ ë” ë§ì€ ì—í¬í¬
        batch_size=32,   # ë” í° ë°°ì¹˜ í¬ê¸°
        lr=0.001         # ì•ˆì •ì ì¸ í•™ìŠµë¥ 
    ) 
"""
ë°ì´í„° í¬ê¸°ì— ìµœì í™”ëœ Shift-GCN í•™ìŠµ
- ì‘ì€ ë°ì´í„°ì…‹ì— ë§ì¶˜ ê²½ëŸ‰ ëª¨ë¸
- ê³¼ì í•© ë°©ì§€ ê°•í™”
- ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import glob
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

class LightShiftGraphConv(nn.Module):
    """ê²½ëŸ‰í™”ëœ Shift Graph Convolution Layer"""
    
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dropout=0.5):
        super(LightShiftGraphConv, self).__init__()
        
        self.kernel_size = kernel_size
        self.conv = nn.Conv2d(in_channels, out_channels * kernel_size, 1)
        self.bn = nn.BatchNorm2d(out_channels * kernel_size)  # ì°¨ì› ìˆ˜ì •
        self.dropout = nn.Dropout(dropout)
        
        # ë” ê°•í•œ ì •ê·œí™”ë¥¼ ìœ„í•œ Shift ë¹„ìœ¨ ì¡°ì •
        self.temporal_shift_ratio = 0.25  # 50% â†’ 25% (ê²½ëŸ‰í™”)
        self.spatial_shift_ratio = 0.125   # 25% â†’ 12.5% (ê²½ëŸ‰í™”)
        
    def forward(self, x, A):
        N, C, T, V = x.size()
        
        # Apply shifts
        x = self.apply_shifts(x)
        
        # Graph convolution
        x = self.conv(x)
        
        # Batch normalization before reshape
        x = self.bn(x)
        x = F.relu(x)
        
        x = x.view(N, self.kernel_size, -1, T, V)
        
        # Apply adjacency matrix
        x = torch.einsum('nkctv,vw->nkctw', x, A)
        x = x.contiguous().view(N, -1, T, V)
        
        # Dropout
        x = self.dropout(x)
        
        return x
    
    def apply_shifts(self, x):
        """ê²½ëŸ‰í™”ëœ Shift ì—°ì‚°"""
        N, C, T, V = x.size()
        
        temporal_shift_channels = int(C * self.temporal_shift_ratio)
        spatial_shift_channels = int(C * self.spatial_shift_ratio)
        
        out = x.clone()
        
        # Temporal shift (left and right)
        if temporal_shift_channels > 0 and T > 1:
            # Left shift
            out[:, :temporal_shift_channels//2, 1:, :] = x[:, :temporal_shift_channels//2, :-1, :]
            out[:, :temporal_shift_channels//2, 0, :] = 0
            
            # Right shift
            out[:, temporal_shift_channels//2:temporal_shift_channels, :-1, :] = x[:, temporal_shift_channels//2:temporal_shift_channels, 1:, :]
            out[:, temporal_shift_channels//2:temporal_shift_channels, -1, :] = 0
        
        # Spatial shift
        if spatial_shift_channels > 0 and V > 1:
            start_idx = temporal_shift_channels
            end_idx = start_idx + spatial_shift_channels
            out[:, start_idx:end_idx, :, 1:] = x[:, start_idx:end_idx, :, :-1]
            out[:, start_idx:end_idx, :, 0] = 0
        
        return out

class OptimizedShiftGCN(nn.Module):
    """ì‘ì€ ë°ì´í„°ì…‹ì— ìµœì í™”ëœ Shift-GCN"""
    
    def __init__(self, num_classes, num_joints, in_channels=3, temporal_kernel_size=5, dropout=0.5):
        super(OptimizedShiftGCN, self).__init__()
        
        self.num_classes = num_classes
        self.num_joints = num_joints
        
        # Adjacency matrix
        self.register_buffer('A', torch.eye(num_joints))
        
        # ë” ê°•í•œ ì •ê·œí™”
        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)
        
        # ê²½ëŸ‰í™”ëœ Graph convolution blocks (10ê°œ â†’ 6ê°œ)
        self.l1 = LightShiftGraphConv(in_channels, 32, 3, dropout=dropout)  # 64 â†’ 32
        self.l2 = LightShiftGraphConv(32, 32, 3, dropout=dropout)
        self.l3 = LightShiftGraphConv(32, 64, 3, dropout=dropout)  # stride=2 ì œê±°
        self.l4 = LightShiftGraphConv(64, 64, 3, dropout=dropout)
        self.l5 = LightShiftGraphConv(64, 128, 3, dropout=dropout)
        self.l6 = LightShiftGraphConv(128, 128, 3, dropout=dropout)
        
        # ë” ì‘ì€ Temporal convolution
        self.tcn = nn.Sequential(
            nn.Conv2d(128, 128, (temporal_kernel_size, 1), padding=(temporal_kernel_size//2, 0)),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ë” ê°•í•œ ì •ê·œí™”ë¥¼ ìœ„í•œ ì¤‘ê°„ ì¸µ ì¶”ê°€
        self.fc_intermediate = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, num_classes)
        )
        
    def set_adjacency_matrix(self, A):
        self.A = torch.FloatTensor(A)
        if next(self.parameters()).is_cuda:
            self.A = self.A.cuda()
    
    def forward(self, x):
        N, C, T, V, M = x.size()
        
        # Focus on first person
        x = x[:, :, :, :, 0]  # (N, C, T, V)
        
        # Data normalization
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, V, T)
        x = x.view(N, C * V, T)
        x = self.data_bn(x)
        x = x.view(N, C, V, T)
        x = x.permute(0, 1, 3, 2).contiguous()  # (N, C, T, V)
        
        # Graph convolution layers (ê²½ëŸ‰í™”)
        x = self.l1(x, self.A)
        x = self.l2(x, self.A)
        x = self.l3(x, self.A)
        x = self.l4(x, self.A)
        x = self.l5(x, self.A)
        x = self.l6(x, self.A)
        
        # Temporal convolution
        x = self.tcn(x)
        
        # Global pooling
        x = F.avg_pool2d(x, x.size()[2:])  # (N, 128, 1, 1)
        x = x.view(N, -1)  # (N, 128)
        
        # Classification with intermediate layer
        x = self.fc_intermediate(x)
        
        return x

class GestureDataset(Dataset):
    """ì œìŠ¤ì²˜ ë°ì´í„°ì…‹ - ë°ì´í„° ì¦ê°• ì¶”ê°€"""
    
    def __init__(self, data_dir, split='train', test_size=0.2, random_state=42, augment=True):
        self.data_dir = data_dir
        self.split = split
        self.augment = augment and (split == 'train')  # í›ˆë ¨ ì‹œì—ë§Œ ì¦ê°•
        
        # Load data and labels
        self.data, self.labels, self.action_to_idx = self.load_data()
        
        # Split train/test
        if len(self.data) > 0:
            # Stratified split for balanced classes
            train_data, test_data, train_labels, test_labels = train_test_split(
                self.data, self.labels, test_size=test_size, random_state=random_state, 
                stratify=self.labels
            )
            
            if split == 'train':
                self.data = train_data
                self.labels = train_labels
            else:
                self.data = test_data
                self.labels = test_labels
        
        print(f"âœ… {split} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ ìƒ˜í”Œ")
        if self.augment:
            print(f"   ğŸ”„ ë°ì´í„° ì¦ê°• í™œì„±í™” (í›ˆë ¨ ë°ì´í„°ë§Œ)")
    
    def load_data(self):
        all_data = []
        all_labels = []
        action_to_idx = {}
        
        data_files = glob.glob(os.path.join(self.data_dir, '*_pose_data.npy'))
        
        if len(data_files) == 0:
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            return [], [], {}
        
        for data_file in data_files:
            action_name = os.path.basename(data_file).replace('_pose_data.npy', '')
            
            if action_name not in action_to_idx:
                action_to_idx[action_name] = len(action_to_idx)
            
            action_data = np.load(data_file)
            action_labels = [action_to_idx[action_name]] * len(action_data)
            
            all_data.extend(action_data)
            all_labels.extend(action_labels)
            
            print(f"ğŸ“Š {action_name}: {len(action_data)}ê°œ ìƒ˜í”Œ, shape: {action_data.shape}")
        
        return np.array(all_data), np.array(all_labels), action_to_idx
    
    def augment_data(self, data):
        """ê°„ë‹¨í•œ ë°ì´í„° ì¦ê°•"""
        # Random temporal jittering (ì‹œê°„ì¶• ë³€í˜•)
        if np.random.rand() < 0.3:
            # Random frame dropping
            T = data.shape[1]
            keep_ratio = 0.9
            keep_frames = int(T * keep_ratio)
            indices = np.sort(np.random.choice(T, keep_frames, replace=False))
            data = data[:, indices, :, :]
            
            # Pad back to original length
            if data.shape[1] < T:
                padding = T - data.shape[1]
                last_frame = data[:, -1:, :, :].repeat(padding, axis=1)
                data = np.concatenate([data, last_frame], axis=1)
        
        # Random spatial jittering (ê³µê°„ì¶• ë³€í˜•)
        if np.random.rand() < 0.3:
            noise_factor = 0.02
            noise = np.random.normal(0, noise_factor, data.shape)
            data = data + noise
        
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data = self.data[idx].copy()
        
        if self.augment:
            data = self.augment_data(data)
        
        data = torch.FloatTensor(data)
        label = torch.LongTensor([self.labels[idx]])[0]
        return data, label

def train_optimized_shift_gcn(data_dir, model_save_path, epochs=200, batch_size=8, lr=0.0005):
    """ìµœì í™”ëœ Shift-GCN í•™ìŠµ"""
    
    if not os.path.exists(data_dir):
        print(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return
    
    # Load metadata
    metadata_file = os.path.join(data_dir, 'metadata.npy')
    if os.path.exists(metadata_file):
        metadata = np.load(metadata_file, allow_pickle=True).item()
        num_joints = metadata['num_joints']
        target_joints = metadata['target_joints']
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ë¡œë“œ: {target_joints}, {num_joints}ê°œ ê´€ì ˆì ")
    else:
        print("âš ï¸ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
        num_joints = 9
    
    # Load adjacency matrix
    adj_file = os.path.join(data_dir, 'adjacency_matrix.npy')
    if os.path.exists(adj_file):
        adjacency_matrix = np.load(adj_file)
        print(f"ğŸ“Š ì¸ì ‘ í–‰ë ¬ ë¡œë“œ: {adjacency_matrix.shape}")
    else:
        print("âš ï¸ ì¸ì ‘ í–‰ë ¬ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¨ìœ„ í–‰ë ¬ ì‚¬ìš©")
        adjacency_matrix = np.eye(num_joints)
    
    # Create datasets with augmentation
    train_dataset = GestureDataset(data_dir, split='train', test_size=0.2, augment=True)
    test_dataset = GestureDataset(data_dir, split='test', test_size=0.2, augment=False)
    
    if len(train_dataset) == 0:
        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„ì„:")
    print(f"   - í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset)}")
    print(f"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_dataset)}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {len(train_dataset) // batch_size}")
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Get number of classes
    num_classes = len(train_dataset.action_to_idx)
    print(f"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
    print(f"ğŸ“Š ì•¡ì…˜ ë§¤í•‘: {train_dataset.action_to_idx}")
    
    # Create optimized model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = OptimizedShiftGCN(
        num_classes=num_classes, 
        num_joints=num_joints, 
        temporal_kernel_size=5,  # 9 â†’ 5 (ê²½ëŸ‰í™”)
        dropout=0.5  # 0.3 â†’ 0.5 (ê°•í•œ ì •ê·œí™”)
    )
    model.set_adjacency_matrix(adjacency_matrix)
    model = model.to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸš€ ìµœì í™”ëœ ëª¨ë¸ ìƒì„± ì™„ë£Œ: {device}")
    print(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    
    # Loss and optimizer (ë” ë³´ìˆ˜ì ì¸ ì„¤ì •)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)  # weight_decay ì¦ê°€
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=20
    )  # verbose ì œê±°
    
    # Training history
    train_losses = []
    train_accuracies = []
    test_accuracies = []
    best_test_acc = 0.0
    patience_counter = 0
    early_stopping_patience = 50  # ì¡°ê¸° ì¢…ë£Œ patience
    
    print(f"\nğŸš€ ìµœì í™”ëœ Shift-GCN í•™ìŠµ ì‹œì‘")
    print(f"   - ì—í¬í¬: {epochs}")
    print(f"   - í•™ìŠµë¥ : {lr}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience} ì—í¬í¬")
    print("=" * 60)
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, labels) in enumerate(train_loader):
            data, labels = data.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping for stability
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_acc = 100.0 * train_correct / train_total
        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc)
        
        # Testing
        model.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for data, labels in test_loader:
                data, labels = data.to(device), labels.to(device)
                outputs = model(data)
                _, predicted = torch.max(outputs.data, 1)
                test_total += labels.size(0)
                test_correct += (predicted == labels).sum().item()
        
        test_acc = 100.0 * test_correct / test_total
        test_accuracies.append(test_acc)
        
        # Update scheduler
        scheduler.step(test_acc)
        
        # Save best model and early stopping
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), model_save_path)
            patience_counter = 0
        else:
            patience_counter += 1
        
        # Print progress
        if (epoch + 1) % 5 == 0 or epoch == 0:  # ë” ìì£¼ ì¶œë ¥
            print(f'Epoch [{epoch+1}/{epochs}]')
            print(f'  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Test Acc: {test_acc:.2f}%, Best: {best_test_acc:.2f}%')
            print(f'  LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print(f'  Patience: {patience_counter}/{early_stopping_patience}')
        
        # Early stopping
        if patience_counter >= early_stopping_patience:
            print(f"\nâ¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"   ìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_test_acc:.2f}%")
    print(f"   ì‹¤ì œ ì—í¬í¬: {epoch + 1}/{epochs}")
    print(f"   ëª¨ë¸ ì €ì¥: {model_save_path}")
    
    # Final evaluation (ì´í›„ ì½”ë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼)
    model.load_state_dict(torch.load(model_save_path))
    model.eval()
    
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            data, labels = data.to(device), labels.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Classification report
    idx_to_action = {v: k for k, v in train_dataset.action_to_idx.items()}
    action_names = [idx_to_action[i] for i in range(num_classes)]
    
    print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€:")
    print(classification_report(all_labels, all_predictions, target_names=action_names))
    
    # Save plots
    plots_dir = os.path.dirname(model_save_path)
    
    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=action_names, yticklabels=action_names)
    plt.title('Optimized Shift-GCN Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'optimized_confusion_matrix.png'))
    plt.close()
    
    # Training history
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    
    plt.subplot(1, 3, 2)
    plt.plot(train_accuracies, label='Train', alpha=0.7)
    plt.plot(test_accuracies, label='Test', alpha=0.7)
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 3, 3)
    overfitting_gap = [train_accuracies[i] - test_accuracies[i] for i in range(len(train_accuracies))]
    plt.plot(overfitting_gap, color='red', alpha=0.7)
    plt.title('Overfitting Gap (Train - Test)')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy Gap (%)')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'optimized_training_history.png'))
    plt.close()
    
    print(f"ğŸ“Š ìµœì í™”ëœ í‰ê°€ ê²°ê³¼ ì €ì¥: {plots_dir}")

if __name__ == "__main__":
    # ìµœì í™”ëœ í•™ìŠµ ì„¤ì •
    data_dir = "./shift_gcn_data"
    model_save_path = "./models/optimized_shift_gcn_model.pth"  # models í´ë”ë¡œ ë³€ê²½
    
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    print("ğŸš€ ìµœì í™”ëœ Shift-GCN ì œìŠ¤ì²˜ ì¸ì‹ í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    # ì‘ì€ ë°ì´í„°ì…‹ì— ìµœì í™”ëœ ì„¤ì •
    train_optimized_shift_gcn(
        data_dir=data_dir,
        model_save_path=model_save_path,
        epochs=200,      # ë” ë§ì€ ì—í¬í¬ (ì¡°ê¸° ì¢…ë£Œë¡œ ì œì–´)
        batch_size=8,    # ë” ì‘ì€ ë°°ì¹˜ (ì•ˆì •ì  í•™ìŠµ)
        lr=0.0005        # ë” ì‘ì€ í•™ìŠµë¥  (ì•ˆì •ì  ìˆ˜ë ´)
    ) 